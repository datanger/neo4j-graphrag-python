#!/usr/bin/env python3
"""
Full Pipeline Integration Tests for MATLAB Code Extractor.

This test suite verifies the complete end-to-end functionality of the MATLAB code extractor,
including Neo4j database integration, cross-file relationship processing, data validation,
error handling, performance, and real-world scenarios.

These tests require a running Neo4j database and test the complete pipeline from
code extraction to database storage and retrieval.
"""

import os
import sys
import asyncio
import unittest
import tempfile
import shutil
import time
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional

# Add the project root to the path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from unittest.mock import MagicMock, patch
import neo4j
import pytest

from neo4j_graphrag.experimental.components.code_extractor.matlab.matlab_extractor import (
    MatlabExtractor, MatlabExtractionResult, get_global_registry
)
from neo4j_graphrag.experimental.components.kg_writer import Neo4jWriter
from neo4j_graphrag.experimental.components.types import (
    Neo4jGraph, TextChunk, TextChunks, DocumentInfo, LexicalGraphConfig
)
from neo4j_graphrag.experimental.components.code_extractor.matlab.requirements import SCHEMA, EXAMPLES
from neo4j_graphrag.experimental.components.code_extractor.matlab.post_processor import MatlabPostProcessor

class MockLLM:
    """Mock LLM for testing."""
    async def generate(self, prompt: str) -> str:
        return "Mock description generated by LLM"

class TestFullPipeline(unittest.TestCase):
    """Integration tests for the complete MATLAB code extractor pipeline."""

    @classmethod
    def setUpClass(cls):
        """Set up the test environment once before all tests."""
        cls.test_dir = Path(__file__).parent / "test_data"
        print(f"Using test data from: {cls.test_dir}")
        
        # Neo4j connection settings
        cls.NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
        cls.NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
        cls.NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password")
        cls.NEO4J_DB = os.getenv("NEO4J_DB", "neo4j")
        
        # Set up extractor
        cls.mock_llm = MockLLM()
        cls.extractor = MatlabExtractor(
            llm=cls.mock_llm,
            enable_post_processing=True
        )
        
        # Initialize Neo4j driver
        try:
            cls.driver = neo4j.GraphDatabase.driver(
                cls.NEO4J_URI,
                auth=(cls.NEO4J_USER, cls.NEO4J_PASSWORD)
            )
            # Test connection
            with cls.driver.session(database=cls.NEO4J_DB) as session:
                session.run("RETURN 1")
            print("✓ Neo4j connection established")
            cls.neo4j_available = True
        except Exception as e:
            print(f"⚠️  Neo4j not available: {e}")
            print("Integration tests requiring Neo4j will be skipped")
            cls.neo4j_available = False
            cls.driver = None

    @classmethod
    def tearDownClass(cls):
        """Clean up after all tests."""
        if cls.driver:
            cls.driver.close()
            print("Neo4j driver closed")

    def setUp(self):
        """Set up before each test."""
        # Reset the global registry for clean tests
        if hasattr(MatlabExtractor, 'reset_global_registry'):
            MatlabExtractor.reset_global_registry()
        
        # Clear Neo4j database before each test
        if self.neo4j_available and self.driver:
            try:
                with self.driver.session(database=self.NEO4J_DB) as session:
                    session.run("MATCH (n) DETACH DELETE n")
                print("Database cleared for test")
            except Exception as e:
                print(f"Warning: Could not clear database: {e}")

    async def _extract_all_files(self, extractor: MatlabExtractor, directory: Path) -> Tuple[List[Any], List[Any]]:
        """Extract nodes and relationships from all MATLAB files in a directory."""
        all_nodes = []
        all_relationships = []
        
        matlab_files = list(directory.rglob("*.m"))
        
        for file_path in matlab_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                chunk = TextChunk(
                    text=content,
                    index=0,
                    metadata={"file_path": str(file_path), "file_name": file_path.name, "code_type": "matlab"}
                )

                doc_info = DocumentInfo(
                    path=str(file_path),
                    metadata={"name": file_path.name}
                )

                result = await extractor.run(
                    chunks=TextChunks(chunks=[chunk]),
                    schema=SCHEMA,
                    document_info=doc_info,
                    lexical_graph_config=LexicalGraphConfig(),
                    examples=EXAMPLES,
                    enable_post_processing=True,
                )
                
                if result and hasattr(result, 'graph') and result.graph:
                    all_nodes.extend(result.graph.nodes)
                    all_relationships.extend(result.graph.relationships)
                    
            except Exception as e:
                print(f"Error extracting from {file_path}: {e}")
                
        return all_nodes, all_relationships

    async def _write_to_neo4j(self, nodes: List[Any], relationships: List[Any]) -> bool:
        """Write extracted data to Neo4j database."""
        if not self.neo4j_available:
            return False
            
        try:
            writer = Neo4jWriter(
                driver=self.driver,
                neo4j_database=self.NEO4J_DB
            )
            
            # Create Neo4jGraph object
            from neo4j_graphrag.experimental.components.types import Neo4jNode, Neo4jRelationship
            
            neo4j_nodes = []
            for node in nodes:
                try:
                    neo4j_node = Neo4jNode(
                        id=str(getattr(node, 'id', '')),
                        label=str(getattr(node, 'label', '')),
                        properties=dict(getattr(node, 'properties', {}))
                    )
                    neo4j_nodes.append(neo4j_node)
                except Exception as e:
                    print(f"Error creating Neo4j node: {e}")
            
            neo4j_relationships = []
            for rel in relationships:
                try:
                    neo4j_rel = Neo4jRelationship(
                        start_node_id=str(getattr(rel, 'start_node_id', '')),
                        end_node_id=str(getattr(rel, 'end_node_id', '')),
                        type=str(getattr(rel, 'type', '')),
                        properties=dict(getattr(rel, 'properties', {}))
                    )
                    neo4j_relationships.append(neo4j_rel)
                except Exception as e:
                    print(f"Error creating Neo4j relationship: {e}")
            
            graph = Neo4jGraph(nodes=neo4j_nodes, relationships=neo4j_relationships)
            
            # Write to Neo4j
            result = await writer.run(graph)
            return result.status == "success"
            
        except Exception as e:
            print(f"Error writing to Neo4j: {e}")
            return False

    async def _read_from_neo4j(self) -> Tuple[List[Dict], List[Dict]]:
        """Read data from Neo4j database."""
        if not self.neo4j_available:
            return [], []
            
        try:
            with self.driver.session(database=self.NEO4J_DB) as session:
                # Read nodes
                node_result = session.run("MATCH (n) RETURN n")
                nodes = [record["n"] for record in node_result]
                
                # Read relationships
                rel_result = session.run("MATCH ()-[r]->() RETURN r")
                relationships = [record["r"] for record in rel_result]
                
                return nodes, relationships
                
        except Exception as e:
            print(f"Error reading from Neo4j: {e}")
            return [], []

    @pytest.mark.asyncio
    async def test_full_pipeline_integration(self):
        """Test complete end-to-end pipeline execution."""
        print("\nTesting full pipeline integration...")
        
        # Extract data from all test files
        nodes, relationships = await self._extract_all_files(self.extractor, self.test_dir)
        
        print(f"Extracted {len(nodes)} nodes and {len(relationships)} relationships")
        
        # Validate extracted data
        self.assertGreater(len(nodes), 0, "Should extract at least some nodes")
        self.assertGreater(len(relationships), 0, "Should extract at least some relationships")
        
        # Check for required node types
        node_labels = set(node.label for node in nodes)
        expected_labels = {"Function", "Variable", "Script"}
        self.assertTrue(expected_labels.issubset(node_labels), 
                       f"Missing node types. Expected: {expected_labels}, Found: {node_labels}")
        
        # Check for required relationship types
        rel_types = set(rel.type for rel in relationships)
        expected_rel_types = {"USES", "DEFINES"}  # Based on current implementation
        found_rel_types = rel_types.intersection(expected_rel_types)
        self.assertGreater(len(found_rel_types), 0, 
                          f"Should have at least some expected relationship types. Found: {rel_types}")
        
        # Write to Neo4j if available
        if self.neo4j_available:
            success = await self._write_to_neo4j(nodes, relationships)
            self.assertTrue(success, "Should successfully write to Neo4j")
            
            # Read back from Neo4j
            db_nodes, db_relationships = await self._read_from_neo4j()
            
            # Validate data persistence
            self.assertEqual(len(db_nodes), len(nodes), 
                           "Number of nodes in database should match extracted nodes")
            self.assertEqual(len(db_relationships), len(relationships), 
                           "Number of relationships in database should match extracted relationships")
            
            print(f"✓ Full pipeline integration test passed ({len(nodes)} nodes, {len(relationships)} relationships)")
        else:
            print("⚠️  Skipping Neo4j integration (database not available)")
            print(f"✓ Pipeline extraction test passed ({len(nodes)} nodes, {len(relationships)} relationships)")

    @pytest.mark.asyncio
    async def test_neo4j_database_integration(self):
        """Test Neo4j database integration specifically."""
        if not self.neo4j_available:
            self.skipTest("Neo4j database not available")
            
        print("\nTesting Neo4j database integration...")
        
        # Test basic database operations
        with self.driver.session(database=self.NEO4J_DB) as session:
            # Test write operation
            result = session.run("CREATE (n:TestNode {name: 'test'}) RETURN n")
            node = result.single()["n"]
            self.assertIsNotNone(node, "Should create a test node")
            
            # Test read operation
            result = session.run("MATCH (n:TestNode) RETURN n")
            nodes = list(result)
            self.assertEqual(len(nodes), 1, "Should find exactly one test node")
            
            # Test relationship creation
            result = session.run("""
                CREATE (n1:TestNode {name: 'node1'})
                CREATE (n2:TestNode {name: 'node2'})
                CREATE (n1)-[r:TEST_REL]->(n2)
                RETURN r
            """)
            rel = result.single()["r"]
            self.assertIsNotNone(rel, "Should create a test relationship")
            
            # Clean up
            session.run("MATCH (n:TestNode) DETACH DELETE n")
            
        print("✓ Neo4j database integration test passed")

    @pytest.mark.asyncio
    async def test_cross_file_relationship_processing(self):
        """Test processing of multiple files and cross-file relationships."""
        print("\nTesting cross-file relationship processing...")
        
        # Extract from main script and helper files
        main_nodes, main_relationships = await self._extract_all_files(
            self.extractor, self.test_dir
        )
        
        # Check for cross-file relationships
        cross_file_relationships = []
        for rel in main_relationships:
            start_props = getattr(rel, 'properties', {})
            end_props = getattr(rel, 'properties', {})
            
            # Look for relationships that span different files
            if (start_props.get('file_path') != end_props.get('file_path') and
                start_props.get('file_path') and end_props.get('file_path')):
                cross_file_relationships.append(rel)
        
        print(f"Found {len(cross_file_relationships)} cross-file relationships")
        
        # Validate cross-file processing
        self.assertGreater(len(main_nodes), 0, "Should extract nodes from multiple files")
        self.assertGreater(len(main_relationships), 0, "Should extract relationships from multiple files")
        
        # Check that we have nodes from different files
        file_paths = set()
        for node in main_nodes:
            props = getattr(node, 'properties', {})
            if 'file_path' in props:
                file_paths.add(props['file_path'])
        
        self.assertGreater(len(file_paths), 1, "Should process multiple files")
        print(f"Processed files: {file_paths}")
        
        print("✓ Cross-file relationship processing test passed")

    @pytest.mark.asyncio
    async def test_data_validation_and_conversion(self):
        """Test data type conversion and validation for Neo4j compatibility."""
        print("\nTesting data validation and conversion...")
        
        # Extract data
        nodes, relationships = await self._extract_all_files(self.extractor, self.test_dir)
        
        # Test node property validation
        for i, node in enumerate(nodes):
            props = getattr(node, 'properties', {})
            
            # Check required properties
            if node.label == 'Variable':
                required_props = ['name', 'file_path', 'scope_id', 'scope_type']
                for prop in required_props:
                    self.assertIn(prop, props, f"Variable node {i} missing required property: {prop}")
            
            # Check property types
            for key, value in props.items():
                # All property values should be Neo4j-compatible
                self.assertIsInstance(value, (str, int, float, bool, type(None)), 
                                   f"Node {i} property '{key}' has invalid type: {type(value)}")
        
        # Test relationship property validation
        for i, rel in enumerate(relationships):
            props = getattr(rel, 'properties', {})
            
            # Check property types
            for key, value in props.items():
                self.assertIsInstance(value, (str, int, float, bool, type(None)), 
                                   f"Relationship {i} property '{key}' has invalid type: {type(value)}")
        
        print("✓ Data validation and conversion test passed")

    @pytest.mark.asyncio
    async def test_error_handling_and_recovery(self):
        """Test error handling and recovery scenarios."""
        print("\nTesting error handling and recovery...")
        
        # Test with invalid file content
        invalid_content = "invalid matlab code with syntax errors"
        
        chunk = TextChunk(
            text=invalid_content,
            index=0,
            metadata={"file_path": "invalid.m", "file_name": "invalid.m", "code_type": "matlab"}
        )
        
        doc_info = DocumentInfo(
            path="invalid.m",
            metadata={"name": "invalid.m"}
        )
        
        # Should handle invalid content gracefully
        try:
            result = await self.extractor.run(
                chunks=TextChunks(chunks=[chunk]),
                schema=SCHEMA,
                document_info=doc_info,
                lexical_graph_config=LexicalGraphConfig(),
                examples=EXAMPLES,
                enable_post_processing=True,
            )
            
            # Should not crash, even with invalid content
            self.assertIsNotNone(result, "Should handle invalid content gracefully")
            
        except Exception as e:
            # If it does raise an exception, it should be a specific type
            self.assertIsInstance(e, (ValueError, SyntaxError, RuntimeError), 
                               f"Unexpected exception type: {type(e)}")
        
        # Test with empty content
        empty_chunk = TextChunk(
            text="",
            index=0,
            metadata={"file_path": "empty.m", "file_name": "empty.m", "code_type": "matlab"}
        )
        
        empty_doc_info = DocumentInfo(
            path="empty.m",
            metadata={"name": "empty.m"}
        )
        
        try:
            result = await self.extractor.run(
                chunks=TextChunks(chunks=[empty_chunk]),
                schema=SCHEMA,
                document_info=empty_doc_info,
                lexical_graph_config=LexicalGraphConfig(),
                examples=EXAMPLES,
                enable_post_processing=True,
            )
            
            # Should handle empty content gracefully
            self.assertIsNotNone(result, "Should handle empty content gracefully")
            
        except Exception as e:
            self.fail(f"Should not raise exception for empty content: {e}")
        
        print("✓ Error handling and recovery test passed")

    @pytest.mark.asyncio
    async def test_performance_and_scalability(self):
        """Test performance and scalability with larger datasets."""
        print("\nTesting performance and scalability...")
        
        # Create a larger test dataset
        large_test_dir = Path(tempfile.mkdtemp())
        try:
            # Create multiple test files
            for i in range(5):
                file_path = large_test_dir / f"large_file_{i}.m"
                content = f"""
% Large test file {i}
function result{i} = large_function_{i}(input{i})
    % Define variables
    var1 = input{i} * 2;
    var2 = var1 + 10;
    var3 = var2 / 3;
    
    % Call other functions
    result{i} = helper_function_{i}(var3);
end

function output = helper_function_{i}(input)
    output = input * 2 + 1;
end
"""
                with open(file_path, 'w') as f:
                    f.write(content)
            
            # Measure extraction time
            start_time = time.time()
            nodes, relationships = await self._extract_all_files(self.extractor, large_test_dir)
            extraction_time = time.time() - start_time
            
            print(f"Extracted {len(nodes)} nodes and {len(relationships)} relationships in {extraction_time:.2f} seconds")
            
            # Performance assertions
            self.assertLess(extraction_time, 30.0, "Extraction should complete within 30 seconds")
            self.assertGreater(len(nodes), 0, "Should extract nodes from large dataset")
            self.assertGreater(len(relationships), 0, "Should extract relationships from large dataset")
            
            # Memory usage check (basic)
            import psutil
            process = psutil.Process()
            memory_usage = process.memory_info().rss / 1024 / 1024  # MB
            print(f"Memory usage: {memory_usage:.2f} MB")
            
            self.assertLess(memory_usage, 500, "Memory usage should be reasonable (< 500 MB)")
            
        finally:
            # Clean up
            shutil.rmtree(large_test_dir)
        
        print("✓ Performance and scalability test passed")

    @pytest.mark.asyncio
    async def test_real_world_scenario(self):
        """Test with realistic MATLAB codebase scenarios."""
        print("\nTesting real-world scenario...")
        
        # Create a realistic MATLAB project structure
        realistic_dir = Path(tempfile.mkdtemp())
        try:
            # Main script
            main_script = realistic_dir / "main_analysis.m"
            main_content = """
% Main analysis script
clear; clc; close all;

% Load configuration
config = load_config();

% Process data
data = load_data(config.data_path);
processed_data = preprocess_data(data, config);

% Run analysis
results = run_analysis(processed_data, config);

% Save results
save_results(results, config.output_path);

% Display summary
display_summary(results);
"""
            with open(main_script, 'w') as f:
                f.write(main_content)
            
            # Configuration function
            config_func = realistic_dir / "load_config.m"
            config_content = """
function config = load_config()
    config.data_path = 'data/input.mat';
    config.output_path = 'results/output.mat';
    config.analysis_type = 'statistical';
    config.parameters = struct('alpha', 0.05, 'method', 'parametric');
end
"""
            with open(config_func, 'w') as f:
                f.write(config_content)
            
            # Data processing function
            data_func = realistic_dir / "preprocess_data.m"
            data_content = """
function processed = preprocess_data(data, config)
    % Remove outliers
    processed = remove_outliers(data);
    
    % Normalize data
    processed = normalize_data(processed);
    
    % Apply filters
    processed = apply_filters(processed, config.parameters);
end

function clean_data = remove_outliers(data)
    threshold = 3 * std(data);
    clean_data = data(abs(data - mean(data)) <= threshold);
end

function norm_data = normalize_data(data)
    norm_data = (data - mean(data)) / std(data);
end

function filtered = apply_filters(data, params)
    filtered = data;  % Placeholder
end
"""
            with open(data_func, 'w') as f:
                f.write(data_content)
            
            # Analysis function
            analysis_func = realistic_dir / "run_analysis.m"
            analysis_content = """
function results = run_analysis(data, config)
    switch config.analysis_type
        case 'statistical'
            results = statistical_analysis(data, config.parameters);
        case 'machine_learning'
            results = ml_analysis(data, config.parameters);
        otherwise
            error('Unknown analysis type');
    end
end

function stats = statistical_analysis(data, params)
    stats.mean = mean(data);
    stats.std = std(data);
    stats.p_value = ttest(data, params.alpha);
end

function ml_results = ml_analysis(data, params)
    ml_results = struct();  % Placeholder
end
"""
            with open(analysis_func, 'w') as f:
                f.write(analysis_content)
            
            # Extract from realistic project
            nodes, relationships = await self._extract_all_files(self.extractor, realistic_dir)
            
            print(f"Realistic scenario: {len(nodes)} nodes, {len(relationships)} relationships")
            
            # Validate realistic patterns
            node_labels = set(node.label for node in nodes)
            rel_types = set(rel.type for rel in relationships)
            
            # Should have all major node types
            self.assertIn('Script', node_labels, "Should have Script nodes")
            self.assertIn('Function', node_labels, "Should have Function nodes")
            self.assertIn('Variable', node_labels, "Should have Variable nodes")
            
            # Should have function calls and variable usage
            self.assertGreater(len(rel_types), 0, "Should have relationship types")
            
            # Check for specific patterns
            function_nodes = [n for n in nodes if n.label == 'Function']
            script_nodes = [n for n in nodes if n.label == 'Script']
            
            self.assertGreater(len(function_nodes), 0, "Should have function definitions")
            self.assertGreater(len(script_nodes), 0, "Should have script definitions")
            
        finally:
            # Clean up
            shutil.rmtree(realistic_dir)
        
        print("✓ Real-world scenario test passed")

    @pytest.mark.asyncio
    async def test_data_integrity_and_consistency(self):
        """Test data integrity and consistency throughout the pipeline."""
        print("\nTesting data integrity and consistency...")
        
        # Extract data
        nodes, relationships = await self._extract_all_files(self.extractor, self.test_dir)
        
        # Test node consistency
        node_ids = set()
        for node in nodes:
            node_id = getattr(node, 'id', '')
            self.assertNotIn(node_id, node_ids, f"Duplicate node ID: {node_id}")
            node_ids.add(node_id)
            
            # Check required properties based on node type
            props = getattr(node, 'properties', {})
            if node.label == 'Variable':
                self.assertIn('name', props, "Variable nodes should have name property")
                self.assertIn('scope_id', props, "Variable nodes should have scope_id property")
                self.assertIn('scope_type', props, "Variable nodes should have scope_type property")
        
        # Test relationship consistency
        for rel in relationships:
            start_id = getattr(rel, 'start_node_id', '')
            end_id = getattr(rel, 'end_node_id', '')
            rel_type = getattr(rel, 'type', '')
            
            # Check that referenced nodes exist
            self.assertIn(start_id, node_ids, f"Relationship references non-existent start node: {start_id}")
            self.assertIn(end_id, node_ids, f"Relationship references non-existent end node: {end_id}")
            
            # Check relationship type is valid
            self.assertIn(rel_type, {"CALLS", "USES", "DEFINES", "MODIFIES", "ASSIGNED_TO"}, 
                         f"Invalid relationship type: {rel_type}")
        
        # Test scope isolation
        variable_nodes = [n for n in nodes if n.label == 'Variable']
        scope_combinations = set()
        
        for var in variable_nodes:
            props = getattr(var, 'properties', {})
            name = props.get('name', '')
            scope_id = props.get('scope_id', '')
            scope_type = props.get('scope_type', '')
            
            combination = (name, scope_id, scope_type)
            self.assertNotIn(combination, scope_combinations, 
                           f"Duplicate variable scope combination: {combination}")
            scope_combinations.add(combination)
        
        print("✓ Data integrity and consistency test passed")

    @pytest.mark.asyncio
    async def test_edge_cases_and_boundary_conditions(self):
        """Test edge cases and boundary conditions."""
        print("\nTesting edge cases and boundary conditions...")
        
        # Test with very large file
        large_content = "% Large file test\n" + "\n".join([
            f"variable_{i} = {i};" for i in range(1000)
        ])
        
        chunk = TextChunk(
            text=large_content,
            index=0,
            metadata={"file_path": "large.m", "file_name": "large.m", "code_type": "matlab"}
        )
        
        doc_info = DocumentInfo(
            path="large.m",
            metadata={"name": "large.m"}
        )
        
        try:
            result = await self.extractor.run(
                chunks=TextChunks(chunks=[chunk]),
                schema=SCHEMA,
                document_info=doc_info,
                lexical_graph_config=LexicalGraphConfig(),
                examples=EXAMPLES,
                enable_post_processing=True,
            )
            
            # Should handle large files
            self.assertIsNotNone(result, "Should handle large files")
            
        except Exception as e:
            self.fail(f"Should not fail on large files: {e}")
        
        # Test with special characters in names
        special_content = """
function result = test_function_with_special_chars()
    variable_with_underscores = 1;
    variableWithCamelCase = 2;
    variable_with_numbers_123 = 3;
    result = variable_with_underscores + variableWithCamelCase + variable_with_numbers_123;
end
"""
        
        chunk = TextChunk(
            text=special_content,
            index=0,
            metadata={"file_path": "special.m", "file_name": "special.m", "code_type": "matlab"}
        )
        
        doc_info = DocumentInfo(
            path="special.m",
            metadata={"name": "special.m"}
        )
        
        try:
            result = await self.extractor.run(
                chunks=TextChunks(chunks=[chunk]),
                schema=SCHEMA,
                document_info=doc_info,
                lexical_graph_config=LexicalGraphConfig(),
                examples=EXAMPLES,
                enable_post_processing=True,
            )
            
            # Should handle special characters
            self.assertIsNotNone(result, "Should handle special characters in names")
            
        except Exception as e:
            self.fail(f"Should not fail on special characters: {e}")
        
        print("✓ Edge cases and boundary conditions test passed")

if __name__ == "__main__":
    # Run the tests
    unittest.main() 