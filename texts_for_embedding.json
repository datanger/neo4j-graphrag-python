[
  "name: test_script_uses_variable file_path: test_script_uses_variable.m start_line: 1 end_line: 39 code_snippet: %% Test Script for Script -[USES]-> Variable Relationship\n% This script demonstrates how scripts can use variables defined in other scripts\n% This should generate Script -[USES]-> Variable relationships\n\n% Load BERT model (this creates a variable 'mdl')\nmdl = bert;\n\n% Use the model variable in calculations\nmodelSize = size(mdl.Parameters.Weights.embedding.weight);\ndisp(['Model embedding size: ', num2str(modelSize)]);\n\n% Use tokenizer from the model\ntokenizer = mdl.Tokenizer;\nvocabSize = tokenizer.VocabularySize;\ndisp(['Vocabulary size: ', num2str(vocabSize)]);\n\n% Create some test data using the model\ntestText = \"Hello world\";\ntokens = tokenize(tokenizer, testText);\nencodedData = encodeTokens(tokenizer, tokens);\n\n% Use the encoded data for prediction\npredictions = predictMaskedToken(mdl, testText);\n\n% Display results\nfprintf('Input text: %s\\n', testText);\nfprintf('Number of tokens: %d\\n', length(tokens{1}));\nfprintf('Encoded data size: %s\\n', mat2str(size(encodedData{1})));\n\n% Use variables from other scripts (if they exist)\n% This demonstrates cross-script variable usage\nif exist('modelSize', 'var')\n    fprintf('Model size from previous calculation: %s\\n', mat2str(modelSize));\nend\n\n% Create a simple calculation using multiple variables\ntotalParameters = prod(modelSize) + vocabSize;\nfprintf('Total parameters estimate: %d\\n', totalParameters);\n scope_id: test_script_uses_variable scope_type: script",
  "name: simple_script file_path: simple_script.m start_line: 1 end_line: 32 code_snippet: %% Simple Script for Testing Variable Usage\n% This is a script (not a function) that uses variables\n\n% Define some variables\nx = 10;\ny = 20;\nz = x + y;\n\n% Use variables in calculations\nresult = x * y + z;\ndisp(['Result: ', num2str(result)]);\n\n% Use variables in conditional statements\nif x > 5\n    message = 'x is greater than 5';\n    disp(message);\nend\n\n% Use variables in loops\nfor i = 1:x\n    disp(['Iteration ', num2str(i), ' of ', num2str(x)]);\nend\n\n% Use variables in array operations\narray = [x, y, z, result];\ndisp(['Array: ', mat2str(array)]);\n\n% Use variables in function calls\nmaxValue = max(array);\nminValue = min(array);\ndisp(['Max: ', num2str(maxValue), ', Min: ', num2str(minValue)]);\n scope_id: simple_script scope_type: script",
  "name: preprocessPredictors file_path: ClassifyTextDataUsingBERT.m start_line: 239 end_line: 244 input_parameters: ['X', 'paddingValue', 'maxSeqLen'] code_snippet: function X = preprocessPredictors(X,paddingValue,maxSeqLen)\n\nX = truncateSequences(X,maxSeqLen);\nX = padsequences(X,2,\"PaddingValue\",paddingValue);\n\nend scope_id: ClassifyTextDataUsingBERT scope_type: script",
  "name: bertEmbed file_path: ClassifyTextDataUsingBERT.m start_line: 250 end_line: 256 input_parameters: ['X', 'parameters', 'args'] code_snippet: function Y = bertEmbed(X,parameters,args)\n\narguments\n    X\n    parameters\n    args.DropoutProbability = 0\nend scope_id: ClassifyTextDataUsingBERT scope_type: script",
  "name: ClassifyTextDataUsingBERT file_path: ClassifyTextDataUsingBERT.m start_line: 1 end_line: 270 code_snippet: %% Classify Text Data Using BERT\n% This example shows how to classify text data using a pretrained BERT\n% model as a feature extractor.\n%\n% The simplest use of a pretrained BERT model is to use it as a feature\n% extractor. In particular, you can use the BERT model to convert documents\n% to feature vectors which you can then use as input to train a deep\n% learning classification network.\n%\n% This example shows how to use a pretrained BERT model to classify failure\n% events given a data set of factory reports.\n\n%% Load Pretrained BERT Model\n% Load a pretrained BERT model using the |bert| function. The model\n% consists of a tokenizer that encodes text as sequences of integers, and\n% a structure of parameters.\nmdl = bert\n\n%%\n% View the BERT model tokenizer. The tokenizer encodes text as sequences of\n% integers and holds the details of padding, start, separator and mask\n% tokens.\ntokenizer = mdl.Tokenizer\n\n%% Load Data\n% Load the example data. The file |factoryReports.csv| contains factory\n% reports, including a text description and categorical labels for each\n% event.\n\nfilename = \"factoryReports.csv\";\ndata = readtable(filename,\"TextType\",\"string\");\nhead(data)\n\n%%\n% The goal of this example is to classify events by the label in the\n% |Category| column. To divide the data into classes, convert these labels\n% to categorical.\ndata.Category = categorical(data.Category);\n\n%%\n% View the number of classes.\nclasses = categories(data.Category);\nnumClasses = numel(classes)\n\n%%\n% View the distribution of the classes in the data using a histogram.\nfigure\nhistogram(data.Category);\nxlabel(\"Class\")\nylabel(\"Frequency\")\ntitle(\"Class Distribution\")\n\n%%\n% Encode the text data using the BERT model tokenizer using the |encode|\n% function and add the tokens to the training data table.\ndata.Tokens = encode(tokenizer, data.Description);\n\n%%\n% The next step is to partition it into sets for training and validation.\n% Partition the data into a training partition and a held-out partition for\n% validation and testing. Specify the holdout percentage to be 20%.\ncvp = cvpartition(data.Category,\"Holdout\",0.2);\ndataTrain = data(training(cvp),:);\ndataValidation = data(test(cvp),:);\n\n%%\n% View the number of training and validation observations.\nnumObservationsTrain = size(dataTrain,1)\nnumObservationsValidation = size(dataValidation,1)\n\n%%\n% Extract the text data, labels, and encoded BERT tokens from the\n% partitioned tables.\ntextDataTrain = dataTrain.Description;\ntextDataValidation = dataValidation.Description;\n\nTTrain = dataTrain.Category;\nTValidation = dataValidation.Category;\n\ntokensTrain = dataTrain.Tokens;\ntokensValidation = dataValidation.Tokens;\n\n%%\n% To check that you have imported the data correctly, visualize the\n% training text data using a word cloud.\n\nfigure\nwordcloud(textDataTrain);\ntitle(\"Training Data\")\n\n%%\n% View the BERT token codes of the first few training documents.\ntokensTrain{1:5}\n\n%% Prepare Data for Training\n% Convert the documents to feature vectors using the BERT model as a\n% feature extractor.\n\n% To extract the features of the training data by iterating over\n% mini-batches, create a |minibatchqueue| object.\n\n% Mini-batch queues require a single datastore that outputs both the\n% predictors and responses. Create array datastores containing the training\n% BERT tokens and labels and combine them using the |combine| function.\ndsXTrain = arrayDatastore(tokensTrain,\"OutputType\",\"same\");\ndsTTrain = arrayDatastore(TTrain);\ncdsTrain = combine(dsXTrain,dsTTrain);\n\n% Create a combined datastore for the validation data using the same steps.\ndsXValidation = arrayDatastore(tokensValidation,\"OutputType\",\"same\");\ndsTValidation = arrayDatastore(TValidation);\ncdsValidation = combine(dsXValidation,dsTValidation);\n\n%%\n% Create a mini-batch queue for the training data. Specify a mini-batch\n% size of 32 and preprocess the mini-batches using the\n% |preprocessPredictors| function, listed at the end of the example.\nminiBatchSize = 32;\npaddingValue = mdl.Tokenizer.PaddingCode;\nmaxSequenceLength = mdl.Parameters.Hyperparameters.NumContext;\n\nmbqTrain = minibatchqueue(cdsTrain,1,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X) preprocessPredictors(X,paddingValue,maxSequenceLength));\n\n%%%\n% Create a mini-batch queue for the validation data using the same steps.\nmbqValidation = minibatchqueue(cdsValidation,1,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X) preprocessPredictors(X,paddingValue,maxSequenceLength));\n\n%%\n% To speed up feature extraction. Convert the BERT model weights to\n% gpuArray if a GPU is available.\nif canUseGPU\n    mdl.Parameters.Weights = dlupdate(@gpuArray,mdl.Parameters.Weights);\nend\n\n%%\n% Convert the training sequences of BERT model tokens to a\n% |N|-by-|embeddingDimension| array of feature vectors, where |N| is the\n% number of training observations and |embeddingDimension| is the dimension\n% of the BERT embedding.\n\nfeaturesTrain = [];\nreset(mbqTrain);\nwhile hasdata(mbqTrain)\n    X = next(mbqTrain);\n    features = bertEmbed(X,mdl.Parameters);\n    featuresTrain = [featuresTrain gather(extractdata(features))];\nend\n\n%%\n% Transpose the training data to have size |N|-by-|embeddingDimension|.\nfeaturesTrain = featuresTrain.';\n\n%%\n% Convert the validation data to feature vectors using the same steps.\nfeaturesValidation = [];\n\nreset(mbqValidation);\nwhile hasdata(mbqValidation)\n    X = next(mbqValidation);\n    features = bertEmbed(X,mdl.Parameters);\n    featuresValidation = cat(2,featuresValidation,gather(extractdata(features)));\nend\nfeaturesValidation = featuresValidation.';\n\n%% Define Deep Learning Network\n% Define a deep learning network that classifies the feature vectors.\n\nnumFeatures = mdl.Parameters.Hyperparameters.HiddenSize;\nlayers = [\n    featureInputLayer(numFeatures)\n    fullyConnectedLayer(numClasses)\n    softmaxLayer\n    classificationLayer];\n\n%% Specify Training Options\n% Specify the training options using the |trainingOptions| function.\n% * Train with a mini-batch size of 64.\n% * Shuffle the data every epoch.\n% * Validate the network using the validation data.\n% * Display the training progress in a plot and suppress the verbose\n%   output.\nopts = trainingOptions('adam',...\n    \"MiniBatchSize\",64,...\n    \"ValidationData\",{featuresValidation,dataValidation.Category},...\n    \"Shuffle\",\"every-epoch\", ...\n    \"Plots\",\"training-progress\", ...\n    \"Verbose\",0);\n\n%% Train Network\n% Train the network using the |trainNetwork| function.\nnet = trainNetwork(featuresTrain,dataTrain.Category,layers,opts);\n\n%% Test Network\n% Make predictions using the validation data and display the results in a\n% confusion matrix.\nYPredValidation = classify(net,featuresValidation);\n\nfigure\nconfusionchart(TValidation,YPredValidation)\n\n%%\n% Calculate the validation accuracy.\naccuracy = mean(dataValidation.Category == YPredValidation)\n\n%% Predict Using New Data\n% Classify the event type of three new reports. Create a string array\n% containing the new reports.\nreportsNew = [ ...\n    \"Coolant is pooling underneath sorter.\"\n    \"Sorter blows fuses at start up.\"\n    \"There are some very loud rattling sounds coming from the assembler.\"];\n\n%%\n% Tokenize the text data using the same steps as the training documents.\ntokensNew = encode(tokenizer,reportsNew);\n\n%%\n% Pad the sequences of tokens to the same length using the |padsequences| \n% function and pad using the tokenizer padding code.\nXNew = padsequences(tokensNew,2,\"PaddingValue\",tokenizer.PaddingCode);\n\n%%\n% Classify the new sequences using the trained model.\nfeaturesNew = bertEmbed(XNew,mdl.Parameters)';\nfeaturesNew = gather(extractdata(featuresNew));\nlabelsNew = classify(net,featuresNew)\n\n%% Supporting Functions\n\n%%% Predictors Preprocessing Functions\n% The |preprocessPredictors| function truncates the mini-batches to have\n% the specified maximum sequence length, pads the sequences to have the\n% same length. Use this preprocessing function to preprocess the predictors\n% only.\nfunction X = preprocessPredictors(X,paddingValue,maxSeqLen)\n\nX = truncateSequences(X,maxSeqLen);\nX = padsequences(X,2,\"PaddingValue\",paddingValue);\n\nend\n\n%%% BERT Embedding Function\n% The |bertEmbed| function maps input data to embedding vectors and\n% optionally applies dropout using the \"DropoutProbability\" name-value\n% pair.\nfunction Y = bertEmbed(X,parameters,args)\n\narguments\n    X\n    parameters\n    args.DropoutProbability = 0\nend\n\ndropoutProbabilitiy = args.DropoutProbability;\n\nY = bert.model(X,parameters, ...\n    \"DropoutProb\",dropoutProbabilitiy, ...\n    \"AttentionDropoutProb\",dropoutProbabilitiy);\n\n% To return single feature vectors, return the first element.\nY = Y(:,1,:);\nY = squeeze(Y);\n\nend\n\n scope_id: ClassifyTextDataUsingBERT scope_type: script",
  "name: encodeScalarString file_path: predictMaskedToken.m start_line: 36 end_line: 48 input_parameters: ['tok', 'str'] output_variables: ['x', 'pieces'] code_snippet: \nfunction [x,pieces] = encodeScalarString(tok,str)\npieces = split(str,tok.MaskToken);\nfulltok = tok.FullTokenizer;\nmaskCode = fulltok.encode(tok.MaskToken);\nx = [];\n\nfor i = 1:numel(pieces)\n    tokens = fulltok.tokenize(pieces(i));\n    if ~isempty(tokens{1})\n        % \"\" tokenizes to empty - awkward\n        x = cat(2,x,fulltok.encode(tokens{1}));\n    end scope_id: predictMaskedToken scope_type: script",
  "name: predictMaskedToken file_path: predictMaskedToken.m start_line: 1 end_line: 13 input_parameters: ['mdl', 'str'] code_snippet: function out = predictMaskedToken(mdl,str)\n% predictMaskedToken   Given a BERT language model, predict the most likely\n% tokens for masked tokens.\n%\n%   out = predictMaskedToken(mdl, text) returns the string out which\n%   replaces instances of mdl.Tokenizer.MaskToken in the string text with\n%   the most likely token according to the BERT model mdl.\n\n% Copyright 2021-2023 The MathWorks, Inc.\narguments\n    mdl {mustBeA(mdl,'struct')}\n    str {mustBeText}\nend scope_id: predictMaskedToken scope_type: script",
  "name: rebuildScalarString file_path: predictMaskedToken.m start_line: 55 end_line: 60 input_parameters: ['pieces', 'predictedTokens'] code_snippet: \nfunction out = rebuildScalarString(pieces,predictedTokens)\nout = \"\";\nfor i = 1:(numel(pieces)-1)\n    out = strcat(out,pieces(i),predictedTokens(i));\nend scope_id: predictMaskedToken scope_type: script",
  "name: predictMaskedToken file_path: predictMaskedToken.m start_line: 1 end_line: 62 code_snippet: function out = predictMaskedToken(mdl,str)\n% predictMaskedToken   Given a BERT language model, predict the most likely\n% tokens for masked tokens.\n%\n%   out = predictMaskedToken(mdl, text) returns the string out which\n%   replaces instances of mdl.Tokenizer.MaskToken in the string text with\n%   the most likely token according to the BERT model mdl.\n\n% Copyright 2021-2023 The MathWorks, Inc.\narguments\n    mdl {mustBeA(mdl,'struct')}\n    str {mustBeText}\nend\nstr = string(str);\ninSize = size(str);\nstr = str(1:end);\n[seqs,pieces] = arrayfun(@(s)encodeScalarString(mdl.Tokenizer,s),str,'UniformOutput',false);\nx = padsequences(seqs,2,'PaddingValue',mdl.Tokenizer.FullTokenizer.encode(mdl.Tokenizer.PaddingToken));\nmaskCode = mdl.Tokenizer.FullTokenizer.encode(mdl.Tokenizer.MaskToken);\nismask = x==maskCode;\nx = dlarray(x);\nprobs = bert.languageModel(x,mdl.Parameters);\nmaskedProbs = extractdata(probs(:,ismask));\n[~,sampleIdx] = max(maskedProbs,[],1);\npredictedTokens = mdl.Tokenizer.FullTokenizer.decode(sampleIdx);\nout = strings(numel(seqs),1);\nnumMaskPerSeq = sum(ismask,2);\nmaskStartIdx = 1;\nfor i = 1:numel(seqs)\n    startIdx = maskStartIdx;\n    maskStartIdx = maskStartIdx+numMaskPerSeq(i);\n    out(i) = rebuildScalarString(pieces{i},predictedTokens(startIdx:(startIdx+numMaskPerSeq(i)-1)));\nend\nout = reshape(out,inSize);\nend\n\nfunction [x,pieces] = encodeScalarString(tok,str)\npieces = split(str,tok.MaskToken);\nfulltok = tok.FullTokenizer;\nmaskCode = fulltok.encode(tok.MaskToken);\nx = [];\n\nfor i = 1:numel(pieces)\n    tokens = fulltok.tokenize(pieces(i));\n    if ~isempty(tokens{1})\n        % \"\" tokenizes to empty - awkward\n        x = cat(2,x,fulltok.encode(tokens{1}));\n    end\n    if i<numel(pieces)\n        x = cat(2,x,maskCode);\n    end\nend\nx = [fulltok.encode(tok.StartToken),x,fulltok.encode(tok.SeparatorToken)];\nend\n\nfunction out = rebuildScalarString(pieces,predictedTokens)\nout = \"\";\nfor i = 1:(numel(pieces)-1)\n    out = strcat(out,pieces(i),predictedTokens(i));\nend\nout = strcat(out,pieces(end));\nend scope_id: predictMaskedToken scope_type: script",
  "name: PredictMaskedTokensUsingFinBERT file_path: PredictMaskedTokensUsingFinBERT.m start_line: 1 end_line: 72 code_snippet: %% Predict Masked Tokens Using FinBERT\n% This example shows how to predict masked tokens using a pretrained\n% FinBERT model.\n%\n% BERT models are trained to perform various tasks. One of the tasks is\n% known as masked language modeling which is the task of predicting tokens\n% in text that have been replaced by a mask value.\n%\n% This example shows how to predict masked tokens for financial text data\n% and calculate the token probabilities using a pretrained FinBERT model.\n\n%% Load Pretrained FinBERT Model\n% Load a pretrained FinBERT model using the |finbert| function. For\n% language model workflows, set the \"Model\" option to \"language-model\". The\n% model consists of a tokenizer that encodes text as sequences of integers,\n% and a structure of parameters.\nmdl = finbert(\"Model\",\"language-model\")\n\n%%\n% View the FinBERT model tokenizer. The tokenizer encodes text as sequences\n% of integers and holds the details of padding, start, separator and mask\n% tokens.\ntokenizer = mdl.Tokenizer\n\n%% Predict Masked Token\n% Create a string containing a piece of text and replace a single word with\n% the tokenizer mask token.\nstr = \"Experts estimate the value of its remaining stake in the company at $ 27 million.\";\nstrMasked = replace(str,\"stake\",tokenizer.MaskToken)\n\n%%\n% Predict the masked token using the |predictMaskedToken| function. The\n% function returns the original string with the mask tokens replaced.\nsentencePred = predictMaskedToken(mdl,strMasked)\n\n%% Calculate Prediction Scores\n% To get the prediction scores for each word in the model vocabulary, you\n% can evaluate the language model directly using the |bert.languageModel|\n% function.\n\n%%\n% First, tokenize the input sentence with the FinBERT model tokenizer using\n% the |tokenize| function. Note that the tokenizer may split single words\n% and also prepends a [CLS] token and appends a [SEP] token to the input.\ntokens = tokenize(tokenizer,str);\ntokens{1}\n\n%%\n% Replace one of the tokens with the mask token.\nidx = 9;\ntokens{1}(idx) = tokenizer.MaskToken\n\n%%\n% Encode the tokens using the FinBERT model tokenizer using the\n% |encodeTokens| function.\nX = encodeTokens(tokenizer,tokens);\nX{1}\n\n%%\n% To get the predictions scores from for the encoded tokens, evaluate the\n% FinBERT language model directly using the |bert.languageModel| function.\n% The language model output is a VocabularySize-by-SequenceLength array.\nscores = bert.languageModel(X{1},mdl.Parameters);\n\n%%\n% View the tokens of the FinBERT model vocabulary corresponding to the top\n% 10 prediction scores for the mask token.\n[~,idxTop] = maxk(extractdata(scores(:,idx)),10);\ntbl = table;\ntbl.Token = arrayfun(@(x) decode(tokenizer,x), idxTop);\ntbl.Score = scores(idxTop,idx)\n scope_id: PredictMaskedTokensUsingFinBERT scope_type: script",
  "name: PredictMaskedTokensUsingBERT file_path: PredictMaskedTokensUsingBERT.m start_line: 1 end_line: 71 code_snippet: %% Predict Masked Tokens Using BERT\n% This example shows how to predict masked tokens using a pretrained BERT\n% model.\n%\n% BERT models are trained to perform various tasks. One of the tasks is\n% known as masked language modeling which is the task of predicting tokens\n% in text that have been replaced by a mask value.\n%\n% This example shows how to predict masked tokens for text data and\n% calculate the token probabilities using a pretrained BERT model.\n\n%% Load Pretrained BERT Model\n% Load a pretrained BERT model using the |bert| function. The model\n% consists of a tokenizer that encodes text as sequences of integers, and a\n% structure of parameters.\nmdl = bert\n\n%%\n% View the BERT model tokenizer. The tokenizer encodes text as sequences of\n% integers and holds the details of padding, start, separator and mask\n% tokens.\ntokenizer = mdl.Tokenizer\n\n%% Predict Masked Token\n% Create a string containing a piece of text and replace a single word with\n% the tokenizer mask token.\nstr = \"Text Analytics Toolbox includes tools for preprocessing raw text from sources such as equipment logs, news feeds, surveys, operator reports, and social media.\";\nstrMasked = replace(str,\"sources\",tokenizer.MaskToken)\n\n%%\n% Predict the masked token using the |predictMaskedToken| function. The\n% function returns the original string with the mask tokens replaced.\nsentencePred = predictMaskedToken(mdl,strMasked)\n\n%% Calculate Prediction Scores\n% To get the prediction scores for each word in the model vocabulary, you\n% can evaluate the language model directly using the |bert.languageModel|\n% function.\n\n%%\n% First, tokenize the input sentence with the BERT model tokenizer using\n% the |tokenize| function. Note that the tokenizer may split single words\n% and also prepends a [CLS] token and appends a [SEP] token to the input.\ntokens = tokenize(tokenizer,str);\ntokens{1}\n\n%%\n% Replace one of the tokens with the mask token.\nidx = 16;\ntokens{1}(idx) = tokenizer.MaskToken\n\n%%\n% Encode the tokens using the BERT model tokenizer using the |encodeTokens|\n% function.\nX = encodeTokens(tokenizer,tokens);\nX{1}\n\n%%\n% To get the predictions scores from for the encoded tokens, evaluate the\n% BERT language model directly using the |bert.languageModel| function. The\n% language model output is a VocabularySize-by-SequenceLength array.\nscores = bert.languageModel(X{1},mdl.Parameters);\n\n%%\n% View the tokens of the BERT model vocabulary corresponding to the top 10\n% prediction scores for the mask token.\n[~,idxTop] = maxk(extractdata(scores(:,idx)),10);\ntbl = table;\ntbl.Token = arrayfun(@(x) decode(tokenizer,x), idxTop);\ntbl.Score = scores(idxTop,idx)\n scope_id: PredictMaskedTokensUsingBERT scope_type: script",
  "name: preprocessMiniBatch file_path: FineTuneBERTJapanese.m start_line: 267 end_line: 273 input_parameters: ['X', 'T', 'paddingValue', 'maxSequenceLength'] output_variables: ['X', 'T'] code_snippet: function [X,T] = preprocessMiniBatch(X,T,paddingValue,maxSequenceLength)\n\nX = preprocessPredictors(X,paddingValue,maxSequenceLength);\nT = cat(2,T{:});\nT = onehotencode(T,1);\n\nend scope_id: FineTuneBERTJapanese scope_type: script",
  "name: modelGradients file_path: FineTuneBERTJapanese.m start_line: 327 end_line: 335 input_parameters: ['X', 'T', 'parameters'] output_variables: ['loss', 'gradients'] code_snippet: function [loss,gradients] = modelGradients(X,T,parameters)\n\ndropout = 0.1;\nY = model(X,parameters,dropout);\nY = softmax(Y,\"DataFormat\",\"CB\");\nloss = crossentropy(Y,T,\"DataFormat\",\"CB\");\ngradients = dlgradient(loss,parameters.Weights);\n\nend scope_id: FineTuneBERTJapanese scope_type: script",
  "name: preprocessPredictors file_path: FineTuneBERTJapanese.m start_line: 280 end_line: 285 input_parameters: ['X', 'paddingValue', 'maxSeqLen'] code_snippet: function X = preprocessPredictors(X,paddingValue,maxSeqLen)\n\nX = truncateSequences(X,maxSeqLen,SeparatorCode=4);\nX = padsequences(X,2,\"PaddingValue\",paddingValue);\n\nend scope_id: FineTuneBERTJapanese scope_type: script",
  "name: bertEmbed file_path: FineTuneBERTJapanese.m start_line: 291 end_line: 297 input_parameters: ['X', 'parameters', 'args'] code_snippet: function Y = bertEmbed(X,parameters,args)\n\narguments\n    X\n    parameters\n    args.DropoutProbability = 0\nend scope_id: FineTuneBERTJapanese scope_type: script",
  "name: model file_path: FineTuneBERTJapanese.m start_line: 313 end_line: 321 input_parameters: ['X', 'parameters', 'dropout'] code_snippet: function Y = model(X,parameters,dropout)\n\nY = bertEmbed(X,parameters,\"DropoutProbability\",dropout);\n\nweights = parameters.Weights.classifier.kernel;\nbias = parameters.Weights.classifier.bias;\nY = fullyconnect(Y,weights,bias,\"DataFormat\",\"CB\");\n\nend scope_id: FineTuneBERTJapanese scope_type: script",
  "name: modelPredictions file_path: FineTuneBERTJapanese.m start_line: 340 end_line: 357 input_parameters: ['parameters', 'mbq', 'classes'] code_snippet: function predictions = modelPredictions(parameters,mbq,classes)\n\npredictions = [];\n\ndropout = 0;\n\nreset(mbq);\n\nwhile hasdata(mbq)\n    \n    dlX = next(mbq);\n    dlYPred = model(dlX,parameters,dropout);\n    dlYPred = softmax(dlYPred,\"DataFormat\",\"CB\");\n    \n    YPred = onehotdecode(dlYPred,classes,1)';\n    \n    predictions = [predictions; YPred];\nend scope_id: FineTuneBERTJapanese scope_type: script",
  "name: FineTuneBERTJapanese file_path: FineTuneBERTJapanese.m start_line: 1 end_line: 359 code_snippet: %% Fine-Tune Pretrained BERT Model\n% This example shows how to fine-tune a pretrained BERT model for text\n% classification.\n%\n% To get the most out of a pretrained BERT model, you can retrain and\n% fine-tune the BERT parameters weights for your task.\n%\n% This example shows how to fine-tune a pretrained BERT model to classify\n% failure events given a data set of factory reports.\n\n%% Load Pretrained BERT Model\n% Load a pretrained BERT model using the |bert| function. The model\n% consists of a tokenizer that encodes text as sequences of integers, and a\n% structure of parameters.\nmdl = bert(Model=\"japanese-base\");\n\n%%\n% View the BERT model tokenizer. The tokenizer encodes text as sequences of\n% integers and holds the details of padding, start, separator and mask\n% tokens.\ntokenizer = mdl.Tokenizer\n\n%% Load Data\n% Load the example data. The file |factoryReportsJP.csv| contains factory\n% reports, including a text description and categorical labels for each\n% event.\n% The table contains these variables:\n% Var1 — Description\n% Var2 — Category\n% Var3 — Urgency\n% Var4 — Resolution\n% Var5 — Cost\n\nfilename = \"factoryReportsJP.csv\";\ndata = readtable(filename,\"TextType\",\"string\",\"ReadVariableNames\",false);\ndata.Properties.VariableNames = [\"Description\", \"Category\", ...\n    \"Urgency\", \"Resolution\", \"Cost\"];\nhead(data)\n\n\n%%\n% The goal of this example is to classify events by the label in the\n% |Category| column. To divide the data into classes, convert these labels\n% to categorical.\ndata.Category = categorical(data.Category);\n\n%%\n% View the number of classes.\nclasses = categories(data.Category);\nnumClasses = numel(classes)\n\n%%\n% View the distribution of the classes in the data using a histogram.\nfigure\nhistogram(data.Category);\nxlabel(\"Class\")\nylabel(\"Frequency\")\ntitle(\"Class Distribution\")\n\n%%\n% Encode the text data using the BERT model tokenizer using the |encode|\n% function and add the tokens to the training data table.\ndata.Tokens = encode(tokenizer, data.Description);\n\n%%\n% The next step is to partition it into sets for training and validation.\n% Partition the data into a training partition and a held-out partition for\n% validation and testing. Specify the holdout percentage to be 20%.\ncvp = cvpartition(data.Category,\"Holdout\",0.2);\ndataTrain = data(training(cvp),:);\ndataValidation = data(test(cvp),:);\n\n%%\n% View the number of training and validation observations.\nnumObservationsTrain = size(dataTrain,1)\nnumObservationsValidation = size(dataValidation,1)\n\n%%\n% Extract the training text data, labels, and encoded BERT tokens from the\n% partitioned tables.\ntextDataTrain = dataTrain.Description;\nTTrain = dataTrain.Category;\ntokensTrain = dataTrain.Tokens;\n\n%%\n% To check that you have imported the data correctly, visualize the\n% training text data using a word cloud.\n\nfigure\nwordcloud(textDataTrain);\ntitle(\"Training Data\")\n\n%% Prepare Data for Training\n% Convert the documents to feature vectors using the BERT model as a\n% feature extractor.\n\n% To extract the features of the training data by iterating over\n% mini-batches, create a |minibatchqueue| object.\n\n% Mini-batch queues require a single datastore that outputs both the\n% predictors and responses. Create array datastores containing the training\n% BERT tokens and labels and combine them using the |combine| function.\ndsXTrain = arrayDatastore(tokensTrain,\"OutputType\",\"same\");\ndsTTrain = arrayDatastore(TTrain);\ncdsTrain = combine(dsXTrain,dsTTrain);\n\n%% Initialize Model Parameters\n% Initialize the weights for the classifier to apply after the BERT\n% embedding.\noutputSize = mdl.Parameters.Hyperparameters.HiddenSize;\nmdl.Parameters.Weights.classifier.kernel = dlarray(randn(numClasses, outputSize));\nmdl.Parameters.Weights.classifier.bias = dlarray(zeros(numClasses, 1));\n\n%% Specify Training Options\n% Train for 4 epochs with a mini-batch size of 32. Train with a learning\n% rate of 0.00001.\nnumEpochs = 4;\nminiBatchSize = 32;\nlearnRate = 1e-5;\n\n%% Train Model\n% Fine tune the model parameters using a custom training loop.\n\n%%\n% Create a mini-batch queue for the training data. Preprocess the\n% mini-batches using the |preprocessMiniBatch| function, listed at the end\n% of the example and discard any partial mini-batches.\npaddingValue = mdl.Tokenizer.PaddingCode;\nmaxSequenceLength = mdl.Parameters.Hyperparameters.NumContext;\n\nmbqTrain = minibatchqueue(cdsTrain,2,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X,Y) preprocessMiniBatch(X,Y,paddingValue,maxSequenceLength), ...\n    \"PartialMiniBatch\",\"discard\");\n\n%%\n% Initialize training progress plot.\n% In 23a you can use trainingProgressMonitor\nfigure\nC = colororder;\nlineLossTrain = animatedline(\"Color\",C(2,:));\n\nylim([0 inf]);\nxlabel(\"Iteration\");\nylabel(\"Loss\");\n\n%%\n% Initialize parameters for the Adam optimizer.\ntrailingAvg = [];\ntrailingAvgSq = [];\n\n%% \n% Extract the model parameters from the pretrained BERT model.\nparameters = mdl.Parameters;\n\n%% \n% Train the model using a custom training loop.\n%\n% For each epoch, shuffle the mini-batch queue and loop over mini-batches\n% of data. At the end of each iteration, update the training progress plot.\n%\n% For each iteration:\n% * Read a mini-batch of data from the mini-batch queue. \n% * Evaluate the model gradients and loss using the |dlfeval| and\n%   |modelGradients| functions.  \n% * Update the network parameters using the |adamupdate| function.\n% * Update the training plot.\n\niteration = 0;\nstart = tic;\n\n% Loop over epochs.\nfor epoch = 1:numEpochs\n    \n    % Shuffle data.\n    shuffle(mbqTrain);\n    \n    % Loop over mini-batches\n    while hasdata(mbqTrain)\n        iteration = iteration + 1;\n        \n        % Read mini-batch of data.\n        [X,T] = next(mbqTrain);\n        \n        % Evaluate loss and gradients.\n        [loss,gradients] = dlfeval(@modelGradients,X,T,parameters);\n        \n        % Update model parameters.\n        [parameters.Weights,trailingAvg,trailingAvgSq] = adamupdate(parameters.Weights,gradients, ...\n            trailingAvg,trailingAvgSq,iteration,learnRate);\n        \n        % Update training plot.\n        loss = double(gather(extractdata(loss)));\n        addpoints(lineLossTrain,iteration,loss);\n        \n        D = duration(0,0,toc(start),'Format','hh:mm:ss');\n        title(\"Epoch: \" + epoch + \", Elapsed: \" + string(D))\n        drawnow\n    end\nend\n\n%% Test Network\n% Test the network using the held-out validation data.\n\n%%\n% Extract the encoded tokens and labels from the validation data table.\ntokensValidation = dataValidation.Tokens;\nTValidation = dataValidation.Category;\n\n%% \n% Create an array datastore containing the encoded tokens.\ndsXValidation = arrayDatastore(tokensValidation,\"OutputType\",\"same\");\n\n%%\n% Create a mini-batch queue for the validation data. Preprocess the\n% mini-batches using the |preprocessPredictors| function, listed at the end\n% of the example.\nmbqValidation = minibatchqueue(dsXValidation,1,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X) preprocessPredictors(X,paddingValue,maxSequenceLength));\n\n%% \n% Make predictions using the |modelPredictions| function, listed at the end\n% of the example, and display the results in a confusion matrix.\nYPredValidation = modelPredictions(parameters,mbqValidation,classes);\n\nfigure\nconfusionchart(TValidation,YPredValidation)\n\n%% Predict Using New Data\n% Classify the event type of three new reports.\n\n%%\n% Create a string array containing the new reports.\nreportsNew = [\n    \"クーラントがソーターの下に溜まっています。\"\n    \"ソーターは起動時にヒューズを飛ばします。\"\n    \"アセンブラから非常に大きなガタガタという音が聞こえます。\"];\n\n%%\n% Encode the text data as sequences of tokens using the BERT model\n% tokenizer.\ntokensNew = encode(tokenizer, reportsNew);\n\n%%\n% Create a mini-batch queue for the new data. Preprocess the mini-batches\n% using the |preprocessPredictors| function, listed at the end of the\n% example.\ndsXNew = arrayDatastore(tokensNew,\"OutputType\",\"same\");\n\nmbqNew = minibatchqueue(dsXNew,1,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X) preprocessPredictors(X,paddingValue,maxSequenceLength));\n\n%%\n% Make predictions using the |modelPredictions| function, listed at the end\n% of the example.\nYPredNew = modelPredictions(parameters,mbqNew,classes)\n\n%% Supporting Functions\n\n%%% Mini-batch Preprocessing Function.\n% The |preprocessMiniBatch| function preprocess the predictors using the\n% |preprocessPredictors| function and then encodes the labels as encoded\n% vectors. Use this preprocessing function to preprocess both predictors\n% and labels.\nfunction [X,T] = preprocessMiniBatch(X,T,paddingValue,maxSequenceLength)\n\nX = preprocessPredictors(X,paddingValue,maxSequenceLength);\nT = cat(2,T{:});\nT = onehotencode(T,1);\n\nend\n\n%%% Predictors Preprocessing Functions\n% The |preprocessPredictors| function truncates the mini-batches to have\n% the specified maximum sequence length, pads the sequences to have the\n% same length. Use this preprocessing function to preprocess the predictors\n% only.\nfunction X = preprocessPredictors(X,paddingValue,maxSeqLen)\n\nX = truncateSequences(X,maxSeqLen,SeparatorCode=4);\nX = padsequences(X,2,\"PaddingValue\",paddingValue);\n\nend\n\n%%% BERT Embedding Function\n% The |bertEmbed| function maps input data to embedding vectors and\n% optionally applies dropout using the \"DropoutProbability\" name-value\n% pair.\nfunction Y = bertEmbed(X,parameters,args)\n\narguments\n    X\n    parameters\n    args.DropoutProbability = 0\nend\n\ndropoutProbabilitiy = args.DropoutProbability;\n\nY = bert.model(X,parameters, ...\n    \"DropoutProb\",dropoutProbabilitiy, ...\n    \"AttentionDropoutProb\",dropoutProbabilitiy);\n\n% To return single feature vectors, return the first element.\nY = Y(:,1,:);\nY = squeeze(Y);\n\nend\n\n%%% Model Function\n% The function |model| performs a forward pass of the classification model.\nfunction Y = model(X,parameters,dropout)\n\nY = bertEmbed(X,parameters,\"DropoutProbability\",dropout);\n\nweights = parameters.Weights.classifier.kernel;\nbias = parameters.Weights.classifier.bias;\nY = fullyconnect(Y,weights,bias,\"DataFormat\",\"CB\");\n\nend\n\n%%% Model Gradients Function\n% The |modelGradients| function performs a forward pass of the\n% classification model and returns the model loss and gradients of the loss\n% with respect to the learnable parameters.\nfunction [loss,gradients] = modelGradients(X,T,parameters)\n\ndropout = 0.1;\nY = model(X,parameters,dropout);\nY = softmax(Y,\"DataFormat\",\"CB\");\nloss = crossentropy(Y,T,\"DataFormat\",\"CB\");\ngradients = dlgradient(loss,parameters.Weights);\n\nend\n\n%%% Model Predictions Function\n% The |modelPredictions| function makes predictions by iterating over\n% mini-batches of data.\nfunction predictions = modelPredictions(parameters,mbq,classes)\n\npredictions = [];\n\ndropout = 0;\n\nreset(mbq);\n\nwhile hasdata(mbq)\n    \n    dlX = next(mbq);\n    dlYPred = model(dlX,parameters,dropout);\n    dlYPred = softmax(dlYPred,\"DataFormat\",\"CB\");\n    \n    YPred = onehotdecode(dlYPred,classes,1)';\n    \n    predictions = [predictions; YPred];\nend\n\nend scope_id: FineTuneBERTJapanese scope_type: script",
  "name: SummarizeTextUsingTransformersExample file_path: SummarizeTextUsingTransformersExample.m start_line: 1 end_line: 30 code_snippet: %% Summarize Text Using Transformers\n% This example shows how to summarize a piece of text using GPT-2.\n% \n% Transformer networks such as GPT-2 can be used to summarize a piece of\n% text. The trained GPT-2 transformer can generate text given an initial\n% sequence of words as input. The model was trained on comments left on\n% various web pages and internet forums.\n% \n% Because lots of these comments themselves contain a summary indicated by\n% the statement \"TL;DR\" (Too long, didn't read), you can use the\n% transformer model to generate a summary by appending \"TL;DR\" to the input\n% text. The |generateSummary| function takes the input text, automatically\n% appends the string |\"TL;DR\"| and generates the summary.\n\n%% Load Transformer Model\n% Load the GPT-2 transformer model using the |gpt2| function.\n\nmdl = gpt2;\n\n%% Load Data\n% Extract the help text for the |eigs| function.\n\ninputText = help('eigs')\n\n%% Generate Summary\n% Summarize the text using the |generateSummary| function.\n\nrng('default')\nsummary = generateSummary(mdl,inputText)\n scope_id: SummarizeTextUsingTransformersExample scope_type: script",
  "name: finbert file_path: finbert.m start_line: 1 end_line: 15 input_parameters: ['nvp'] code_snippet: function mdl = finbert(nvp)\n% finbert   Pretrained FinBERT transformer model\n%   mdl = finbert loads a pretrained FinBERT model and downloads the model\n%   weights and vocab file if necessary.\n%\n%   mdl = finbert('Model', modelName) loads the FinBERT model specified by\n%   modelName. Supported values for modelName are \"sentiment-model\"\n%   (default) and \"language-model\".\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    nvp.Model (1,1) string {mustBeMember(nvp.Model,...\n        [\"language-model\";\n        \"sentiment-model\"])} = \"sentiment-model\"\nend scope_id: finbert scope_type: script",
  "name: finbert file_path: finbert.m start_line: 1 end_line: 28 code_snippet: function mdl = finbert(nvp)\n% finbert   Pretrained FinBERT transformer model\n%   mdl = finbert loads a pretrained FinBERT model and downloads the model\n%   weights and vocab file if necessary.\n%\n%   mdl = finbert('Model', modelName) loads the FinBERT model specified by\n%   modelName. Supported values for modelName are \"sentiment-model\"\n%   (default) and \"language-model\".\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    nvp.Model (1,1) string {mustBeMember(nvp.Model,...\n        [\"language-model\";\n        \"sentiment-model\"])} = \"sentiment-model\"\nend\n% Download the license file\nfinbert.internal.getSupportFilePath(nvp.Model,'finbert.RIGHTS');\nparams = finbert.load(nvp.Model);\n% Get the IgnoreCase hyperparameter, then remove it, downstream code\n% shouldn't need it.\nignoreCase = params.Hyperparameters.IgnoreCase;\n% Get vocab file path\nvocabFile = finbert.internal.getSupportFilePath(nvp.Model,\"vocab.txt\");\nparams.Hyperparameters = rmfield(params.Hyperparameters,'IgnoreCase');\nmdl = struct(...\n    'Tokenizer',bert.tokenizer.BERTTokenizer(vocabFile,'IgnoreCase',ignoreCase),...\n    'Parameters',params);\nend scope_id: finbert scope_type: script",
  "name: test_script_uses_entry file_path: test_script_uses_entry.m start_line: 1 end_line: 43 code_snippet: %% Entry Script for Testing Script -[USES]-> Variable Relationships\n% This script calls other scripts and uses variables from them\n\n% First, run the simple script to create some variables\nsimple_script;\n\n% Now use variables that were created in simple_script\n% Note: In MATLAB, variables from scripts persist in the workspace\ndisp(['Using x from simple_script: ', num2str(x)]);\ndisp(['Using y from simple_script: ', num2str(y)]);\ndisp(['Using result from simple_script: ', num2str(result)]);\n\n% Create new variables using the ones from simple_script\nnewResult = x * 2 + y * 3;\ndisp(['New calculation: ', num2str(newResult)]);\n\n% Use variables in a more complex calculation\nfinalResult = result + newResult + z;\ndisp(['Final result: ', num2str(finalResult)]);\n\n% Test with BERT model (if available)\ntry\n    mdl = bert;\n    disp('BERT model loaded successfully');\n\n    % Use model variables\n    if isfield(mdl, 'Tokenizer')\n        tokenizer = mdl.Tokenizer;\n        disp(['Tokenizer type: ', class(tokenizer)]);\n    end\n\n    if isfield(mdl, 'Parameters')\n        params = mdl.Parameters;\n        disp('Model parameters available');\n    end\ncatch ME\n    disp(['BERT model not available: ', ME.message]);\nend\n\n% Display workspace variables\ndisp('Workspace variables:');\nwhos\n scope_id: test_script_uses_entry scope_type: script",
  "name: truncateSequences file_path: truncateSequences.m start_line: 1 end_line: 29 input_parameters: ['z', 'maxSeqLen', 'nvp'] code_snippet: function z = truncateSequences(z,maxSeqLen,nvp)\n% truncateSequences   Truncates sequences with care not to remove special\n% tokens. For use with BERT models.\n% \n%   Z = truncateSequences(X,maxSeqLen) truncates an input array X to have\n%   sequence length maxSeqLen. The input X is of size\n%   1-by-SequenceLength-by-BatchSize. The truncation does not remove\n%   separator tokens - entries of X equal to 103. If X is an encoded\n%   sequence-pair then the first and second sequences are truncated to have\n%   similar lengths.\n% \n%   Z = truncateSequences(X,maxSeqLen,'SeparatorCode',sepCode) allows the\n%   usage of a custom separator code for inputs X that we encoded using a\n%   different technique to the default BERT tokenizer. The default value is\n%   103. See also the SeparatorCode property of a BERTTokenizer.\n%\n% Example:\n%   tokenizer = bert.tokenizer.BERTTokenizer();\n%   sequences = tokenizer.encode([\"Hello World!\",\"I am a model.\"]);\n%   truncatedSequences = truncateSequences(sequences,5)\n%   % Note that truncatedSequences each still start with 102 and end with\n%   103 - the encoded start and separator tokens.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    z\n    maxSeqLen (1,1) double {mustBeInteger,mustBeGreaterThan(maxSeqLen,2)}\n    nvp.SeparatorCode (1,1) double {mustBeInteger} = 103\nend scope_id: truncateSequences scope_type: script",
  "name: iTruncateScalarSequence file_path: truncateSequences.m start_line: 32 end_line: 36 input_parameters: ['z', 'maxSeqLen', 'sepCode'] code_snippet: \nfunction z = iTruncateScalarSequence(z,maxSeqLen,sepCode)\nif isa(z,'dlarray')\n    z = extractdata(z);\nend scope_id: truncateSequences scope_type: script",
  "name: iTruncateSingleSequence file_path: truncateSequences.m start_line: 44 end_line: 49 input_parameters: ['z', 'maxSeqLen'] code_snippet: \nfunction z = iTruncateSingleSequence(z,maxSeqLen)\nmaxSeqLen = min(maxSeqLen-2,numel(z)-2);\nindices = [1,2:(maxSeqLen+1),numel(z)];\nz = z(indices);\nend scope_id: truncateSequences scope_type: script",
  "name: iTruncateSequencePair file_path: truncateSequences.m start_line: 50 end_line: 65 input_parameters: ['z', 'idx', 'maxSeqLen'] code_snippet: \nfunction z = iTruncateSequencePair(z,idx,maxSeqLen)\nz1 = z(1:idx);\nz2 = z((idx+1):end);\nn1 = numel(z1);\nn2 = numel(z2);\nN = n1+n2;\nif N>maxSeqLen\n    delta = N-maxSeqLen;\n    if n1 > n2\n        n1 = n1-delta;\n        if n1 < n2\n            gap = ceil((n2-n1)/2);\n            n1 = n1+gap;\n            n2 = n2-gap;\n        end scope_id: truncateSequences scope_type: script",
  "name: truncateSequences file_path: truncateSequences.m start_line: 1 end_line: 78 code_snippet: function z = truncateSequences(z,maxSeqLen,nvp)\n% truncateSequences   Truncates sequences with care not to remove special\n% tokens. For use with BERT models.\n% \n%   Z = truncateSequences(X,maxSeqLen) truncates an input array X to have\n%   sequence length maxSeqLen. The input X is of size\n%   1-by-SequenceLength-by-BatchSize. The truncation does not remove\n%   separator tokens - entries of X equal to 103. If X is an encoded\n%   sequence-pair then the first and second sequences are truncated to have\n%   similar lengths.\n% \n%   Z = truncateSequences(X,maxSeqLen,'SeparatorCode',sepCode) allows the\n%   usage of a custom separator code for inputs X that we encoded using a\n%   different technique to the default BERT tokenizer. The default value is\n%   103. See also the SeparatorCode property of a BERTTokenizer.\n%\n% Example:\n%   tokenizer = bert.tokenizer.BERTTokenizer();\n%   sequences = tokenizer.encode([\"Hello World!\",\"I am a model.\"]);\n%   truncatedSequences = truncateSequences(sequences,5)\n%   % Note that truncatedSequences each still start with 102 and end with\n%   103 - the encoded start and separator tokens.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    z\n    maxSeqLen (1,1) double {mustBeInteger,mustBeGreaterThan(maxSeqLen,2)}\n    nvp.SeparatorCode (1,1) double {mustBeInteger} = 103\nend\nz = cellfun(@(z) iTruncateScalarSequence(z,maxSeqLen,nvp.SeparatorCode),z,'UniformOutput', false);\nend\n\nfunction z = iTruncateScalarSequence(z,maxSeqLen,sepCode)\nif isa(z,'dlarray')\n    z = extractdata(z);\nend\nidx = find(z==sepCode);\nif idx == numel(z)\n    z = iTruncateSingleSequence(z,maxSeqLen);\nelse\n    z = iTruncateSequencePair(z,idx,maxSeqLen);\nend\nend\n\nfunction z = iTruncateSingleSequence(z,maxSeqLen)\nmaxSeqLen = min(maxSeqLen-2,numel(z)-2);\nindices = [1,2:(maxSeqLen+1),numel(z)];\nz = z(indices);\nend\n\nfunction z = iTruncateSequencePair(z,idx,maxSeqLen)\nz1 = z(1:idx);\nz2 = z((idx+1):end);\nn1 = numel(z1);\nn2 = numel(z2);\nN = n1+n2;\nif N>maxSeqLen\n    delta = N-maxSeqLen;\n    if n1 > n2\n        n1 = n1-delta;\n        if n1 < n2\n            gap = ceil((n2-n1)/2);\n            n1 = n1+gap;\n            n2 = n2-gap;\n        end\n    else\n        n2 = n2-delta;\n        if n2 < n1\n            gap = ceil((n1-n2)/2);\n            n1 = n1-gap;\n            n2 = n2+gap;\n        end\n    end\n    z1 = z1([1:(n1-1),numel(z1)]);\n    z2 = z2([1:(n2-1),numel(z2)]);\nend\nz = [z1,z2];\nend scope_id: truncateSequences scope_type: script",
  "name: preprocessMiniBatch file_path: FineTuneBERT.m start_line: 257 end_line: 263 input_parameters: ['X', 'T', 'paddingValue', 'maxSequenceLength'] output_variables: ['X', 'T'] code_snippet: function [X,T] = preprocessMiniBatch(X,T,paddingValue,maxSequenceLength)\n\nX = preprocessPredictors(X,paddingValue,maxSequenceLength);\nT = cat(2,T{:});\nT = onehotencode(T,1);\n\nend scope_id: FineTuneBERT scope_type: script",
  "name: modelGradients file_path: FineTuneBERT.m start_line: 317 end_line: 325 input_parameters: ['X', 'T', 'parameters'] output_variables: ['loss', 'gradients'] code_snippet: function [loss,gradients] = modelGradients(X,T,parameters)\n\ndropout = 0.1;\nY = model(X,parameters,dropout);\nY = softmax(Y,\"DataFormat\",\"CB\");\nloss = crossentropy(Y,T,\"DataFormat\",\"CB\");\ngradients = dlgradient(loss,parameters.Weights);\n\nend scope_id: FineTuneBERT scope_type: script",
  "name: preprocessPredictors file_path: FineTuneBERT.m start_line: 270 end_line: 275 input_parameters: ['X', 'paddingValue', 'maxSeqLen'] code_snippet: function X = preprocessPredictors(X,paddingValue,maxSeqLen)\n\nX = truncateSequences(X,maxSeqLen);\nX = padsequences(X,2,\"PaddingValue\",paddingValue);\n\nend scope_id: FineTuneBERT scope_type: script",
  "name: bertEmbed file_path: FineTuneBERT.m start_line: 281 end_line: 287 input_parameters: ['X', 'parameters', 'args'] code_snippet: function Y = bertEmbed(X,parameters,args)\n\narguments\n    X\n    parameters\n    args.DropoutProbability = 0\nend scope_id: FineTuneBERT scope_type: script",
  "name: model file_path: FineTuneBERT.m start_line: 303 end_line: 311 input_parameters: ['X', 'parameters', 'dropout'] code_snippet: function Y = model(X,parameters,dropout)\n\nY = bertEmbed(X,parameters,\"DropoutProbability\",dropout);\n\nweights = parameters.Weights.classifier.kernel;\nbias = parameters.Weights.classifier.bias;\nY = fullyconnect(Y,weights,bias,\"DataFormat\",\"CB\");\n\nend scope_id: FineTuneBERT scope_type: script",
  "name: modelPredictions file_path: FineTuneBERT.m start_line: 330 end_line: 347 input_parameters: ['parameters', 'mbq', 'classes'] code_snippet: function predictions = modelPredictions(parameters,mbq,classes)\n\npredictions = [];\n\ndropout = 0;\n\nreset(mbq);\n\nwhile hasdata(mbq)\n    \n    dlX = next(mbq);\n    dlYPred = model(dlX,parameters,dropout);\n    dlYPred = softmax(dlYPred,\"DataFormat\",\"CB\");\n    \n    YPred = onehotdecode(dlYPred,classes,1)';\n    \n    predictions = [predictions; YPred];\nend scope_id: FineTuneBERT scope_type: script",
  "name: FineTuneBERT file_path: FineTuneBERT.m start_line: 1 end_line: 353 code_snippet: %% Fine-Tune Pretrained BERT Model\n% This example shows how to fine-tune a pretrained BERT model for text\n% classification.\n%\n% To get the most out of a pretrained BERT model, you can retrain and\n% fine-tune the BERT parameters weights for your task.\n%\n% This example shows how to fine-tune a pretrained BERT model to classify\n% failure events given a data set of factory reports.\n\n%% Load Pretrained BERT Model\n% Load a pretrained BERT model using the |bert| function. The model\n% consists of a tokenizer that encodes text as sequences of integers, and a\n% structure of parameters.\nmdl = bert\n\n%%\n% View the BERT model tokenizer. The tokenizer encodes text as sequences of\n% integers and holds the details of padding, start, separator and mask\n% tokens.\ntokenizer = mdl.Tokenizer\n\n%% Load Data\n% Load the example data. The file |factoryReports.csv| contains factory\n% reports, including a text description and categorical labels for each\n% event.\n\nfilename = \"factoryReports.csv\";\ndata = readtable(filename,\"TextType\",\"string\");\nhead(data)\n\n%%\n% The goal of this example is to classify events by the label in the\n% |Category| column. To divide the data into classes, convert these labels\n% to categorical.\ndata.Category = categorical(data.Category);\n\n%%\n% View the number of classes.\nclasses = categories(data.Category);\nnumClasses = numel(classes)\n\n%%\n% View the distribution of the classes in the data using a histogram.\nfigure\nhistogram(data.Category);\nxlabel(\"Class\")\nylabel(\"Frequency\")\ntitle(\"Class Distribution\")\n\n%%\n% Encode the text data using the BERT model tokenizer using the |encode|\n% function and add the tokens to the training data table.\ndata.Tokens = encode(tokenizer, data.Description);\n\n%%\n% The next step is to partition it into sets for training and validation.\n% Partition the data into a training partition and a held-out partition for\n% validation and testing. Specify the holdout percentage to be 20%.\ncvp = cvpartition(data.Category,\"Holdout\",0.2);\ndataTrain = data(training(cvp),:);\ndataValidation = data(test(cvp),:);\n\n%%\n% View the number of training and validation observations.\nnumObservationsTrain = size(dataTrain,1)\nnumObservationsValidation = size(dataValidation,1)\n\n%%\n% Extract the training text data, labels, and encoded BERT tokens from the\n% partitioned tables.\ntextDataTrain = dataTrain.Description;\nTTrain = dataTrain.Category;\ntokensTrain = dataTrain.Tokens;\n\n%%\n% To check that you have imported the data correctly, visualize the\n% training text data using a word cloud.\n\nfigure\nwordcloud(textDataTrain);\ntitle(\"Training Data\")\n\n%% Prepare Data for Training\n% Convert the documents to feature vectors using the BERT model as a\n% feature extractor.\n\n% To extract the features of the training data by iterating over\n% mini-batches, create a |minibatchqueue| object.\n\n% Mini-batch queues require a single datastore that outputs both the\n% predictors and responses. Create array datastores containing the training\n% BERT tokens and labels and combine them using the |combine| function.\ndsXTrain = arrayDatastore(tokensTrain,\"OutputType\",\"same\");\ndsTTrain = arrayDatastore(TTrain);\ncdsTrain = combine(dsXTrain,dsTTrain);\n\n%% Initialize Model Parameters\n% Initialize the weights for the classifier to apply after the BERT\n% embedding.\noutputSize = mdl.Parameters.Hyperparameters.HiddenSize;\nmdl.Parameters.Weights.classifier.kernel = dlarray(randn(numClasses, outputSize));\nmdl.Parameters.Weights.classifier.bias = dlarray(zeros(numClasses, 1));\n\n%% Specify Training Options\n% Train for 4 epochs with a mini-batch size of 32. Train with a learning\n% rate of 0.00001.\nnumEpochs = 4;\nminiBatchSize = 32;\nlearnRate = 1e-5;\n\n%% Train Model\n% Fine tune the model parameters using a custom training loop.\n\n%%\n% Create a mini-batch queue for the training data. Preprocess the\n% mini-batches using the |preprocessMiniBatch| function, listed at the end\n% of the example and discard any partial mini-batches.\npaddingValue = mdl.Tokenizer.PaddingCode;\nmaxSequenceLength = mdl.Parameters.Hyperparameters.NumContext;\n\nmbqTrain = minibatchqueue(cdsTrain,2,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X,Y) preprocessMiniBatch(X,Y,paddingValue,maxSequenceLength), ...\n    \"PartialMiniBatch\",\"discard\");\n\n%%\n% Initialize training progress plot.\nfigure\nC = colororder;\nlineLossTrain = animatedline(\"Color\",C(2,:));\n\nylim([0 inf]);\nxlabel(\"Iteration\");\nylabel(\"Loss\");\n\n%%\n% Initialize parameters for the Adam optimizer.\ntrailingAvg = [];\ntrailingAvgSq = [];\n\n%% \n% Extract the model parameters from the pretrained BERT model.\nparameters = mdl.Parameters;\n\n%% \n% Train the model using a custom training loop.\n%\n% For each epoch, shuffle the mini-batch queue and loop over mini-batches\n% of data. At the end of each iteration, update the training progress plot.\n%\n% For each iteration:\n% * Read a mini-batch of data from the mini-batch queue. \n% * Evaluate the model gradients and loss using the |dlfeval| and\n%   |modelGradients| functions.  \n% * Update the network parameters using the |adamupdate| function.\n% * Update the training plot.\n\niteration = 0;\nstart = tic;\n\n% Loop over epochs.\nfor epoch = 1:numEpochs\n    \n    % Shuffle data.\n    shuffle(mbqTrain);\n    \n    % Loop over mini-batches\n    while hasdata(mbqTrain)\n        iteration = iteration + 1;\n        \n        % Read mini-batch of data.\n        [X,T] = next(mbqTrain);\n        \n        % Evaluate loss and gradients.\n        [loss,gradients] = dlfeval(@modelGradients,X,T,parameters);\n        \n        % Update model parameters.\n        [parameters.Weights,trailingAvg,trailingAvgSq] = adamupdate(parameters.Weights,gradients, ...\n            trailingAvg,trailingAvgSq,iteration,learnRate);\n        \n        % Update training plot.\n        loss = double(gather(extractdata(loss)));\n        addpoints(lineLossTrain,iteration,loss);\n        \n        D = duration(0,0,toc(start),'Format','hh:mm:ss');\n        title(\"Epoch: \" + epoch + \", Elapsed: \" + string(D))\n        drawnow\n    end\nend\n\n%% Test Network\n% Test the network using the held-out validation data.\n\n%%\n% Extract the encoded tokens and labels from the validation data table.\ntokensValidation = dataValidation.Tokens;\nTValidation = dataValidation.Category;\n\n%% \n% Create an array datastore containing the encoded tokens.\ndsXValidation = arrayDatastore(tokensValidation,\"OutputType\",\"same\");\n\n%%\n% Create a mini-batch queue for the validation data. Preprocess the\n% mini-batches using the |preprocessPredictors| function, listed at the end\n% of the example.\nmbqValidation = minibatchqueue(dsXValidation,1,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X) preprocessPredictors(X,paddingValue,maxSequenceLength));\n\n%% \n% Make predictions using the |modelPredictions| function, listed at the end\n% of the example, and display the results in a confusion matrix.\nYPredValidation = modelPredictions(parameters,mbqValidation,classes);\n\nfigure\nconfusionchart(TValidation,YPredValidation)\n\n%% Predict Using New Data\n% Classify the event type of three new reports.\n\n%%\n% Create a string array containing the new reports.\nreportsNew = [ ...\n    \"Coolant is pooling underneath sorter.\"\n    \"Sorter blows fuses at start up.\"\n    \"There are some very loud rattling sounds coming from the assembler.\"];\n\n%%\n% Encode the text data as sequences of tokens using the BERT model\n% tokenizer.\ntokensNew = encode(tokenizer, reportsNew);\n\n%%\n% Create a mini-batch queue for the new data. Preprocess the mini-batches\n% using the |preprocessPredictors| function, listed at the end of the\n% example.\ndsXNew = arrayDatastore(tokensNew,\"OutputType\",\"same\");\n\nmbqNew = minibatchqueue(dsXNew,1,...\n    \"MiniBatchSize\",miniBatchSize, ...\n    \"MiniBatchFcn\",@(X) preprocessPredictors(X,paddingValue,maxSequenceLength));\n\n%%\n% Make predictions using the |modelPredictions| function, listed at the end\n% of the example.\nYPredNew = modelPredictions(parameters,mbqNew,classes)\n\n%% Supporting Functions\n\n%%% Mini-batch Preprocessing Function.\n% The |preprocessMiniBatch| function preprocess the predictors using the\n% |preprocessPredictors| function and then encodes the labels as encoded\n% vectors. Use this preprocessing function to preprocess both predictors\n% and labels.\nfunction [X,T] = preprocessMiniBatch(X,T,paddingValue,maxSequenceLength)\n\nX = preprocessPredictors(X,paddingValue,maxSequenceLength);\nT = cat(2,T{:});\nT = onehotencode(T,1);\n\nend\n\n%%% Predictors Preprocessing Functions\n% The |preprocessPredictors| function truncates the mini-batches to have\n% the specified maximum sequence length, pads the sequences to have the\n% same length. Use this preprocessing function to preprocess the predictors\n% only.\nfunction X = preprocessPredictors(X,paddingValue,maxSeqLen)\n\nX = truncateSequences(X,maxSeqLen);\nX = padsequences(X,2,\"PaddingValue\",paddingValue);\n\nend\n\n%%% BERT Embedding Function\n% The |bertEmbed| function maps input data to embedding vectors and\n% optionally applies dropout using the \"DropoutProbability\" name-value\n% pair.\nfunction Y = bertEmbed(X,parameters,args)\n\narguments\n    X\n    parameters\n    args.DropoutProbability = 0\nend\n\ndropoutProbabilitiy = args.DropoutProbability;\n\nY = bert.model(X,parameters, ...\n    \"DropoutProb\",dropoutProbabilitiy, ...\n    \"AttentionDropoutProb\",dropoutProbabilitiy);\n\n% To return single feature vectors, return the first element.\nY = Y(:,1,:);\nY = squeeze(Y);\n\nend\n\n%%% Model Function\n% The function |model| performs a forward pass of the classification model.\nfunction Y = model(X,parameters,dropout)\n\nY = bertEmbed(X,parameters,\"DropoutProbability\",dropout);\n\nweights = parameters.Weights.classifier.kernel;\nbias = parameters.Weights.classifier.bias;\nY = fullyconnect(Y,weights,bias,\"DataFormat\",\"CB\");\n\nend\n\n%%% Model Gradients Function\n% The |modelGradients| function performs a forward pass of the\n% classification model and returns the model loss and gradients of the loss\n% with respect to the learnable parameters.\nfunction [loss,gradients] = modelGradients(X,T,parameters)\n\ndropout = 0.1;\nY = model(X,parameters,dropout);\nY = softmax(Y,\"DataFormat\",\"CB\");\nloss = crossentropy(Y,T,\"DataFormat\",\"CB\");\ngradients = dlgradient(loss,parameters.Weights);\n\nend\n\n%%% Model Predictions Function\n% The |modelPredictions| function makes predictions by iterating over\n% mini-batches of data.\nfunction predictions = modelPredictions(parameters,mbq,classes)\n\npredictions = [];\n\ndropout = 0;\n\nreset(mbq);\n\nwhile hasdata(mbq)\n    \n    dlX = next(mbq);\n    dlYPred = model(dlX,parameters,dropout);\n    dlYPred = softmax(dlYPred,\"DataFormat\",\"CB\");\n    \n    YPred = onehotdecode(dlYPred,classes,1)';\n    \n    predictions = [predictions; YPred];\nend\n\nend\n\n\n\n scope_id: FineTuneBERT scope_type: script",
  "name: bert file_path: bert.m start_line: 1 end_line: 22 input_parameters: ['nvp'] code_snippet: function mdl = bert(nvp)\n% bert   Pretrained BERT transformer model\n%   mdl = bert loads a pretrained BERT-Base model and downloads the model\n%   weights and vocab file if necessary.\n%\n%   mdl = bert('Model', modelName) loads the BERT model specified by\n%   modelName.  Supported values for modelName are \"base\" (default),\n%   \"multilingual-cased\",\"medium\",\"small\",\"mini\", \"tiny\", \"japanese-base\", \n%   and \"japanese-base-wwm\"\n\n% Copyright 2021-2023 The MathWorks, Inc.\narguments\n    nvp.Model (1,1) string {mustBeMember(nvp.Model,[\n        \"base\"\n        \"multilingual-cased\"\n        \"medium\"\n        \"small\"\n        \"mini\"\n        \"tiny\"\n        \"japanese-base\"\n        \"japanese-base-wwm\"])} = \"base\"\nend scope_id: bert scope_type: script",
  "name: iJapaneseBERTModel file_path: bert.m start_line: 44 end_line: 65 input_parameters: ['modelName', 'zipFileName'] code_snippet: \nfunction mdl = iJapaneseBERTModel(modelName, zipFileName)\nzipFilePath = bert.internal.getSupportFilePath(modelName, zipFileName);\nmodelDir = fullfile(fileparts(zipFilePath), replace(zipFileName, \".zip\", \"\"));      \nunzip(zipFilePath, modelDir);\n% Build the tokenizer\nbtok = bert.tokenizer.internal.TokenizedDocumentTokenizer(\"Language\",\"ja\",\"TokenizeMethod\",\"mecab\",IgnoreCase=false);\nvocabFile = fullfile(modelDir, \"vocab.txt\");\nftok = bert.tokenizer.internal.FullTokenizer(vocabFile,BasicTokenizer=btok);\ntok = bert.tokenizer.BERTTokenizer(vocabFile,FullTokenizer=ftok);\n% Build the model\nparams.Weights = load(fullfile(modelDir, \"weights.mat\"));\nparams.Weights = dlupdate(@dlarray,params.Weights);\nparams.Hyperparameters = struct(...\n    NumHeads=12,...\n    NumLayers=12,...\n    NumContext=512,...\n    HiddenSize=768);\nmdl = struct(...\n    Tokenizer=tok,...\n    Parameters=params);\nend scope_id: bert scope_type: script",
  "name: bert file_path: bert.m start_line: 1 end_line: 65 code_snippet: function mdl = bert(nvp)\n% bert   Pretrained BERT transformer model\n%   mdl = bert loads a pretrained BERT-Base model and downloads the model\n%   weights and vocab file if necessary.\n%\n%   mdl = bert('Model', modelName) loads the BERT model specified by\n%   modelName.  Supported values for modelName are \"base\" (default),\n%   \"multilingual-cased\",\"medium\",\"small\",\"mini\", \"tiny\", \"japanese-base\", \n%   and \"japanese-base-wwm\"\n\n% Copyright 2021-2023 The MathWorks, Inc.\narguments\n    nvp.Model (1,1) string {mustBeMember(nvp.Model,[\n        \"base\"\n        \"multilingual-cased\"\n        \"medium\"\n        \"small\"\n        \"mini\"\n        \"tiny\"\n        \"japanese-base\"\n        \"japanese-base-wwm\"])} = \"base\"\nend\n\nswitch nvp.Model\n    case \"japanese-base\"\n        mdl = iJapaneseBERTModel(\"japanese-base\", \"bert-base-japanese.zip\");\n    case \"japanese-base-wwm\"\n        mdl = iJapaneseBERTModel(\"japanese-base-wwm\", \"bert-base-japanese-whole-word-masking.zip\");\n    otherwise\n        % Download the license file\n        bert.internal.getSupportFilePath(nvp.Model,\"bert.RIGHTS\");\n        params = bert.load(nvp.Model);\n        % Get the IgnoreCase hyperparameter, then remove it, downstream code\n        % shouldn't need it.\n        ignoreCase = params.Hyperparameters.IgnoreCase;\n        % Get vocab file\n        vocabFile = bert.internal.getSupportFilePath(nvp.Model,\"vocab.txt\");\n        params.Hyperparameters = rmfield(params.Hyperparameters,'IgnoreCase');\n        mdl = struct(...\n            'Tokenizer',bert.tokenizer.BERTTokenizer(vocabFile,'IgnoreCase',ignoreCase),...\n            'Parameters',params);\nend\nend\n\nfunction mdl = iJapaneseBERTModel(modelName, zipFileName)\nzipFilePath = bert.internal.getSupportFilePath(modelName, zipFileName);\nmodelDir = fullfile(fileparts(zipFilePath), replace(zipFileName, \".zip\", \"\"));      \nunzip(zipFilePath, modelDir);\n% Build the tokenizer\nbtok = bert.tokenizer.internal.TokenizedDocumentTokenizer(\"Language\",\"ja\",\"TokenizeMethod\",\"mecab\",IgnoreCase=false);\nvocabFile = fullfile(modelDir, \"vocab.txt\");\nftok = bert.tokenizer.internal.FullTokenizer(vocabFile,BasicTokenizer=btok);\ntok = bert.tokenizer.BERTTokenizer(vocabFile,FullTokenizer=ftok);\n% Build the model\nparams.Weights = load(fullfile(modelDir, \"weights.mat\"));\nparams.Weights = dlupdate(@dlarray,params.Weights);\nparams.Hyperparameters = struct(...\n    NumHeads=12,...\n    NumLayers=12,...\n    NumContext=512,...\n    HiddenSize=768);\nmdl = struct(...\n    Tokenizer=tok,...\n    Parameters=params);\nend scope_id: bert scope_type: script",
  "name: test_entry_script file_path: test_entry_script.m start_line: 1 end_line: 15 code_snippet: %% Test Entry Script for Cross-Scope Analysis\n% This script calls other scripts to test cross-scope relationship generation\n\n% Load BERT model\nmdl = bert;\n\n% Call other scripts\nbert;\npredictMaskedToken;\nfinbert;\n\n% Use variables from called scripts\nresult = mdl;\ntokenizer = mdl.Tokenizer;\n scope_id: test_entry_script scope_type: script",
  "name: gpt2 file_path: gpt2.m start_line: 1 end_line: 10 code_snippet: function mdl = gpt2()\n%GPT2 Pretrained GPT-2 transformer model\n%   mdl = gpt2 loads a pretrained GPT-2 transformer model and if necessary,\n%   downloads the model weights.\n\nmdl = struct;\nmdl.Tokenizer = gpt2.tokenizer.GPT2Tokenizer('gpt2-355M', '.');\nparamsStructFile = gpt2.internal.getSupportFilePath(\"gpt2_355M_params.mat\");\nmdl.Parameters = gpt2.load(paramsStructFile);\nend scope_id: gpt2 scope_type: script",
  "name: gpt2 file_path: gpt2.m start_line: 1 end_line: 11 code_snippet: function mdl = gpt2()\n%GPT2 Pretrained GPT-2 transformer model\n%   mdl = gpt2 loads a pretrained GPT-2 transformer model and if necessary,\n%   downloads the model weights.\n\nmdl = struct;\nmdl.Tokenizer = gpt2.tokenizer.GPT2Tokenizer('gpt2-355M', '.');\nparamsStructFile = gpt2.internal.getSupportFilePath(\"gpt2_355M_params.mat\");\nmdl.Parameters = gpt2.load(paramsStructFile);\nend\n scope_id: gpt2 scope_type: script",
  "name: iIsScalarString file_path: generateSummary.m start_line: 106 end_line: 114 input_parameters: ['s'] code_snippet: \nfunction iIsScalarString(s)\nvalidateattributes(s,{'string','char'},{});\nswitch class(s)\n    case \"string\"\n        validateattributes(s,{'string'},{'scalar'});\n    case \"char\"\n        validateattributes(s,{'char'},{'row'});\nend scope_id: generateSummary scope_type: script",
  "name: generateSummary file_path: generateSummary.m start_line: 1 end_line: 29 input_parameters: ['mdl', 'text', 'nameValueArguments'] code_snippet: function summary = generateSummary(mdl, text, nameValueArguments)\n% GENERATESUMMARY   Generate summary of text with GPT-2\n%\n%   summary = GENERATESUMMARY(mdl, text) generates a summary of the\n%   string or char array text using the transformer model mdl. The output\n%   summary is a char array.\n%\n%   summary = GENERATESUMMARY(mdl, text, 'PARAM1', 'VAL1', ...)\n%   specifies optional name/value pairs for creating the summary:\n%\n%   'MaxSummaryLength'      - The maximum number of tokens in the generated\n%                             summary. The default is 50.\n%   'TopK'                  - The number of tokens to sample from when\n%                             generating the summary. The default is 2.\n%   'Temperature'           - Temperature applied to the GPT-2 output\n%                             probability distribution. The default is 1.\n%   'StopCharacter'         - If the model generates this character its\n%                             summary is finished. The default is '.'.\n\n%   Copyright 2020 The MathWorks, Inc.\n\narguments\n    mdl\n    text                                      {iIsScalarString}\n    nameValueArguments.MaxSummaryLength (1,1) {mustBeInteger, mustBePositive} = 50\n    nameValueArguments.TopK             (1,1) {mustBeInteger, mustBePositive} = 2\n    nameValueArguments.Temperature      (1,1) {mustBePositive} = 1\n    nameValueArguments.StopCharacter          {iIsScalarString} = '.'\nend scope_id: generateSummary scope_type: script",
  "name: generateSummary file_path: generateSummary.m start_line: 1 end_line: 115 code_snippet: function summary = generateSummary(mdl, text, nameValueArguments)\n% GENERATESUMMARY   Generate summary of text with GPT-2\n%\n%   summary = GENERATESUMMARY(mdl, text) generates a summary of the\n%   string or char array text using the transformer model mdl. The output\n%   summary is a char array.\n%\n%   summary = GENERATESUMMARY(mdl, text, 'PARAM1', 'VAL1', ...)\n%   specifies optional name/value pairs for creating the summary:\n%\n%   'MaxSummaryLength'      - The maximum number of tokens in the generated\n%                             summary. The default is 50.\n%   'TopK'                  - The number of tokens to sample from when\n%                             generating the summary. The default is 2.\n%   'Temperature'           - Temperature applied to the GPT-2 output\n%                             probability distribution. The default is 1.\n%   'StopCharacter'         - If the model generates this character its\n%                             summary is finished. The default is '.'.\n\n%   Copyright 2020 The MathWorks, Inc.\n\narguments\n    mdl\n    text                                      {iIsScalarString}\n    nameValueArguments.MaxSummaryLength (1,1) {mustBeInteger, mustBePositive} = 50\n    nameValueArguments.TopK             (1,1) {mustBeInteger, mustBePositive} = 2\n    nameValueArguments.Temperature      (1,1) {mustBePositive} = 1\n    nameValueArguments.StopCharacter          {iIsScalarString} = '.'\nend\n\n% Unpack arguments\nmaxSummaryLength    = nameValueArguments.MaxSummaryLength;\ntopK                = nameValueArguments.TopK;\ntemperature         = nameValueArguments.Temperature;\nstopCharacter       = nameValueArguments.StopCharacter;\n\n% Remove newline tokens\ninputText = replace(char(text), newline, char.empty());\n\n% Get the GPT-2 tokenizer and model.\nenc = mdl.Tokenizer;\nparameters = mdl.Parameters;\n\n% To instruct the GPT-2 network to generate a summary, we append TL;DR to\n% the end of the text\ninputText = [inputText ' TL;DR'];\n\n% Encode some text\ninputTokens = enc.encode(inputText);\n\n% Ensure the text to be summarized fits within the context window of the\n% model\nif length(inputTokens) > (parameters.Hyperparameters.NumContext-maxSummaryLength-3)\n    inputTokens = inputTokens(1:(parameters.Hyperparameters.NumContext-maxSummaryLength-3));\n    inputText = enc.decode(inputTokens);\n    inputText = [inputText ' TL;DR'];\n    inputTokens = enc.encode(inputText);\nend\n\n% Initialize the cell array of pasts\npasts = cell(parameters.Hyperparameters.NumLayers,1);\n\n% Feed in the input except for the last token\n[~, presents] = gpt2.model( inputTokens(1:(end-1)), pasts, ...\n    parameters );\n\n% Initialize the previous token.\npreviousToken = inputTokens(end);\n\n% Initialize the summary\nsummary = [];\n\n% Generate the summary\nfor i = 1:maxSummaryLength\n    % Now run the model for another step\n    [logits, presents] = gpt2.model( ...\n        previousToken, presents, parameters );\n    \n    % Apply the temperature to the last logit\n    logits = logits./temperature;\n    \n    % Filter out all except the top K logits\n    logits = sampling.topKLogits(logits, topK);\n    \n    % Apply softmax to get probabilities.\n    probabilities = softmax(logits,'DataFormat','CTB');\n    \n    % Sample from a categorical distribution with the logits\n    nextToken = sampling.sampleFromCategorical(extractdata(probabilities));\n    \n    % Set the previous token for the next iteration\n    previousToken = nextToken;\n    \n    % Work out whether we need to start a new line\n    textToPrint = enc.decode(nextToken);    \n    \n    % Grow the summary array\n    summary = [summary textToPrint]; %#ok<AGROW>\n    \n    % Stop if we generate a stop character\n    if textToPrint == stopCharacter\n        break\n    end\nend\nend\n\nfunction iIsScalarString(s)\nvalidateattributes(s,{'string','char'},{});\nswitch class(s)\n    case \"string\"\n        validateattributes(s,{'string'},{'scalar'});\n    case \"char\"\n        validateattributes(s,{'char'},{'row'});\nend\nend scope_id: generateSummary scope_type: script",
  "name: SentimentAnalysisWithFinBERT file_path: SentimentAnalysisWithFinBERT.m start_line: 1 end_line: 50 code_snippet: %% Analyze Sentiment Using FinBERT\n% This example shows how to analyze sentiment of financial data using a\n% pretrained FinBERT model.\n%\n% FinBERT is a sentiment analysis model trained on financial text data and\n% fine-tuned for sentiment analysis. The model is based on the BERT-Base\n% architecture.\n%\n% This example shows how to classify the sentiment of financial news\n% reports using a pretrained FinBERT model.\n\n%% Load Pretrained FinBERT Model\n% Load a pretrained FinBERT model using the |finbert| function. The model\n% consists of a tokenizer that encodes text as sequences of integers, and\n% a structure of parameters.\nmdl = finbert\n\n%%\n% View the FinBERT model tokenizer. The tokenizer encodes text as sequences\n% of integers and holds the details of padding, start, separator and mask\n% tokens.\ntokenizer = mdl.Tokenizer\n\n%% Analyze Sentiment in Text\n% Create an string array of text data.\nstr = [\n    \"In an unprecendented move the stock market has hit new records today following news of a new vaccine.\"\n    \"Businesses in this sector suffer dramatic losses on the back of the pandemic.\"\n    \"The ship unloader is totally enclosed along the entire conveying line to the storage facilities.\"\n    \"Experts estimate the value of its remaining stake in the company at $ 27 million.\"\n    \"The company said that sales in the three months to the end of March slid to EUR86 .4 m US$ 113.4 m from EUR91 .2 m last year.\"\n    \"Finance experts calculate that it has lost EUR 4mn in the failed project.\"\n    \"They signed a large deal with an international industrial group which will deliver automation solutions and connectivity services.\"\n    \"Operating profit rose to EUR 5mn from EUR 2.8 mn in the fourth quarter of 2008\"];\n\n%%\n% Encode the text data as a sequence of tokens using the FinBERT model\n% tokenizer.\ntokens = encode(tokenizer,str);\n\n%% \n% Pad the sequences to have the same length using the |padsequences|\n% function. Specify the padding value to match the FinBERT model tokenizer.\nX = padsequences(tokens,2,\"PaddingValue\",mdl.Tokenizer.PaddingCode);\n\n%% \n% Evaluate the sentiment and sentiment scores using the\n% |finbert.sentimentModel| function.\n[sentiment,scores] = finbert.sentimentModel(X,mdl.Parameters)\n scope_id: SentimentAnalysisWithFinBERT scope_type: script",
  "name: bert.languageModel file_path: +bert/languageModel.m start_line: 1 end_line: 14 input_parameters: ['x', 'p'] code_snippet: function z = languageModel(x,p)\n% languageModel   The BERT language model.\n%\n%   Z = bert.languageModel(X,parameters) performs inference with a BERT model\n%   on the input X, and applies the output layer projection onto the\n%   associated vocabulary. The input X is a 1-by-numInputTokens-by-numObs\n%   array of encoded tokens. The return is an array Z of size\n%   vocabularySize-by-numInputTokens-by-numObs. In particular the language model is\n%   trained to predict a reasonable word for each masked input token.\n\n% Copyright 2021 The MathWorks, Inc.\nif ~isfield(p.Weights,'masked_LM')\n    error(\"bert:languageModel:MissingLMWeights\",\"Parameters do not include masked_LM weights\");\nend scope_id: languageModel scope_type: script",
  "name: languageModel file_path: +bert/languageModel.m start_line: 1 end_line: 17 code_snippet: function z = languageModel(x,p)\n% languageModel   The BERT language model.\n%\n%   Z = bert.languageModel(X,parameters) performs inference with a BERT model\n%   on the input X, and applies the output layer projection onto the\n%   associated vocabulary. The input X is a 1-by-numInputTokens-by-numObs\n%   array of encoded tokens. The return is an array Z of size\n%   vocabularySize-by-numInputTokens-by-numObs. In particular the language model is\n%   trained to predict a reasonable word for each masked input token.\n\n% Copyright 2021 The MathWorks, Inc.\nif ~isfield(p.Weights,'masked_LM')\n    error(\"bert:languageModel:MissingLMWeights\",\"Parameters do not include masked_LM weights\");\nend\nz = bert.model(x,p);\nz = bert.layer.languageModelHead(z,p.Weights.masked_LM,p.Weights.embeddings.word_embeddings);\nend scope_id: languageModel scope_type: script",
  "name: bert.load file_path: +bert/load.m start_line: 1 end_line: 10 input_parameters: ['modelName'] code_snippet: function params = load(modelName)\n% load   Load BERT parameters\n%\n%   parameters = load(modelName) will load the model weights associated to\n%   modelName and the hyperparameters associated to that model.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string\nend scope_id: load scope_type: script",
  "name: load file_path: +bert/load.m start_line: 1 end_line: 19 code_snippet: function params = load(modelName)\n% load   Load BERT parameters\n%\n%   parameters = load(modelName) will load the model weights associated to\n%   modelName and the hyperparameters associated to that model.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string\nend\n\n% the URLs don't have bert- at the start.\nparamsStructFile = bert.internal.getSupportFilePath(modelName,\"parameters.mat\");\nparamsStruct = load(paramsStructFile);\n\nparams = struct(...\n    'Hyperparameters',paramsStruct.Hyperparameters,...\n    'Weights',bert.internal.createParameterStruct(paramsStruct.Weights));\nend scope_id: load scope_type: script",
  "name: bert.mustBeLessThanOrEqualNumLayers file_path: +bert/model.m start_line: 96 end_line: 99 input_parameters: ['x', 'params'] code_snippet: \nfunction mustBeLessThanOrEqualNumLayers(x,params)\nmustBeLessThanOrEqual(x,params.Hyperparameters.NumLayers);\nend scope_id: model scope_type: script",
  "name: bert.mustBeALogicalOrDlarrayLogical file_path: +bert/model.m start_line: 100 end_line: 104 input_parameters: ['val'] code_snippet: \nfunction mustBeALogicalOrDlarrayLogical(val)\nif isa(val,'dlarray')\n    val = extractdata(val);\nend scope_id: model scope_type: script",
  "name: bert.mustBeNumericDlarray file_path: +bert/model.m start_line: 107 end_line: 111 input_parameters: ['val'] code_snippet: \nfunction mustBeNumericDlarray(val)\nmustBeA(val,'dlarray');\nmustBeNumeric(extractdata(val));\nend scope_id: model scope_type: script",
  "name: bert.model file_path: +bert/model.m start_line: 1 end_line: 60 input_parameters: ['x', 'parameters', 'nvp'] code_snippet: function varargout = model(x,parameters,nvp)\n% model   A BERT model forward pass.\n%\n%   Z = model(X,parameters) performs inference with a BERT model on the\n%   input X. X is a 1-by-numInputTokens-by-numObs array of encoded tokens.\n%   The return is an array Z of size\n%   (NumHeads*HeadSize)-by-numInputTokens-by-numObs. Each\n%   Z(:,i,j) corresponds to the BERT embedding of input token X(1,i,j).\n%\n%   Z = model(X,parameters,'PARAM1', VAL1, 'PARAM2', VAL2, ...) specifies\n%   the optional parameter name/value pairs:\n%\n%      'InputMask'             - A logical mask with the same size as X.\n%                                The mask should be false at indices \n%                                (i,j,k) for which X(i,j,k) corresponds to\n%                                padding, and true elsewhere. The default\n%                                is empty, for which the padding is\n%                                inferred by the entries of X that match\n%                                the PaddingCode name-value pair.\n%\n%      'DropoutProb'           - The probability of dropout for the output\n%                                activation. It is standard to set this to a\n%                                non-zero value during training, for example\n%                                0.1. The default is 0.\n%\n%      'AttentionDropoutProb'  - The probability of dropout used in the\n%                                attention layer. It is standard to set\n%                                this to a non-zero value during training,\n%                                for example 0.1. The default is 0.\n%\n%      'Outputs'               - Specify the indices of the layers to\n%                                return outputs from as a vector of\n%                                positive integers, or 'last' to specify\n%                                the final encoder layer only. The default\n%                                is 'last'.\n%\n%      'SeparatorCode'         - The positive integer corresponding to the\n%                                separator token. The default is 103, as\n%                                specified in the default BERT vocab.txt.\n%\n%      'PaddingCode'           - The positive integer corresponding to the\n%                                padding token. The default is 1, as\n%                                specified in the default BERT vocab.txt.\n%\n% References:\n% [1] https://arxiv.org/abs/1810.04805\n% [2] https://github.com/google-research/bert/\n\n% Copyright 2021 The MathWorks, Inc.\n\narguments\n    x dlarray {mustBeNumericDlarray,mustBeNonempty}\n    parameters {mustBeA(parameters,'struct')}\n    nvp.InputMask {mustBeNumericOrLogical} = logical.empty()\n    nvp.DropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.DropoutProb,1),mustBeNumeric} = 0\n    nvp.AttentionDropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.AttentionDropoutProb,1),mustBeNumeric} = 0\n    nvp.Outputs {mustBePositive,mustBeLessThanOrEqualNumLayers(nvp.Outputs,parameters),mustBeInteger,mustBeNumeric} = parameters.Hyperparameters.NumLayers\n    nvp.PaddingCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 1\n    nvp.SeparatorCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 103\nend scope_id: model scope_type: script",
  "name: bert.extractdata file_path: +bert/model.m start_line: 96 end_line: 99 input_parameters: ['val'] code_snippet: \nfunction mustBeLessThanOrEqualNumLayers(x,params)\nmustBeLessThanOrEqual(x,params.Hyperparameters.NumLayers);\nend scope_id: model scope_type: script",
  "name: model file_path: +bert/model.m start_line: 1 end_line: 111 code_snippet: function varargout = model(x,parameters,nvp)\n% model   A BERT model forward pass.\n%\n%   Z = model(X,parameters) performs inference with a BERT model on the\n%   input X. X is a 1-by-numInputTokens-by-numObs array of encoded tokens.\n%   The return is an array Z of size\n%   (NumHeads*HeadSize)-by-numInputTokens-by-numObs. Each\n%   Z(:,i,j) corresponds to the BERT embedding of input token X(1,i,j).\n%\n%   Z = model(X,parameters,'PARAM1', VAL1, 'PARAM2', VAL2, ...) specifies\n%   the optional parameter name/value pairs:\n%\n%      'InputMask'             - A logical mask with the same size as X.\n%                                The mask should be false at indices \n%                                (i,j,k) for which X(i,j,k) corresponds to\n%                                padding, and true elsewhere. The default\n%                                is empty, for which the padding is\n%                                inferred by the entries of X that match\n%                                the PaddingCode name-value pair.\n%\n%      'DropoutProb'           - The probability of dropout for the output\n%                                activation. It is standard to set this to a\n%                                non-zero value during training, for example\n%                                0.1. The default is 0.\n%\n%      'AttentionDropoutProb'  - The probability of dropout used in the\n%                                attention layer. It is standard to set\n%                                this to a non-zero value during training,\n%                                for example 0.1. The default is 0.\n%\n%      'Outputs'               - Specify the indices of the layers to\n%                                return outputs from as a vector of\n%                                positive integers, or 'last' to specify\n%                                the final encoder layer only. The default\n%                                is 'last'.\n%\n%      'SeparatorCode'         - The positive integer corresponding to the\n%                                separator token. The default is 103, as\n%                                specified in the default BERT vocab.txt.\n%\n%      'PaddingCode'           - The positive integer corresponding to the\n%                                padding token. The default is 1, as\n%                                specified in the default BERT vocab.txt.\n%\n% References:\n% [1] https://arxiv.org/abs/1810.04805\n% [2] https://github.com/google-research/bert/\n\n% Copyright 2021 The MathWorks, Inc.\n\narguments\n    x dlarray {mustBeNumericDlarray,mustBeNonempty}\n    parameters {mustBeA(parameters,'struct')}\n    nvp.InputMask {mustBeNumericOrLogical} = logical.empty()\n    nvp.DropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.DropoutProb,1),mustBeNumeric} = 0\n    nvp.AttentionDropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.AttentionDropoutProb,1),mustBeNumeric} = 0\n    nvp.Outputs {mustBePositive,mustBeLessThanOrEqualNumLayers(nvp.Outputs,parameters),mustBeInteger,mustBeNumeric} = parameters.Hyperparameters.NumLayers\n    nvp.PaddingCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 1\n    nvp.SeparatorCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 103\nend\n\nnvp.Outputs = nvp.Outputs(:);\nnargoutchk(0,numel(nvp.Outputs));\n\nmaxLayer = max(nvp.Outputs);\n\nw = parameters.Weights;\nhyperparameters = parameters.Hyperparameters;\n\n% Identify padding tokens - if InputMask is set just use it.\nif isempty(nvp.InputMask)\n    inputMask = x~=nvp.PaddingCode;\nelse\n    assert(isequal(size(nvp.InputMask),size(x)),\"bert:model:InvalidMaskSize\",\"Expected InputMask to have same size as input X.\");\n    inputMask = logical(nvp.InputMask);\nend\n\n% Assuming CTB format of x.\nxsz = size(x);\nseq_len = xsz(2);\n\n% Apply embeddings\ntypes = dlarray(bert.internal.inferTypeID(x,nvp.SeparatorCode));\npositions = dlarray(1:seq_len);\nz = bert.layer.embedding(x,types,positions,w.embeddings,nvp.DropoutProb);\n\n% Transformer layers\nnum_layers = min(hyperparameters.NumLayers,maxLayer);\nvarargout = cell(numel(nvp.Outputs),1);\nfor i = 1:num_layers\n    z = bert.layer.block(z,w.encoder_layers.(\"layer_\"+i),hyperparameters,'InputMask',inputMask);\n    toAssign = nvp.Outputs==i;\n    varargout(toAssign) = repelem({z},sum(toAssign));\nend\nend\n\nfunction mustBeLessThanOrEqualNumLayers(x,params)\nmustBeLessThanOrEqual(x,params.Hyperparameters.NumLayers);\nend\n\nfunction mustBeALogicalOrDlarrayLogical(val)\nif isa(val,'dlarray')\n    val = extractdata(val);\nend\nmustBeA(val,'logical');\nend\n\nfunction mustBeNumericDlarray(val)\nmustBeA(val,'dlarray');\nmustBeNumeric(extractdata(val));\nend scope_id: model scope_type: script",
  "name: sampling.sampleFromCategorical file_path: +sampling/sampleFromCategorical.m start_line: 1 end_line: 20 input_parameters: ['probabilities'] code_snippet: function sample = sampleFromCategorical(probabilities)\n% sampleFromCategorical   Sample from a categorical distribution\n%\n%   sample = sampleFromCategorical(probabilities) returns an index sampled\n%   from the categorical distribution represented by the input\n%   probabilities.\n%\n%   Input:\n%       probabilities   - A numClasses-by-1 vector of probabilities. The\n%                         elements of this vector should sum to 1.\n%\n%   Output:\n%       sample          - A number between 1 and numClasses that is sampled\n%                         from the input probabilities.\n\ncdf = cumsum(probabilities);\nsample = find( cdf > rand );\nsample = sample(1);\n\nend scope_id: sampleFromCategorical scope_type: script",
  "name: sampleFromCategorical file_path: +sampling/sampleFromCategorical.m start_line: 1 end_line: 20 code_snippet: function sample = sampleFromCategorical(probabilities)\n% sampleFromCategorical   Sample from a categorical distribution\n%\n%   sample = sampleFromCategorical(probabilities) returns an index sampled\n%   from the categorical distribution represented by the input\n%   probabilities.\n%\n%   Input:\n%       probabilities   - A numClasses-by-1 vector of probabilities. The\n%                         elements of this vector should sum to 1.\n%\n%   Output:\n%       sample          - A number between 1 and numClasses that is sampled\n%                         from the input probabilities.\n\ncdf = cumsum(probabilities);\nsample = find( cdf > rand );\nsample = sample(1);\n\nend scope_id: sampleFromCategorical scope_type: script",
  "name: sampling.topKLogits file_path: +sampling/topKLogits.m start_line: 1 end_line: 21 input_parameters: ['logits', 'topK'] code_snippet: function logits = topKLogits(logits, topK)\n% topKLogits   Return the top K logits\n%\n%   logits = topKLogits(logits, k) will return a vector of logits where any\n%   classes that are not in the top K largest values will be supressed.\n%   Values are supressed by setting them to large negative values.\n%\n%   Inputs:\n%       logits  - A numClasses-by-1 vector of logits.\n%       k       - The number of values to 'keep'. Everything outside of the\n%                 top K values will be supressed. Note for many typical use\n%                 cases this parameter can have a big effect.\n%\n%   Outputs:\n%       logits  - A numClasses-by-1 vector of logits.\n\nif isa(logits, 'dlarray')\n    extractedLogits = extractdata(logits);\nelse\n    extractedLogits = logits;\nend scope_id: topKLogits scope_type: script",
  "name: topKLogits file_path: +sampling/topKLogits.m start_line: 1 end_line: 36 code_snippet: function logits = topKLogits(logits, topK)\n% topKLogits   Return the top K logits\n%\n%   logits = topKLogits(logits, k) will return a vector of logits where any\n%   classes that are not in the top K largest values will be supressed.\n%   Values are supressed by setting them to large negative values.\n%\n%   Inputs:\n%       logits  - A numClasses-by-1 vector of logits.\n%       k       - The number of values to 'keep'. Everything outside of the\n%                 top K values will be supressed. Note for many typical use\n%                 cases this parameter can have a big effect.\n%\n%   Outputs:\n%       logits  - A numClasses-by-1 vector of logits.\n\nif isa(logits, 'dlarray')\n    extractedLogits = extractdata(logits);\nelse\n    extractedLogits = logits;\nend\n[~,classRanks] = sort(extractedLogits, 1, 'descend');\n\nnotTopKIndices = ( (topK+1):size(classRanks,1) )';\nnotTopKRows = classRanks(notTopKIndices,:);\nnotTopKColumns = repmat(1:size(extractedLogits,2),size(notTopKIndices,1),1);\n\nnotTopKIndices = sub2ind(size(extractedLogits), notTopKRows, notTopKColumns);\n\n% We want to make sure that when softmax is applied to the logits, the\n% probability for the classes that are not in the top K are zero. We do\n% this by setting entries that are not in the top K to large negative\n% values.\nlogits( notTopKIndices ) = -1e10;\n\nend scope_id: topKLogits scope_type: script",
  "name: canConstructModelWithDefault file_path: test/tbert.m start_line: 16 end_line: 20 input_parameters: ['test'] code_snippet:         \n        function canConstructModelWithDefault(test)\n            % Verify the default model can be constructed.\n            test.verifyWarningFree(@() bert());\n        end scope_id: tbert scope_type: script",
  "name: canConstructAllModels file_path: test/tbert.m start_line: 21 end_line: 25 input_parameters: ['test', 'AllModels'] code_snippet: \n        function canConstructAllModels(test, AllModels)\n            % Verify the all available models can be constructed.\n            test.verifyWarningFree(@() bert('Model', AllModels));\n        end scope_id: tbert scope_type: script",
  "name: canConstructModelWithNVPAndVerifyDefault file_path: test/tbert.m start_line: 26 end_line: 32 input_parameters: ['test'] code_snippet:            \n        function canConstructModelWithNVPAndVerifyDefault(test)\n            % Verify the default model matches the default model.\n            mdl = test.verifyWarningFree(@() bert('Model', \"base\"));\n            mdlDefault = bert();\n            test.verifyThat(mdl, iIsEqualTo(mdlDefault));\n        end scope_id: tbert scope_type: script",
  "name: checkBertIgnoreCase file_path: test/tbert.m start_line: 33 end_line: 44 input_parameters: ['test', 'UncasedVersion'] code_snippet:         \n        function checkBertIgnoreCase(test, UncasedVersion)\n            % Test that bert() is case insensitive.\n            \n            txt = \"Nipson anomemata. Memonan opsin.\";\n            \n            mdl = bert('Model', UncasedVersion);\n            yLower = mdl.Tokenizer.encode(txt);\n            yUpper = mdl.Tokenizer.encode(upper(txt));\n            \n            test.verifyThat(yLower, iIsEqualTo(yUpper));            \n        end scope_id: tbert scope_type: script",
  "name: multicasedVersionIsCaseSensitive file_path: test/tbert.m start_line: 45 end_line: 56 input_parameters: ['test'] code_snippet:         \n        function multicasedVersionIsCaseSensitive(test)\n            % Check that the multicased version is case sensitive.\n            \n            txt = \"Nipson anomemata. Memonan opsin.\";\n            \n            mdl = bert('Model', \"multilingual-cased\");\n            yLower = mdl.Tokenizer.encode(txt);\n            yUpper = mdl.Tokenizer.encode(upper(txt));\n            \n            test.verifyThat(yLower, iIsNotEqualTo(yUpper));\n        end scope_id: tbert scope_type: script",
  "name: canDoNSP file_path: test/tbert.m start_line: 57 end_line: 70 input_parameters: ['test'] code_snippet:         \n        function canDoNSP(test)\n            % Verify the next-sentence prediction works.\n            mdl = bert();\n            text_a = \"MATLAB combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly.\";\n            text_b_next = \"It includes the Live Editor for creating scripts that combine code, output, and formatted text in an executable notebook.\";\n            text_b_random = \"This is just a test, no need to be alarmed.\";\n            x = mdl.Tokenizer.encode([text_a;text_a],[text_b_next;text_b_random]);\n            x = padsequences(x,2,'PaddingValue',mdl.Tokenizer.PaddingCode);\n            prediction = bert.layer.classifierHead(bert.model(x,mdl.Parameters),mdl.Parameters.Weights.pooler,mdl.Parameters.Weights.sequence_relation);\n            probability = softmax(prediction,'DataFormat','CB');\n            classes = onehotdecode(probability,[\"IsNext\",\"IsNotNext\"],1);\n            test.verifyEqual(classes,categorical([\"IsNext\",\"IsNotNext\"]));\n        end scope_id: tbert scope_type: script",
  "name: bert file_path: test/tbert.m start_line: 57 end_line: 70 code_snippet:         \n        function canDoNSP(test)\n            % Verify the next-sentence prediction works.\n            mdl = bert();\n            text_a = \"MATLAB combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly.\";\n            text_b_next = \"It includes the Live Editor for creating scripts that combine code, output, and formatted text in an executable notebook.\";\n            text_b_random = \"This is just a test, no need to be alarmed.\";\n            x = mdl.Tokenizer.encode([text_a;text_a],[text_b_next;text_b_random]);\n            x = padsequences(x,2,'PaddingValue',mdl.Tokenizer.PaddingCode);\n            prediction = bert.layer.classifierHead(bert.model(x,mdl.Parameters),mdl.Parameters.Weights.pooler,mdl.Parameters.Weights.sequence_relation);\n            probability = softmax(prediction,'DataFormat','CB');\n            classes = onehotdecode(probability,[\"IsNext\",\"IsNotNext\"],1);\n            test.verifyEqual(classes,categorical([\"IsNext\",\"IsNotNext\"]));\n        end scope_id: tbert scope_type: script",
  "name: iIsEqualTo file_path: test/tbert.m start_line: 73 end_line: 76 input_parameters: ['varargin'] code_snippet: \nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend scope_id: tbert scope_type: script",
  "name: iIsNotEqualTo file_path: test/tbert.m start_line: 77 end_line: 80 input_parameters: ['varargin'] code_snippet: \nfunction constraint = iIsNotEqualTo(varargin)\nconstraint = ~matlab.unittest.constraints.IsEqualTo( varargin{:} );\nend scope_id: tbert scope_type: script",
  "name: tbert file_path: test/tbert.m start_line: 1 end_line: 80 code_snippet: classdef(SharedTestFixtures = {\n        DownloadBERTFixture, DownloadJPBERTFixture}) tbert < matlab.unittest.TestCase\n    % tbert   System level tests for bert\n    \n    % Copyright 2021 The MathWorks, Inc.\n    \n    properties(TestParameter)\n        UncasedVersion = {\"base\", ...\n                   \"tiny\"}\n        AllModels = {\"base\",\"multilingual-cased\",\"medium\",...\n            \"small\",\"mini\",\"tiny\",\"japanese-base\",...\n            \"japanese-base-wwm\"}\n    end\n    \n    methods(Test)\n        \n        function canConstructModelWithDefault(test)\n            % Verify the default model can be constructed.\n            test.verifyWarningFree(@() bert());\n        end\n\n        function canConstructAllModels(test, AllModels)\n            % Verify the all available models can be constructed.\n            test.verifyWarningFree(@() bert('Model', AllModels));\n        end\n           \n        function canConstructModelWithNVPAndVerifyDefault(test)\n            % Verify the default model matches the default model.\n            mdl = test.verifyWarningFree(@() bert('Model', \"base\"));\n            mdlDefault = bert();\n            test.verifyThat(mdl, iIsEqualTo(mdlDefault));\n        end\n        \n        function checkBertIgnoreCase(test, UncasedVersion)\n            % Test that bert() is case insensitive.\n            \n            txt = \"Nipson anomemata. Memonan opsin.\";\n            \n            mdl = bert('Model', UncasedVersion);\n            yLower = mdl.Tokenizer.encode(txt);\n            yUpper = mdl.Tokenizer.encode(upper(txt));\n            \n            test.verifyThat(yLower, iIsEqualTo(yUpper));            \n        end\n        \n        function multicasedVersionIsCaseSensitive(test)\n            % Check that the multicased version is case sensitive.\n            \n            txt = \"Nipson anomemata. Memonan opsin.\";\n            \n            mdl = bert('Model', \"multilingual-cased\");\n            yLower = mdl.Tokenizer.encode(txt);\n            yUpper = mdl.Tokenizer.encode(upper(txt));\n            \n            test.verifyThat(yLower, iIsNotEqualTo(yUpper));\n        end\n        \n        function canDoNSP(test)\n            % Verify the next-sentence prediction works.\n            mdl = bert();\n            text_a = \"MATLAB combines a desktop environment tuned for iterative analysis and design processes with a programming language that expresses matrix and array mathematics directly.\";\n            text_b_next = \"It includes the Live Editor for creating scripts that combine code, output, and formatted text in an executable notebook.\";\n            text_b_random = \"This is just a test, no need to be alarmed.\";\n            x = mdl.Tokenizer.encode([text_a;text_a],[text_b_next;text_b_random]);\n            x = padsequences(x,2,'PaddingValue',mdl.Tokenizer.PaddingCode);\n            prediction = bert.layer.classifierHead(bert.model(x,mdl.Parameters),mdl.Parameters.Weights.pooler,mdl.Parameters.Weights.sequence_relation);\n            probability = softmax(prediction,'DataFormat','CB');\n            classes = onehotdecode(probability,[\"IsNext\",\"IsNotNext\"],1);\n            test.verifyEqual(classes,categorical([\"IsNext\",\"IsNotNext\"]));\n        end\n    end\nend\n\nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend\n\nfunction constraint = iIsNotEqualTo(varargin)\nconstraint = ~matlab.unittest.constraints.IsEqualTo( varargin{:} );\nend scope_id: tbert scope_type: script",
  "name: canConstructModelWithDefault file_path: test/tfinbert.m start_line: 7 end_line: 10 input_parameters: ['test'] code_snippet:         function canConstructModelWithDefault(test)\n            % Verify the default model can be constructed.\n            test.verifyWarningFree(@() finbert());\n        end scope_id: tfinbert scope_type: script",
  "name: canConstructModelWithNVP file_path: test/tfinbert.m start_line: 11 end_line: 18 input_parameters: ['test'] code_snippet:         \n        function canConstructModelWithNVP(test)\n            % Verify the default model matches the sentiment analysis\n            % model.\n            mdl = test.verifyWarningFree(@() finbert('Model','sentiment-model'));\n            mdlDefault = finbert();\n            test.verifyEqual(mdl,mdlDefault);\n        end scope_id: tfinbert scope_type: script",
  "name: canConstructLanguageModel file_path: test/tfinbert.m start_line: 19 end_line: 23 input_parameters: ['test'] code_snippet:         \n        function canConstructLanguageModel(test)\n            % Verify that we can construct the language model\n            test.verifyWarningFree(@() finbert('Model','language-model'));\n        end         scope_id: tfinbert scope_type: script",
  "name: tfinbert file_path: test/tfinbert.m start_line: 1 end_line: 25 code_snippet: classdef (SharedTestFixtures={\n        DownloadFinBERTFixture}) ...\n        tfinbert < matlab.unittest.TestCase\n    \n    \n    methods(Test)        \n        function canConstructModelWithDefault(test)\n            % Verify the default model can be constructed.\n            test.verifyWarningFree(@() finbert());\n        end\n        \n        function canConstructModelWithNVP(test)\n            % Verify the default model matches the sentiment analysis\n            % model.\n            mdl = test.verifyWarningFree(@() finbert('Model','sentiment-model'));\n            mdlDefault = finbert();\n            test.verifyEqual(mdl,mdlDefault);\n        end\n        \n        function canConstructLanguageModel(test)\n            % Verify that we can construct the language model\n            test.verifyWarningFree(@() finbert('Model','language-model'));\n        end        \n    end\nend scope_id: tfinbert scope_type: script",
  "name: verifyOutputDimSizes file_path: test/tpredictMaskedToken.m start_line: 13 end_line: 18 input_parameters: ['test', 'Models', 'ValidText'] code_snippet:         function verifyOutputDimSizes(test, Models, ValidText)\n            inSize = size(ValidText);\n            mdl = bert(\"Model\", Models);\n            outputText = predictMaskedToken(mdl,ValidText);\n            test.verifyEqual(size(outputText), inSize);\n        end scope_id: tpredictMaskedToken scope_type: script",
  "name: maskTokenIsRemoved file_path: test/tpredictMaskedToken.m start_line: 19 end_line: 25 input_parameters: ['test', 'Models'] code_snippet:         \n        function maskTokenIsRemoved(test, Models)\n            text = \"This has a [MASK] token.\";\n            mdl = bert(\"Model\", Models);\n            outputText = predictMaskedToken(mdl,text);\n            test.verifyFalse(contains(outputText, \"[MASK]\"));\n        end scope_id: tpredictMaskedToken scope_type: script",
  "name: inputWithoutMASKRemainsTheSame file_path: test/tpredictMaskedToken.m start_line: 26 end_line: 32 input_parameters: ['test', 'Models'] code_snippet: \n        function inputWithoutMASKRemainsTheSame(test, Models)\n            text = \"This has a no mask token.\";\n            mdl = bert(\"Model\", Models);\n            outputText = predictMaskedToken(mdl,text);\n            test.verifyEqual(text, outputText);\n        end scope_id: tpredictMaskedToken scope_type: script",
  "name: size file_path: test/tpredictMaskedToken.m start_line: 13 end_line: 18 input_parameters: ['ValidText'] code_snippet:         function verifyOutputDimSizes(test, Models, ValidText)\n            inSize = size(ValidText);\n            mdl = bert(\"Model\", Models);\n            outputText = predictMaskedToken(mdl,ValidText);\n            test.verifyEqual(size(outputText), inSize);\n        end scope_id: tpredictMaskedToken scope_type: script",
  "name: tpredictMaskedToken file_path: test/tpredictMaskedToken.m start_line: 1 end_line: 50 code_snippet: classdef(SharedTestFixtures={\n        DownloadBERTFixture, DownloadJPBERTFixture}) tpredictMaskedToken < matlab.unittest.TestCase\n    % tpredictMaskedToken   Unit test for predictMaskedToken\n    \n    % Copyright 2023 The MathWorks, Inc.\n\n    properties(TestParameter)\n        Models = {\"tiny\",\"japanese-base-wwm\"}\n        ValidText = iGetValidText;\n    end\n    \n    methods(Test)\n        function verifyOutputDimSizes(test, Models, ValidText)\n            inSize = size(ValidText);\n            mdl = bert(\"Model\", Models);\n            outputText = predictMaskedToken(mdl,ValidText);\n            test.verifyEqual(size(outputText), inSize);\n        end\n        \n        function maskTokenIsRemoved(test, Models)\n            text = \"This has a [MASK] token.\";\n            mdl = bert(\"Model\", Models);\n            outputText = predictMaskedToken(mdl,text);\n            test.verifyFalse(contains(outputText, \"[MASK]\"));\n        end\n\n        function inputWithoutMASKRemainsTheSame(test, Models)\n            text = \"This has a no mask token.\";\n            mdl = bert(\"Model\", Models);\n            outputText = predictMaskedToken(mdl,text);\n            test.verifyEqual(text, outputText);\n        end\n    end\nend\n\nfunction validText = iGetValidText\nmanyStrs = [\"Accelerating the pace of [MASK] and science\";\n            \"The cat [MASK] soundly.\";\n            \"The [MASK] set beautifully.\"];\nsingleStr = \"Artificial intelligence continues to shape the future of industries,\" + ...\n    \" as innovative applications emerge in fields such as healthcare, transportation,\" + ...\n    \" entertainment, and finance, driving productivity and enhancing human capabilities.\";\nvalidText = struct('StringsAsColumns',manyStrs,...\n             'StringsAsRows',manyStrs',...\n             'ManyStrings',repmat(singleStr,3),...\n             'SingleString',singleStr,...\n             'FirstWordMasked', \"[MASK] the pace.\",...\n             'LastWordMasked', \"Accelerating the [MASK].\",...\n             'SingleMaskToken', \"[MASK]\");\nend scope_id: tpredictMaskedToken scope_type: script",
  "name: gpt2.download file_path: +gpt2/download.m start_line: 1 end_line: 30 code_snippet: function download()\n% download   Download all of the necessary weight files for transformer model\n%\n%   download() will download all of the files that define the pretrained\n%   GPT-2 355M model.\n\n% Create directories for the model.\nmodelType = 'gpt2-355M';\nmodelDirectory = fullfile(fileparts(mfilename('fullpath')),'..',modelType);\niCreateDirectoryIfItDoesNotExist(modelDirectory);\n\n% Download 'encoder.txt'. This is equivalent to 'encoder.json' from the\n% original GPT-2.\niDownloadFileIfItDoesNotExist( ...\n    fullfile(modelDirectory,'encoder.txt'), ...\n    'https://ssd.mathworks.com/supportfiles/nnet/data/networks/gpt2_encoder.txt' );\n\n% Download 'vocab.bpe'. This file contains the BPE ranks for the encoder.\n% This file is identical to the one used by the original OpenAI repo.\niDownloadFileIfItDoesNotExist( ...\n    fullfile(modelDirectory,'vocab.bpe'), ...\n    'https://ssd.mathworks.com/supportfiles/nnet/data/networks/gpt2_vocab.bpe' );\n\n% Download 'parameters.mat'. This contains all of the weights in the GPT-2\n% model. They have been exported from the original TensorFlow\n% implementation.\niDownloadFileIfItDoesNotExist( ...\n    fullfile(modelDirectory,'parameters.mat'), ...\n    'https://ssd.mathworks.com/supportfiles/nnet/data/networks/gpt2_355M_params.mat' );\nend scope_id: download scope_type: script",
  "name: gpt2.iCreateDirectoryIfItDoesNotExist file_path: +gpt2/download.m start_line: 31 end_line: 38 input_parameters: ['directory'] code_snippet: \nfunction iCreateDirectoryIfItDoesNotExist(directory)\nif ~exist(directory, 'dir')\n    fprintf('Creating directory ''%s''...\\n', directory);\n    mkdir(directory);\nelse\n    fprintf('Skipped creating directory ''%s'' as it already exists\\n', directory);\nend scope_id: download scope_type: script",
  "name: gpt2.iDownloadFileIfItDoesNotExist file_path: +gpt2/download.m start_line: 40 end_line: 47 input_parameters: ['destination', 'source'] code_snippet: \nfunction iDownloadFileIfItDoesNotExist(destination, source)\nif ~exist(destination, 'file')\n    fprintf('Downloading file ''%s'' ...\\n', destination);\n    websave(destination, source);\nelse\n    fprintf('Skipped downloading file ''%s'' as it already exists\\n', destination);\nend scope_id: download scope_type: script",
  "name: download file_path: +gpt2/download.m start_line: 1 end_line: 48 code_snippet: function download()\n% download   Download all of the necessary weight files for transformer model\n%\n%   download() will download all of the files that define the pretrained\n%   GPT-2 355M model.\n\n% Create directories for the model.\nmodelType = 'gpt2-355M';\nmodelDirectory = fullfile(fileparts(mfilename('fullpath')),'..',modelType);\niCreateDirectoryIfItDoesNotExist(modelDirectory);\n\n% Download 'encoder.txt'. This is equivalent to 'encoder.json' from the\n% original GPT-2.\niDownloadFileIfItDoesNotExist( ...\n    fullfile(modelDirectory,'encoder.txt'), ...\n    'https://ssd.mathworks.com/supportfiles/nnet/data/networks/gpt2_encoder.txt' );\n\n% Download 'vocab.bpe'. This file contains the BPE ranks for the encoder.\n% This file is identical to the one used by the original OpenAI repo.\niDownloadFileIfItDoesNotExist( ...\n    fullfile(modelDirectory,'vocab.bpe'), ...\n    'https://ssd.mathworks.com/supportfiles/nnet/data/networks/gpt2_vocab.bpe' );\n\n% Download 'parameters.mat'. This contains all of the weights in the GPT-2\n% model. They have been exported from the original TensorFlow\n% implementation.\niDownloadFileIfItDoesNotExist( ...\n    fullfile(modelDirectory,'parameters.mat'), ...\n    'https://ssd.mathworks.com/supportfiles/nnet/data/networks/gpt2_355M_params.mat' );\nend\n\nfunction iCreateDirectoryIfItDoesNotExist(directory)\nif ~exist(directory, 'dir')\n    fprintf('Creating directory ''%s''...\\n', directory);\n    mkdir(directory);\nelse\n    fprintf('Skipped creating directory ''%s'' as it already exists\\n', directory);\nend\nend\n\nfunction iDownloadFileIfItDoesNotExist(destination, source)\nif ~exist(destination, 'file')\n    fprintf('Downloading file ''%s'' ...\\n', destination);\n    websave(destination, source);\nelse\n    fprintf('Skipped downloading file ''%s'' as it already exists\\n', destination);\nend\nend scope_id: download scope_type: script",
  "name: gpt2.load file_path: +gpt2/load.m start_line: 1 end_line: 30 input_parameters: ['filepath'] code_snippet: function newParameters = load(filepath)\n% load   Load GPT-2\n%\n%   parameters = load(filepath) will load a GPT-2 model from the directory\n%   specified by filepath.\n\n% Load the parameters\ns = load(filepath);\n\n% First, assign the hyperparameters and then remove them from the loaded\n% structure.\nnewParameters = struct;\nnewParameters.Hyperparameters = s.hyperparameters;\ns = rmfield(s, 'hyperparameters');\n\n% Next, assign the weights\nnewParameters.Weights = struct;\n\noriginalWeightNames = fieldnames(s);\nnewWeightNames = erase(originalWeightNames, 'model_');\n\n% Permute all of the weights to match the MATLAB format\nfor i = 1:numel(originalWeightNames)\n    if startsWith(newWeightNames{i},'h')\n        nameSplit = split(newWeightNames{i}, '_');\n        newParameters.Weights.(char(nameSplit(1))).(char(join(nameSplit(2:end),'_'))) = ...\n            s.(originalWeightNames{i});\n    else\n        newParameters.Weights.(char(newWeightNames{i})) = s.(originalWeightNames{i});\n    end scope_id: load scope_type: script",
  "name: gpt2.iStructRecurseFun file_path: +gpt2/load.m start_line: 36 end_line: 45 input_parameters: ['F', 's', 'varargin'] code_snippet: \nfunction s = iStructRecurseFun(F, s, varargin)\nif ~isstruct(s)\n    s = F(s, varargin{:});\nelse\n    fn = fieldnames(s);\n    for i = 1:numel(fn)\n        others = cellfun(@(o)o.(fn{i}), varargin, 'UniformOutput', false);\n        s.(fn{i}) = iStructRecurseFun(F, s.(fn{i}), others{:});\n    end scope_id: load scope_type: script",
  "name: load file_path: +gpt2/load.m start_line: 1 end_line: 47 code_snippet: function newParameters = load(filepath)\n% load   Load GPT-2\n%\n%   parameters = load(filepath) will load a GPT-2 model from the directory\n%   specified by filepath.\n\n% Load the parameters\ns = load(filepath);\n\n% First, assign the hyperparameters and then remove them from the loaded\n% structure.\nnewParameters = struct;\nnewParameters.Hyperparameters = s.hyperparameters;\ns = rmfield(s, 'hyperparameters');\n\n% Next, assign the weights\nnewParameters.Weights = struct;\n\noriginalWeightNames = fieldnames(s);\nnewWeightNames = erase(originalWeightNames, 'model_');\n\n% Permute all of the weights to match the MATLAB format\nfor i = 1:numel(originalWeightNames)\n    if startsWith(newWeightNames{i},'h')\n        nameSplit = split(newWeightNames{i}, '_');\n        newParameters.Weights.(char(nameSplit(1))).(char(join(nameSplit(2:end),'_'))) = ...\n            s.(originalWeightNames{i});\n    else\n        newParameters.Weights.(char(newWeightNames{i})) = s.(originalWeightNames{i});\n    end\nend\n\nnewParameters.Weights = iStructRecurseFun(@dlarray, newParameters.Weights);\n\nend\n\nfunction s = iStructRecurseFun(F, s, varargin)\nif ~isstruct(s)\n    s = F(s, varargin{:});\nelse\n    fn = fieldnames(s);\n    for i = 1:numel(fn)\n        others = cellfun(@(o)o.(fn{i}), varargin, 'UniformOutput', false);\n        s.(fn{i}) = iStructRecurseFun(F, s.(fn{i}), others{:});\n    end\nend\nend scope_id: load scope_type: script",
  "name: gpt2.model file_path: +gpt2/model.m start_line: 1 end_line: 76 input_parameters: ['X', 'pasts', 'parameters'] output_variables: ['logits', 'presents'] code_snippet: function [logits, presents] = model(X, pasts, parameters)\n% model   A GPT-2 model\n%\n%   [logits, presents] = model(X, pasts, parameters) performs prediction\n%   with a GPT-2 model on the input X. X is a\n%   1-by-numInputSubwords-by-numObs array of tokenized text, and the model\n%   returns an array logits that is 50257-by-numInputSubwords-by-numObs.\n%   This array can be used to predict the next subword. See below for more\n%   details of inputs and outputs.\n%\n%   Inputs:\n%       X                 - A 1-by-numInputSubwords-by-numObs array. This\n%                           array is a tokenized sentence. It should be\n%                           created using the tokenizer for GPT-2.\n%       pasts             - A numLayers-by-1 cell array containing \"keys\"\n%                           and \"values\" for the attention layers. These\n%                           come from the previous subwords in the text we\n%                           are processing. If there are no previous words,\n%                           this should be an empty cell.\n%       parameters        - The parameters for the GPT-2 model in a struct.\n%                           It has two fields, 'Hyperparameters' and\n%                           'Weights'. 'Hyperparameters' has the following\n%                           fields:\n%                             - NumHeads: The number of attention heads in\n%                               each multi-head attention layer.\n%                             - NumLayers: The number of blocks in the\n%                               GPT-2 model.\n%                             - NumContext: The size of the context\n%                               embedding.\n%                           'Weights' includes the folloeing fields:\n%                             - wte_0: A numFeatures-by-50257 embedding\n%                               matrix for subwords.\n%                             - wpe_0: A numFeatures-by-numContext\n%                               positional embedding matrix for subwords.\n%                             - h<X>: Structs containing weights for the\n%                               transformer blocks where <X> is the number\n%                               for the block. Numbers start from 0.\n%                             - ln_f_g_0: Weight vector for final layer\n%                               normalization.\n%                             - ln_f_b_0: Bias vector for final layer\n%                               normalization.\n%\n%   Outputs:\n%       logits            - A 50257-by-numInputSubwords-by-numObs array of\n%                           logits (pre-softmax outputs). If we apply\n%                           softmax to this array, we get the probabilities\n%                           for the next subword. However, we usually want\n%                           to do more pre-processing before doing this\n%                           (like taking the top-K entries). 50257 is the\n%                           number of subwords in the vocabulary for\n%                           GPT-2's tokenizer.\n%       presents          - A numLayers-by-1 cell array containing \"keys\"\n%                           and \"values\" from the attention blocks. We feed\n%                           these back in as the 'pasts' input.\n\nhyperparameters     = parameters.Hyperparameters;\nweights             = parameters.Weights;\n\n% Apply the embedding. If there are inputs for the \"past\", we need to\n% offset the position embedding to account for this.\n% Word embedding\nseqLen = size(X, 2);\nh = weights.wte_0(:, X);\nh = reshape(h, size(h,1), seqLen, []);\n% Positional embedding\npositionOffset = size(pasts{1},2);\nh = h + weights.wpe_0(:, positionOffset + (1:seqLen) );\n\n% Run the layers\npresents = cell(hyperparameters.NumLayers,1);\nfor i = 1:hyperparameters.NumLayers\n    layerName = ['h' num2str(i-1)];\n    [h, present] = gpt2.layer.block( h, pasts{i}, weights.(layerName), ...\n        hyperparameters );\n    presents{i} = present;\nend scope_id: model scope_type: script",
  "name: model file_path: +gpt2/model.m start_line: 1 end_line: 85 code_snippet: function [logits, presents] = model(X, pasts, parameters)\n% model   A GPT-2 model\n%\n%   [logits, presents] = model(X, pasts, parameters) performs prediction\n%   with a GPT-2 model on the input X. X is a\n%   1-by-numInputSubwords-by-numObs array of tokenized text, and the model\n%   returns an array logits that is 50257-by-numInputSubwords-by-numObs.\n%   This array can be used to predict the next subword. See below for more\n%   details of inputs and outputs.\n%\n%   Inputs:\n%       X                 - A 1-by-numInputSubwords-by-numObs array. This\n%                           array is a tokenized sentence. It should be\n%                           created using the tokenizer for GPT-2.\n%       pasts             - A numLayers-by-1 cell array containing \"keys\"\n%                           and \"values\" for the attention layers. These\n%                           come from the previous subwords in the text we\n%                           are processing. If there are no previous words,\n%                           this should be an empty cell.\n%       parameters        - The parameters for the GPT-2 model in a struct.\n%                           It has two fields, 'Hyperparameters' and\n%                           'Weights'. 'Hyperparameters' has the following\n%                           fields:\n%                             - NumHeads: The number of attention heads in\n%                               each multi-head attention layer.\n%                             - NumLayers: The number of blocks in the\n%                               GPT-2 model.\n%                             - NumContext: The size of the context\n%                               embedding.\n%                           'Weights' includes the folloeing fields:\n%                             - wte_0: A numFeatures-by-50257 embedding\n%                               matrix for subwords.\n%                             - wpe_0: A numFeatures-by-numContext\n%                               positional embedding matrix for subwords.\n%                             - h<X>: Structs containing weights for the\n%                               transformer blocks where <X> is the number\n%                               for the block. Numbers start from 0.\n%                             - ln_f_g_0: Weight vector for final layer\n%                               normalization.\n%                             - ln_f_b_0: Bias vector for final layer\n%                               normalization.\n%\n%   Outputs:\n%       logits            - A 50257-by-numInputSubwords-by-numObs array of\n%                           logits (pre-softmax outputs). If we apply\n%                           softmax to this array, we get the probabilities\n%                           for the next subword. However, we usually want\n%                           to do more pre-processing before doing this\n%                           (like taking the top-K entries). 50257 is the\n%                           number of subwords in the vocabulary for\n%                           GPT-2's tokenizer.\n%       presents          - A numLayers-by-1 cell array containing \"keys\"\n%                           and \"values\" from the attention blocks. We feed\n%                           these back in as the 'pasts' input.\n\nhyperparameters     = parameters.Hyperparameters;\nweights             = parameters.Weights;\n\n% Apply the embedding. If there are inputs for the \"past\", we need to\n% offset the position embedding to account for this.\n% Word embedding\nseqLen = size(X, 2);\nh = weights.wte_0(:, X);\nh = reshape(h, size(h,1), seqLen, []);\n% Positional embedding\npositionOffset = size(pasts{1},2);\nh = h + weights.wpe_0(:, positionOffset + (1:seqLen) );\n\n% Run the layers\npresents = cell(hyperparameters.NumLayers,1);\nfor i = 1:hyperparameters.NumLayers\n    layerName = ['h' num2str(i-1)];\n    [h, present] = gpt2.layer.block( h, pasts{i}, weights.(layerName), ...\n        hyperparameters );\n    presents{i} = present;\nend\n\nh = transformer.layer.normalization( h, ...\n    weights.ln_f_g_0, ...\n    weights.ln_f_b_0 );\n\n% Calculate logits (50257-by-numInputSubwords-by-numObs)\nlogits = dlmtimes(weights.wte_0', h);\n\nend scope_id: model scope_type: script",
  "name: finbert.sentimentModel file_path: +finbert/sentimentModel.m start_line: 1 end_line: 15 input_parameters: ['x', 'p'] output_variables: ['clf', 'score'] code_snippet: function [clf,score] = sentimentModel(x,p)\n% sentimentModel   The FinBERT sentiment analysis model.\n%\n% clf = finbert.sentimentModel(x,p)   Given an input x of size\n%   1-by-numInputTokens-by-numObs and FinBERT parameters struct p, the output\n%   clf is a categorical of size 1-by-numObs with categories\n%   \"positive\",\"neutral\" or \"negative\".\n%\n% [clf,score] = finbert.sentimentModel(x,p)   The second output is a sentiment\n%   score in the range [-1,1].\n\n% Copyright 2021 The MathWorks, Inc.\nif ~isfield(p.Weights,'classifier')\n    error(\"finbert:sentimentAnalysis:NoClassifier\",\"Parameters do not include classifier weights\");\nend scope_id: sentimentModel scope_type: script",
  "name: sentimentModel file_path: +finbert/sentimentModel.m start_line: 1 end_line: 24 code_snippet: function [clf,score] = sentimentModel(x,p)\n% sentimentModel   The FinBERT sentiment analysis model.\n%\n% clf = finbert.sentimentModel(x,p)   Given an input x of size\n%   1-by-numInputTokens-by-numObs and FinBERT parameters struct p, the output\n%   clf is a categorical of size 1-by-numObs with categories\n%   \"positive\",\"neutral\" or \"negative\".\n%\n% [clf,score] = finbert.sentimentModel(x,p)   The second output is a sentiment\n%   score in the range [-1,1].\n\n% Copyright 2021 The MathWorks, Inc.\nif ~isfield(p.Weights,'classifier')\n    error(\"finbert:sentimentAnalysis:NoClassifier\",\"Parameters do not include classifier weights\");\nend\n\nz = bert.model(x,p);\ny = bert.layer.classifierHead(z,p.Weights.pooler,p.Weights.classifier);\nlogits = softmax(y,'DataFormat','CB');\n[~,clf_i] = max(extractdata(logits));\nclasses = [\"positive\",\"negative\",\"neutral\"];\nscore = logits(1,:) - logits(2,:);\nclf = categorical(classes(clf_i),classes);\nend scope_id: sentimentModel scope_type: script",
  "name: finbert.languageModel file_path: +finbert/languageModel.m start_line: 1 end_line: 13 input_parameters: ['x', 'params'] code_snippet: function z = languageModel(x,params)\n% languageModel   The FinBERT language model.\n%\n%   Z = finbert.languageModel(X,parameters) performs inference with a FinBERT model\n%   on the input X, and applies the output layer projection onto the\n%   associated vocabulary. The input X is a 1-by-numInputTokens-by-numObs\n%   array of encoded tokens. The return is an array Z of size\n%   vocabularySize-by-numInputTokens-by-numObs. In particular the language model is\n%   trained to predict a reasonable word for each masked input token.\n\n% Copyright 2021 The MathWorks, Inc.\nz = bert.languageModel(x,params);\nend scope_id: languageModel scope_type: script",
  "name: languageModel file_path: +finbert/languageModel.m start_line: 1 end_line: 13 code_snippet: function z = languageModel(x,params)\n% languageModel   The FinBERT language model.\n%\n%   Z = finbert.languageModel(X,parameters) performs inference with a FinBERT model\n%   on the input X, and applies the output layer projection onto the\n%   associated vocabulary. The input X is a 1-by-numInputTokens-by-numObs\n%   array of encoded tokens. The return is an array Z of size\n%   vocabularySize-by-numInputTokens-by-numObs. In particular the language model is\n%   trained to predict a reasonable word for each masked input token.\n\n% Copyright 2021 The MathWorks, Inc.\nz = bert.languageModel(x,params);\nend scope_id: languageModel scope_type: script",
  "name: finbert.load file_path: +finbert/load.m start_line: 1 end_line: 10 input_parameters: ['modelName'] code_snippet: function params = load(modelName)\n% load   Load FinBERT parameters\n%\n%   parameters = load(modelName) will load the model weights associated to\n%   modelName and the hyperparameters associated to that model.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string = \"sentiment-model\"\nend scope_id: load scope_type: script",
  "name: load file_path: +finbert/load.m start_line: 1 end_line: 18 code_snippet: function params = load(modelName)\n% load   Load FinBERT parameters\n%\n%   parameters = load(modelName) will load the model weights associated to\n%   modelName and the hyperparameters associated to that model.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string = \"sentiment-model\"\nend\n\nparamsStructFile = finbert.internal.getSupportFilePath(modelName,\"parameters.mat\");\nparamsStruct = load(paramsStructFile);\n\nparams = struct(...\n    'Hyperparameters',paramsStruct.Hyperparameters,...\n    'Weights',bert.internal.createParameterStruct(paramsStruct.Weights));\nend scope_id: load scope_type: script",
  "name: finbert.mustBeLessThanOrEqualNumLayers file_path: +finbert/model.m start_line: 67 end_line: 70 input_parameters: ['x', 'params'] code_snippet: \nfunction mustBeLessThanOrEqualNumLayers(x,params)\nmustBeLessThanOrEqual(x,params.Hyperparameters.NumLayers);\nend scope_id: model scope_type: script",
  "name: finbert.mustBeALogicalOrDlarrayLogical file_path: +finbert/model.m start_line: 71 end_line: 75 input_parameters: ['val'] code_snippet: \nfunction mustBeALogicalOrDlarrayLogical(val)\nif isa(val,'dlarray')\n    val = extractdata(val);\nend scope_id: model scope_type: script",
  "name: finbert.mustBeNumericDlarray file_path: +finbert/model.m start_line: 78 end_line: 82 input_parameters: ['val'] code_snippet: \nfunction mustBeNumericDlarray(val)\nmustBeA(val,'dlarray');\nmustBeNumeric(extractdata(val));\nend scope_id: model scope_type: script",
  "name: finbert.model file_path: +finbert/model.m start_line: 1 end_line: 55 input_parameters: ['x', 'parameters', 'nvp'] code_snippet: function varargout = model(x,parameters,nvp)\n% model   A FinBERT model forward pass.\n%\n%   Z = model(X,parameters) performs inference with a BERT model on the\n%   input X. X is a 1-by-numInputTokens-by-numObs array of encoded tokens.\n%   The return is an array Z of size\n%   (NumHeads*HeadSize)-by-numInputTokens-by-numObs. Each\n%   Z(:,i,j) corresponds to the BERT embedding of input token X(1,i,j).\n%\n%   Z = model(X,parameters,'PARAM1', VAL1, 'PARAM2', VAL2, ...) specifies\n%   the optional parameter name/value pairs:\n%\n%      'InputMask'             - A logical mask with the same size as X.\n%                                The mask should be false at indices \n%                                (i,j,k) for which X(i,j,k) corresponds to\n%                                padding, and true elsewhere. The default\n%                                is empty, for which the padding is\n%                                inferred by the entries of X that match\n%                                the PaddingCode name-value pair.\n%\n%      'DropoutProb'           - The probability of dropout for the output\n%                                activation. It is standard to set this to a\n%                                non-zero value during training, for example\n%                                0.1. The default is 0.\n%\n%      'AttentionDropoutProb'  - The probability of dropout used in the\n%                                attention layer. It is standard to set\n%                                this to a non-zero value during training,\n%                                for example 0.1. The default is 0.\n%\n%      'Outputs'               - Specify the indices of the layers to\n%                                return outputs from as a vector of\n%                                positive integers, or 'last' to specify\n%                                the final encoder layer only. The default\n%                                is 'last'.\n%\n%      'SeparatorCode'         - The positive integer corresponding to the\n%                                separator token. The default is 103, as\n%                                specified in the default BERT vocab.txt.\n%\n%      'PaddingCode'           - The positive integer corresponding to the\n%                                padding token. The default is 1, as\n%                                specified in the default BERT vocab.txt.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    x dlarray {mustBeNumericDlarray,mustBeNonempty}\n    parameters {mustBeA(parameters,'struct')}\n    nvp.InputMask {mustBeALogicalOrDlarrayLogical} = logical.empty()\n    nvp.DropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.DropoutProb,1),mustBeNumeric} = 0\n    nvp.AttentionDropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.AttentionDropoutProb,1),mustBeNumeric} = 0\n    nvp.Outputs {mustBePositive,mustBeLessThanOrEqualNumLayers(nvp.Outputs,parameters),mustBeInteger,mustBeNumeric} = parameters.Hyperparameters.NumLayers\n    nvp.PaddingCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 1\n    nvp.SeparatorCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 103\nend scope_id: model scope_type: script",
  "name: finbert.extractdata file_path: +finbert/model.m start_line: 67 end_line: 70 input_parameters: ['val'] code_snippet: \nfunction mustBeLessThanOrEqualNumLayers(x,params)\nmustBeLessThanOrEqual(x,params.Hyperparameters.NumLayers);\nend scope_id: model scope_type: script",
  "name: model file_path: +finbert/model.m start_line: 1 end_line: 82 code_snippet: function varargout = model(x,parameters,nvp)\n% model   A FinBERT model forward pass.\n%\n%   Z = model(X,parameters) performs inference with a BERT model on the\n%   input X. X is a 1-by-numInputTokens-by-numObs array of encoded tokens.\n%   The return is an array Z of size\n%   (NumHeads*HeadSize)-by-numInputTokens-by-numObs. Each\n%   Z(:,i,j) corresponds to the BERT embedding of input token X(1,i,j).\n%\n%   Z = model(X,parameters,'PARAM1', VAL1, 'PARAM2', VAL2, ...) specifies\n%   the optional parameter name/value pairs:\n%\n%      'InputMask'             - A logical mask with the same size as X.\n%                                The mask should be false at indices \n%                                (i,j,k) for which X(i,j,k) corresponds to\n%                                padding, and true elsewhere. The default\n%                                is empty, for which the padding is\n%                                inferred by the entries of X that match\n%                                the PaddingCode name-value pair.\n%\n%      'DropoutProb'           - The probability of dropout for the output\n%                                activation. It is standard to set this to a\n%                                non-zero value during training, for example\n%                                0.1. The default is 0.\n%\n%      'AttentionDropoutProb'  - The probability of dropout used in the\n%                                attention layer. It is standard to set\n%                                this to a non-zero value during training,\n%                                for example 0.1. The default is 0.\n%\n%      'Outputs'               - Specify the indices of the layers to\n%                                return outputs from as a vector of\n%                                positive integers, or 'last' to specify\n%                                the final encoder layer only. The default\n%                                is 'last'.\n%\n%      'SeparatorCode'         - The positive integer corresponding to the\n%                                separator token. The default is 103, as\n%                                specified in the default BERT vocab.txt.\n%\n%      'PaddingCode'           - The positive integer corresponding to the\n%                                padding token. The default is 1, as\n%                                specified in the default BERT vocab.txt.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    x dlarray {mustBeNumericDlarray,mustBeNonempty}\n    parameters {mustBeA(parameters,'struct')}\n    nvp.InputMask {mustBeALogicalOrDlarrayLogical} = logical.empty()\n    nvp.DropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.DropoutProb,1),mustBeNumeric} = 0\n    nvp.AttentionDropoutProb (1,1) {mustBeNonnegative,mustBeLessThanOrEqual(nvp.AttentionDropoutProb,1),mustBeNumeric} = 0\n    nvp.Outputs {mustBePositive,mustBeLessThanOrEqualNumLayers(nvp.Outputs,parameters),mustBeInteger,mustBeNumeric} = parameters.Hyperparameters.NumLayers\n    nvp.PaddingCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 1\n    nvp.SeparatorCode (1,1) {mustBePositive,mustBeInteger,mustBeNumeric} = 103\nend\n\n% Pure wrapper.\nvarargout = cell(max(nargout,1),1);\n[varargout{:}] = bert.model(x,parameters,...\n    'InputMask',nvp.InputMask,...\n    'DropoutProb',nvp.DropoutProb,...\n    'AttentionDropoutProb',nvp.AttentionDropoutProb,...\n    'Outputs',nvp.Outputs,...\n    'PaddingCode',nvp.PaddingCode,...\n    'SeparatorCode',nvp.SeparatorCode);\nend\n\nfunction mustBeLessThanOrEqualNumLayers(x,params)\nmustBeLessThanOrEqual(x,params.Hyperparameters.NumLayers);\nend\n\nfunction mustBeALogicalOrDlarrayLogical(val)\nif isa(val,'dlarray')\n    val = extractdata(val);\nend\nmustBeA(val,'logical');\nend\n\nfunction mustBeNumericDlarray(val)\nmustBeA(val,'dlarray');\nmustBeNumeric(extractdata(val));\nend scope_id: model scope_type: script",
  "name: bert.layer.classifier file_path: +bert/+layer/classifier.m start_line: 1 end_line: 13 input_parameters: ['x', 'p'] code_snippet: function y = classifier(x,p)\n% classifier   The standard BERT classifier, a single fullyconnect.\n%\n%   Z = classifier(X,classifierWeights) applies a fullyconnect operation to\n%   the input X with weights classifierWeights.kernel and bias\n%   classifierWeights.bias. The input X must be an unformatted dlarray of\n%   size hiddenSize-by-numObs. The classifierWeights.kernel must be of size\n%   outputSize-by-hiddenSize, and the classifierWeights.bias must be of\n%   size outputSize-by-1.\n\n% Copyright 2021 The MathWorks, Inc.\ny = transformer.layer.convolution1d(x,p.kernel,p.bias);\nend scope_id: classifier scope_type: script",
  "name: classifier file_path: +bert/+layer/classifier.m start_line: 1 end_line: 13 code_snippet: function y = classifier(x,p)\n% classifier   The standard BERT classifier, a single fullyconnect.\n%\n%   Z = classifier(X,classifierWeights) applies a fullyconnect operation to\n%   the input X with weights classifierWeights.kernel and bias\n%   classifierWeights.bias. The input X must be an unformatted dlarray of\n%   size hiddenSize-by-numObs. The classifierWeights.kernel must be of size\n%   outputSize-by-hiddenSize, and the classifierWeights.bias must be of\n%   size outputSize-by-1.\n\n% Copyright 2021 The MathWorks, Inc.\ny = transformer.layer.convolution1d(x,p.kernel,p.bias);\nend scope_id: classifier scope_type: script",
  "name: bert.layer.languageModelHead file_path: +bert/+layer/languageModelHead.m start_line: 1 end_line: 22 input_parameters: ['z', 'p', 'word_embeddings'] code_snippet: function z = languageModelHead(z,p,word_embeddings)\n% languageModelHead   The standard BERT operations for masked\n% language modeling.\n%\n%   Z = languageModelHead(X,languageModelWeights,wordEmbeddingMatrix)\n%   applies the language model operations to an input X. The input X must\n%   be an unformatted dlarray of size\n%   hiddenSize-by-numInputTokens-by-numObs. The languageModelWeights must\n%   be a struct with fields 'transform' and 'LayerNorm' such as the\n%   mdl.Parameters.Weights.masked_LM struct where mdl = bert(). The\n%   wordEmbeddingMatrix must be the word embedding matrix used by the bert\n%   model such as mdl.Parameters.Weights.embeddings.word_embeddings where\n%   mdl = bert().\n\n% Copyright 2021 The MathWorks, Inc.\nz = transformer.layer.convolution1d(z,p.transform.kernel,p.transform.bias);\nz = transformer.layer.gelu(z);\nz = transformer.layer.normalization(z,p.LayerNorm.gamma,p.LayerNorm.beta);\nz = dlmtimes(word_embeddings.',z);\nz = z + p.output.bias;\nz = softmax(z,'DataFormat','CTB');\nend scope_id: languageModelHead scope_type: script",
  "name: languageModelHead file_path: +bert/+layer/languageModelHead.m start_line: 1 end_line: 22 code_snippet: function z = languageModelHead(z,p,word_embeddings)\n% languageModelHead   The standard BERT operations for masked\n% language modeling.\n%\n%   Z = languageModelHead(X,languageModelWeights,wordEmbeddingMatrix)\n%   applies the language model operations to an input X. The input X must\n%   be an unformatted dlarray of size\n%   hiddenSize-by-numInputTokens-by-numObs. The languageModelWeights must\n%   be a struct with fields 'transform' and 'LayerNorm' such as the\n%   mdl.Parameters.Weights.masked_LM struct where mdl = bert(). The\n%   wordEmbeddingMatrix must be the word embedding matrix used by the bert\n%   model such as mdl.Parameters.Weights.embeddings.word_embeddings where\n%   mdl = bert().\n\n% Copyright 2021 The MathWorks, Inc.\nz = transformer.layer.convolution1d(z,p.transform.kernel,p.transform.bias);\nz = transformer.layer.gelu(z);\nz = transformer.layer.normalization(z,p.LayerNorm.gamma,p.LayerNorm.beta);\nz = dlmtimes(word_embeddings.',z);\nz = z + p.output.bias;\nz = softmax(z,'DataFormat','CTB');\nend scope_id: languageModelHead scope_type: script",
  "name: bert.layer.embedding file_path: +bert/+layer/embedding.m start_line: 1 end_line: 21 input_parameters: ['x', 'types', 'positions', 'w', 'dropout'] code_snippet: function  z = embedding(x,types,positions,w,dropout)\n% embedding   The BERT embeddings of encoded tokens, token types and token\n% positions.\n%\n%   Z = embedding(X,types,positions,weights,dropoutProbability) computes \n%   the embedding of encoded tokens X, token types specified by types, and \n%   token positions. Inputs X, types and positions are \n%   1-by-numInputTokens-by-numObs unformatted dlarray-s. The types take\n%   values 1 or 2. The weights input is a struct of embedding weights such\n%   as mdl.Parameters.Weights.embeddings where mdl = bert(). The\n%   dropoutProbability is a scalar double between 0 and 1 corresponding to\n%   the post-embedding dropout probability.\n\n% Copyright 2021 The MathWorks, Inc.\nwordEmbedding = embed(x,w.word_embeddings,'DataFormat','CTB');\ntypeEmbedding = embed(types,w.token_type_embeddings,'DataFormat','CTB');\npositionEmbedding = embed(positions,w.position_embeddings,'DataFormat','CTB');\nz = wordEmbedding+typeEmbedding+positionEmbedding;\nz = transformer.layer.normalization(z,w.LayerNorm.gamma,w.LayerNorm.beta);\nz = transformer.layer.dropout(z,dropout);\nend scope_id: embedding scope_type: script",
  "name: embedding file_path: +bert/+layer/embedding.m start_line: 1 end_line: 21 code_snippet: function  z = embedding(x,types,positions,w,dropout)\n% embedding   The BERT embeddings of encoded tokens, token types and token\n% positions.\n%\n%   Z = embedding(X,types,positions,weights,dropoutProbability) computes \n%   the embedding of encoded tokens X, token types specified by types, and \n%   token positions. Inputs X, types and positions are \n%   1-by-numInputTokens-by-numObs unformatted dlarray-s. The types take\n%   values 1 or 2. The weights input is a struct of embedding weights such\n%   as mdl.Parameters.Weights.embeddings where mdl = bert(). The\n%   dropoutProbability is a scalar double between 0 and 1 corresponding to\n%   the post-embedding dropout probability.\n\n% Copyright 2021 The MathWorks, Inc.\nwordEmbedding = embed(x,w.word_embeddings,'DataFormat','CTB');\ntypeEmbedding = embed(types,w.token_type_embeddings,'DataFormat','CTB');\npositionEmbedding = embed(positions,w.position_embeddings,'DataFormat','CTB');\nz = wordEmbedding+typeEmbedding+positionEmbedding;\nz = transformer.layer.normalization(z,w.LayerNorm.gamma,w.LayerNorm.beta);\nz = transformer.layer.dropout(z,dropout);\nend scope_id: embedding scope_type: script",
  "name: bert.layer.block file_path: +bert/+layer/block.m start_line: 1 end_line: 36 input_parameters: ['z', 'weights', 'hyperParameters', 'nvp'] code_snippet: function z = block(z,weights,hyperParameters,nvp)\n% block   Transformer block for BERT\n% \n%   Z = block(X,weights,hyperParameters) computes the BERT style\n%   transformer block on the input X as described in [1]. Here X is a\n%   (numFeatures*numHeads)-by-numInputSubwords array. The weights and\n%   hyperParameters must be structs in the same format as returned by the\n%   bert() function.\n%\n%   Z = block(X,weights,hyperParameters,'PARAM1',VAL1,'PARAM2',VAL2)\n%   specifies the optional parameter name/value pairs:\n%\n%     'HiddenDropout'    - The dropout probability to be applied between\n%                          the self attention mechanism and the residual\n%                          connection. The default is 0.\n%\n%     'AttentionDropout' - The dropout probability to be applied to the\n%                          attention probabilities. The default is 0.\n%\n%     'InputMask'        - A logical mask to be used in the attention\n%                          mechanism, for example to block attending to\n%                          padding tokens. The default is [], no masking is\n%                          applied.\n%  \n% References:\n% [1] https://arxiv.org/abs/1810.04805\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n  z\n  weights\n  hyperParameters\n  nvp.HiddenDropout (1,1) double {mustBeNonnegative, mustBeLessThanOrEqual(nvp.HiddenDropout,1)} =  0\n  nvp.AttentionDropout (1,1) double {mustBeNonnegative, mustBeLessThanOrEqual(nvp.AttentionDropout,1)} = 0\n  nvp.InputMask = []\nend   scope_id: block scope_type: script",
  "name: bert.layer.attention file_path: +bert/+layer/block.m start_line: 40 end_line: 69 input_parameters: ['z', 'w', 'num_heads', 'attentionDropout', 'dropout', 'mask'] code_snippet: \nfunction z = attention(z,w,num_heads,attentionDropout,dropout,mask)\n% The self attention part of the transformer layer.\nlayer_input = z;\n\n% Get weights\nQ_w = w.query.kernel;\nQ_b = w.query.bias;\nK_w = w.key.kernel;\nK_b = w.key.bias;\nV_w = w.value.kernel;\nV_b = w.value.bias;\n\n% Put weights into format for transformer.layer.attention\nweights.attn_c_attn_w_0 = cat(1,Q_w,K_w,V_w);\nweights.attn_c_attn_b_0 = cat(1,Q_b,K_b,V_b);\nweights.attn_c_proj_w_0 = w.output.kernel;\nweights.attn_c_proj_b_0 = w.output.bias;\nhyperparameters.NumHeads = num_heads;\nz = transformer.layer.attention(z,[],weights,hyperparameters,'CausalMask',false,'Dropout',attentionDropout,'InputMask',mask);\n\n% Dropout\nz = transformer.layer.dropout(z,dropout);\n\n% Residual connection\nz = layer_input+z;\n\n% Layer normalize.\nz = transformer.layer.normalization(z,w.LayerNorm.gamma,w.LayerNorm.beta);\nend scope_id: block scope_type: script",
  "name: bert.layer.ffn file_path: +bert/+layer/block.m start_line: 70 end_line: 96 input_parameters: ['z', 'w', 'dropout'] code_snippet: \nfunction z = ffn(z,w,dropout)\n% The feed-forward network part of the transformer layer.\n\n% Weights for embedding in higher dimensional space\nint_w = w.intermediate.kernel;\nint_b = w.intermediate.bias;\n\n% Weights for projecting back down to original space\nout_w = w.output.kernel;\nout_b = w.output.bias;\n\n% Create weights struct for multiLayerPerceptron\nweights.mlp_c_fc_w_0 = int_w;\nweights.mlp_c_fc_b_0 = int_b;\nweights.mlp_c_proj_w_0 = out_w;\nweights.mlp_c_proj_b_0 = out_b;\nffn_out = transformer.layer.multiLayerPerceptron(z,weights);\n\n% Dropout\nffn_out = transformer.layer.dropout(ffn_out,dropout);\n\n% Layer normalize.\nout_g = w.LayerNorm.gamma;\nout_b = w.LayerNorm.beta;\nz = transformer.layer.normalization(ffn_out+z,out_g,out_b);\nend scope_id: block scope_type: script",
  "name: block file_path: +bert/+layer/block.m start_line: 1 end_line: 96 code_snippet: function z = block(z,weights,hyperParameters,nvp)\n% block   Transformer block for BERT\n% \n%   Z = block(X,weights,hyperParameters) computes the BERT style\n%   transformer block on the input X as described in [1]. Here X is a\n%   (numFeatures*numHeads)-by-numInputSubwords array. The weights and\n%   hyperParameters must be structs in the same format as returned by the\n%   bert() function.\n%\n%   Z = block(X,weights,hyperParameters,'PARAM1',VAL1,'PARAM2',VAL2)\n%   specifies the optional parameter name/value pairs:\n%\n%     'HiddenDropout'    - The dropout probability to be applied between\n%                          the self attention mechanism and the residual\n%                          connection. The default is 0.\n%\n%     'AttentionDropout' - The dropout probability to be applied to the\n%                          attention probabilities. The default is 0.\n%\n%     'InputMask'        - A logical mask to be used in the attention\n%                          mechanism, for example to block attending to\n%                          padding tokens. The default is [], no masking is\n%                          applied.\n%  \n% References:\n% [1] https://arxiv.org/abs/1810.04805\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n  z\n  weights\n  hyperParameters\n  nvp.HiddenDropout (1,1) double {mustBeNonnegative, mustBeLessThanOrEqual(nvp.HiddenDropout,1)} =  0\n  nvp.AttentionDropout (1,1) double {mustBeNonnegative, mustBeLessThanOrEqual(nvp.AttentionDropout,1)} = 0\n  nvp.InputMask = []\nend  \nz = attention(z,weights.attention,hyperParameters.NumHeads,nvp.AttentionDropout,nvp.HiddenDropout,nvp.InputMask);\nz = ffn(z,weights.feedforward,nvp.HiddenDropout);\nend\n\nfunction z = attention(z,w,num_heads,attentionDropout,dropout,mask)\n% The self attention part of the transformer layer.\nlayer_input = z;\n\n% Get weights\nQ_w = w.query.kernel;\nQ_b = w.query.bias;\nK_w = w.key.kernel;\nK_b = w.key.bias;\nV_w = w.value.kernel;\nV_b = w.value.bias;\n\n% Put weights into format for transformer.layer.attention\nweights.attn_c_attn_w_0 = cat(1,Q_w,K_w,V_w);\nweights.attn_c_attn_b_0 = cat(1,Q_b,K_b,V_b);\nweights.attn_c_proj_w_0 = w.output.kernel;\nweights.attn_c_proj_b_0 = w.output.bias;\nhyperparameters.NumHeads = num_heads;\nz = transformer.layer.attention(z,[],weights,hyperparameters,'CausalMask',false,'Dropout',attentionDropout,'InputMask',mask);\n\n% Dropout\nz = transformer.layer.dropout(z,dropout);\n\n% Residual connection\nz = layer_input+z;\n\n% Layer normalize.\nz = transformer.layer.normalization(z,w.LayerNorm.gamma,w.LayerNorm.beta);\nend\n\nfunction z = ffn(z,w,dropout)\n% The feed-forward network part of the transformer layer.\n\n% Weights for embedding in higher dimensional space\nint_w = w.intermediate.kernel;\nint_b = w.intermediate.bias;\n\n% Weights for projecting back down to original space\nout_w = w.output.kernel;\nout_b = w.output.bias;\n\n% Create weights struct for multiLayerPerceptron\nweights.mlp_c_fc_w_0 = int_w;\nweights.mlp_c_fc_b_0 = int_b;\nweights.mlp_c_proj_w_0 = out_w;\nweights.mlp_c_proj_b_0 = out_b;\nffn_out = transformer.layer.multiLayerPerceptron(z,weights);\n\n% Dropout\nffn_out = transformer.layer.dropout(ffn_out,dropout);\n\n% Layer normalize.\nout_g = w.LayerNorm.gamma;\nout_b = w.LayerNorm.beta;\nz = transformer.layer.normalization(ffn_out+z,out_g,out_b);\nend scope_id: block scope_type: script",
  "name: bert.layer.classifierHead file_path: +bert/+layer/classifierHead.m start_line: 1 end_line: 12 input_parameters: ['x', 'poolerWeights', 'classifierWeights'] code_snippet: function z = classifierHead(x,poolerWeights,classifierWeights)\n% classifierHead   The standard classification head for a BERT model.\n% \n%   Z = classifierHead(X,poolerWeights,classifierWeights) applies\n%   bert.layer.pooler and bert.layer.classifier to X with poolerWeights and\n%   classifierWeights respectively. Both poolerWeights and\n%   classifierWeights must be structs with fields 'kernel' and 'bias'.\n\n% Copyright 2021 The MathWorks, Inc.\nz = bert.layer.pooler(x,poolerWeights);\nz = bert.layer.classifier(z,classifierWeights);\nend scope_id: classifierHead scope_type: script",
  "name: classifierHead file_path: +bert/+layer/classifierHead.m start_line: 1 end_line: 12 code_snippet: function z = classifierHead(x,poolerWeights,classifierWeights)\n% classifierHead   The standard classification head for a BERT model.\n% \n%   Z = classifierHead(X,poolerWeights,classifierWeights) applies\n%   bert.layer.pooler and bert.layer.classifier to X with poolerWeights and\n%   classifierWeights respectively. Both poolerWeights and\n%   classifierWeights must be structs with fields 'kernel' and 'bias'.\n\n% Copyright 2021 The MathWorks, Inc.\nz = bert.layer.pooler(x,poolerWeights);\nz = bert.layer.classifier(z,classifierWeights);\nend scope_id: classifierHead scope_type: script",
  "name: bert.layer.pooler file_path: +bert/+layer/pooler.m start_line: 1 end_line: 15 input_parameters: ['x', 'p'] code_snippet: function y = pooler(x,p)\n% pooler   The standard BERT pooler. Takes first sequence element then \n% applies fullyconnect with tanh activation.\n%\n%   Z = pooler(X,poolerWeights) pools the input X using poolerWeights. The\n%   input X must be a hiddenSize-by-numInputTokens-by-numObs unformatted\n%   dlarray, such as the output of the bert.model function. The\n%   poolerWeights must be a struct with fields 'kernel' and 'bias' of size\n%   outputSize-by-hiddenSize and outputSize-by-1 respectively.\n\n% Copyright 2021 The MathWorks, Inc.\nz = squeeze(x(:,1,:));\ny = transformer.layer.convolution1d(z,p.kernel,p.bias);\ny = tanh(y);\nend scope_id: pooler scope_type: script",
  "name: pooler file_path: +bert/+layer/pooler.m start_line: 1 end_line: 15 code_snippet: function y = pooler(x,p)\n% pooler   The standard BERT pooler. Takes first sequence element then \n% applies fullyconnect with tanh activation.\n%\n%   Z = pooler(X,poolerWeights) pools the input X using poolerWeights. The\n%   input X must be a hiddenSize-by-numInputTokens-by-numObs unformatted\n%   dlarray, such as the output of the bert.model function. The\n%   poolerWeights must be a struct with fields 'kernel' and 'bias' of size\n%   outputSize-by-hiddenSize and outputSize-by-1 respectively.\n\n% Copyright 2021 The MathWorks, Inc.\nz = squeeze(x(:,1,:));\ny = transformer.layer.convolution1d(z,p.kernel,p.bias);\ny = tanh(y);\nend scope_id: pooler scope_type: script",
  "name: bert.internal.encodeWithMaskToken file_path: +bert/+internal/encodeWithMaskToken.m start_line: 1 end_line: 9 input_parameters: ['tok', 'str'] output_variables: ['x', 'untokenizedPieces', 'ismask'] code_snippet: function [x,untokenizedPieces,ismask] = encodeWithMaskToken(tok,str)\n% encodeWithMaskToken   This function handles the case of encoding an input\n% string that includes tokens such as [MASK].\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    tok bert.tokenizer.BERTTokenizer\n    str (1,:) string\nend scope_id: encodeWithMaskToken scope_type: script",
  "name: bert.internal.encodeScalarString file_path: +bert/+internal/encodeWithMaskToken.m start_line: 15 end_line: 27 input_parameters: ['tok', 'str'] output_variables: ['x', 'pieces'] code_snippet: \nfunction [x,pieces] = encodeScalarString(tok,str)\npieces = split(str,tok.MaskToken);\nfulltok = tok.FullTokenizer;\nmaskCode = fulltok.encode(tok.MaskToken);\nx = [];\n\nfor i = 1:numel(pieces)\n    tokens = fulltok.tokenize(pieces(i));\n    if ~isempty(tokens)\n        % \"\" tokenizes to empty - awkward\n        x = cat(2,x,fulltok.encode(tokens));\n    end scope_id: encodeWithMaskToken scope_type: script",
  "name: encodeWithMaskToken file_path: +bert/+internal/encodeWithMaskToken.m start_line: 1 end_line: 33 code_snippet: function [x,untokenizedPieces,ismask] = encodeWithMaskToken(tok,str)\n% encodeWithMaskToken   This function handles the case of encoding an input\n% string that includes tokens such as [MASK].\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    tok bert.tokenizer.BERTTokenizer\n    str (1,:) string\nend\n[seqs,untokenizedPieces] = arrayfun(@(s)encodeScalarString(tok,s),str,'UniformOutput',false);\nx = padsequences(seqs,2,'PaddingValue',tok.PaddingCode);\nmaskCode = tok.MaskCode;\nismask = x==maskCode;\nend\n\nfunction [x,pieces] = encodeScalarString(tok,str)\npieces = split(str,tok.MaskToken);\nfulltok = tok.FullTokenizer;\nmaskCode = fulltok.encode(tok.MaskToken);\nx = [];\n\nfor i = 1:numel(pieces)\n    tokens = fulltok.tokenize(pieces(i));\n    if ~isempty(tokens)\n        % \"\" tokenizes to empty - awkward\n        x = cat(2,x,fulltok.encode(tokens));\n    end\n    if i<numel(pieces)\n        x = cat(2,x,maskCode);\n    end\nend\nx = [fulltok.encode(tok.StartToken),x,fulltok.encode(tok.SeparatorToken)];\nend scope_id: encodeWithMaskToken scope_type: script",
  "name: bert.internal.iParseAttentionAndFeedforwardParamName file_path: +bert/+internal/createParameterStruct.m start_line: 73 end_line: 80 input_parameters: ['name', 'attnOrFeedforward'] output_variables: ['name', 'subname'] code_snippet: \nfunction [name,subname] = iParseAttentionAndFeedforwardParamName(name,attnOrFeedforward)\nswitch attnOrFeedforward\n    case \"attention\"\n        [name,subname] = iParseAttentionParamName(name);\n    case \"feedforward\"\n        [name,subname] = iParseFeedforwardParamName(name);\nend scope_id: createParameterStruct scope_type: script",
  "name: bert.internal.iParseAttentionParamName file_path: +bert/+internal/createParameterStruct.m start_line: 82 end_line: 91 input_parameters: ['name'] output_variables: ['subname', 'subsubname'] code_snippet: \nfunction [subname,subsubname] = iParseAttentionParamName(name)\nif contains(name,\"LayerNorm\")\n    [subname,subsubname] = iParseLayerNorm(name);\nelse\n    name = strrep(name,\"self_\",\"\");\n    name = strrep(name,\"_dense\",\"\");\n    subname = extractBetween(name,\"attention_\",\"_\");\n    subsubname = extractAfter(name,subname+\"_\");\nend scope_id: createParameterStruct scope_type: script",
  "name: bert.internal.iParseFeedforwardParamName file_path: +bert/+internal/createParameterStruct.m start_line: 93 end_line: 100 input_parameters: ['name'] output_variables: ['subname', 'subsubname'] code_snippet: \nfunction [subname,subsubname] = iParseFeedforwardParamName(name)\nif contains(name,\"LayerNorm\")\n    [subname,subsubname] = iParseLayerNorm(name);\nelse\n    subname = extractBefore(name,\"_\");\n    subsubname = extractAfter(name,\"dense_\");\nend scope_id: createParameterStruct scope_type: script",
  "name: bert.internal.iParseLayerNorm file_path: +bert/+internal/createParameterStruct.m start_line: 102 end_line: 106 input_parameters: ['name'] output_variables: ['subname', 'subsubname'] code_snippet: \nfunction [subname,subsubname] = iParseLayerNorm(name)\nsubname = \"LayerNorm\";\nsubsubname = extractAfter(name,\"LayerNorm_\");\nend scope_id: createParameterStruct scope_type: script",
  "name: bert.internal.iParseLM file_path: +bert/+internal/createParameterStruct.m start_line: 107 end_line: 115 input_parameters: ['name'] output_variables: ['subname', 'subsubname'] code_snippet: \nfunction [subname,subsubname] = iParseLM(name)\nif contains(name,\"LayerNorm\")\n    [subname,subsubname] = iParseLayerNorm(name);\nelse\n    name = strrep(name,\"dense_\",\"\");\n    subname = extractBefore(name,\"_\");\n    subsubname = extractAfter(name,\"_\");\nend     scope_id: createParameterStruct scope_type: script",
  "name: bert.internal.createParameterStruct file_path: +bert/+internal/createParameterStruct.m start_line: 1 end_line: 38 input_parameters: ['oldWeightsStruct'] code_snippet: function weightsStruct = createParameterStruct(oldWeightsStruct)\n% createParameterStruct   Given the flat struct of BERT model weights, this\n% function parses that into a tree-like struct of weights.\n\n% Copyright 2021 The MathWorks, Inc.\n\nf = fieldnames(oldWeightsStruct);\nfor i = 1:numel(f)\n    name = f{i};\n    encoderLayerPrefix = \"bert_encoder_layer\";\n    embeddingLayerPrefix = \"bert_embeddings\";\n    poolingLayerPrefix = \"bert_pooler_dense\";\n    langModPrefix = \"cls_predictions\";\n    nspPrefix = \"cls_seq_relationship_output\";\n    genericClassifierPrefix = \"classifier_\";\n    \n    weight = dlarray(oldWeightsStruct.(name));\n\n    if startsWith(name,encoderLayerPrefix)\n        % BERT transformer layer weights.\n        layerIndex = extractBetween(name,encoderLayerPrefix+\"_\",\"_\");\n        newLayerIndex = str2double(layerIndex)+1;\n        layerName = encoderLayerPrefix+\"_\"+layerIndex;\n        shortLayerName = \"layer_\"+newLayerIndex;\n        paramName = extractAfter(name,layerName+\"_\");\n        attentionOrFeedforward = iParseAttentionOrFeedforward(paramName);\n        [subParamName,subsubParamName] = iParseAttentionAndFeedforwardParamName(paramName,attentionOrFeedforward);\n        weightsStruct.(\"encoder_layers\").(shortLayerName).(attentionOrFeedforward).(subParamName).(subsubParamName) = weight;\n        \n    elseif startsWith(name,embeddingLayerPrefix)\n        % Emebdding parameters\n        paramName = extractAfter(name,embeddingLayerPrefix+\"_\");\n        if contains(paramName,\"LayerNorm\")\n            [subname,subsubname] = iParseLayerNorm(paramName);\n            weightsStruct.(\"embeddings\").(subname).(subsubname) = weight;\n        else\n            weightsStruct.(\"embeddings\").(paramName) = weight;\n        end scope_id: createParameterStruct scope_type: script",
  "name: bert.internal.iParseAttentionOrFeedforward file_path: +bert/+internal/createParameterStruct.m start_line: 65 end_line: 71 input_parameters: ['name'] code_snippet: \nfunction name = iParseAttentionOrFeedforward(name)\nif contains(name, \"attention\")\n    name = \"attention\";\nelse\n    name = \"feedforward\";\nend scope_id: createParameterStruct scope_type: script",
  "name: createParameterStruct file_path: +bert/+internal/createParameterStruct.m start_line: 1 end_line: 116 code_snippet: function weightsStruct = createParameterStruct(oldWeightsStruct)\n% createParameterStruct   Given the flat struct of BERT model weights, this\n% function parses that into a tree-like struct of weights.\n\n% Copyright 2021 The MathWorks, Inc.\n\nf = fieldnames(oldWeightsStruct);\nfor i = 1:numel(f)\n    name = f{i};\n    encoderLayerPrefix = \"bert_encoder_layer\";\n    embeddingLayerPrefix = \"bert_embeddings\";\n    poolingLayerPrefix = \"bert_pooler_dense\";\n    langModPrefix = \"cls_predictions\";\n    nspPrefix = \"cls_seq_relationship_output\";\n    genericClassifierPrefix = \"classifier_\";\n    \n    weight = dlarray(oldWeightsStruct.(name));\n\n    if startsWith(name,encoderLayerPrefix)\n        % BERT transformer layer weights.\n        layerIndex = extractBetween(name,encoderLayerPrefix+\"_\",\"_\");\n        newLayerIndex = str2double(layerIndex)+1;\n        layerName = encoderLayerPrefix+\"_\"+layerIndex;\n        shortLayerName = \"layer_\"+newLayerIndex;\n        paramName = extractAfter(name,layerName+\"_\");\n        attentionOrFeedforward = iParseAttentionOrFeedforward(paramName);\n        [subParamName,subsubParamName] = iParseAttentionAndFeedforwardParamName(paramName,attentionOrFeedforward);\n        weightsStruct.(\"encoder_layers\").(shortLayerName).(attentionOrFeedforward).(subParamName).(subsubParamName) = weight;\n        \n    elseif startsWith(name,embeddingLayerPrefix)\n        % Emebdding parameters\n        paramName = extractAfter(name,embeddingLayerPrefix+\"_\");\n        if contains(paramName,\"LayerNorm\")\n            [subname,subsubname] = iParseLayerNorm(paramName);\n            weightsStruct.(\"embeddings\").(subname).(subsubname) = weight;\n        else\n            weightsStruct.(\"embeddings\").(paramName) = weight;\n        end\n        \n    elseif startsWith(name,poolingLayerPrefix)\n        paramName = extractAfter(name,poolingLayerPrefix+\"_\");\n        weightsStruct.(\"pooler\").(paramName) = weight;\n        \n    elseif startsWith(name,langModPrefix)\n        paramName = extractAfter(name,langModPrefix+\"_\");\n        [subname,subsubname] = iParseLM(paramName);\n        weightsStruct.(\"masked_LM\").(subname).(subsubname) = weight;\n        \n    elseif startsWith(name,nspPrefix)\n        paramName = extractAfter(name,nspPrefix+\"_\");\n        if strcmp(paramName,\"weights\")\n            % This parameter wasn't renamed and transposed before\n            % uploading. We can fix it here.\n            paramName = \"kernel\";\n            weight = weight.';\n        end\n        weightsStruct.(\"sequence_relation\").(paramName) = weight;\n        \n    elseif startsWith(name,genericClassifierPrefix)\n        paramName = extractAfter(name,genericClassifierPrefix);\n        weightsStruct.(\"classifier\").(paramName) = weight;\n    end\nend\nend\n\nfunction name = iParseAttentionOrFeedforward(name)\nif contains(name, \"attention\")\n    name = \"attention\";\nelse\n    name = \"feedforward\";\nend\nend\n\nfunction [name,subname] = iParseAttentionAndFeedforwardParamName(name,attnOrFeedforward)\nswitch attnOrFeedforward\n    case \"attention\"\n        [name,subname] = iParseAttentionParamName(name);\n    case \"feedforward\"\n        [name,subname] = iParseFeedforwardParamName(name);\nend\nend\n\nfunction [subname,subsubname] = iParseAttentionParamName(name)\nif contains(name,\"LayerNorm\")\n    [subname,subsubname] = iParseLayerNorm(name);\nelse\n    name = strrep(name,\"self_\",\"\");\n    name = strrep(name,\"_dense\",\"\");\n    subname = extractBetween(name,\"attention_\",\"_\");\n    subsubname = extractAfter(name,subname+\"_\");\nend\nend\n\nfunction [subname,subsubname] = iParseFeedforwardParamName(name)\nif contains(name,\"LayerNorm\")\n    [subname,subsubname] = iParseLayerNorm(name);\nelse\n    subname = extractBefore(name,\"_\");\n    subsubname = extractAfter(name,\"dense_\");\nend\nend\n\nfunction [subname,subsubname] = iParseLayerNorm(name)\nsubname = \"LayerNorm\";\nsubsubname = extractAfter(name,\"LayerNorm_\");\nend\n\nfunction [subname,subsubname] = iParseLM(name)\nif contains(name,\"LayerNorm\")\n    [subname,subsubname] = iParseLayerNorm(name);\nelse\n    name = strrep(name,\"dense_\",\"\");\n    subname = extractBefore(name,\"_\");\n    subsubname = extractAfter(name,\"_\");\nend    \nend scope_id: createParameterStruct scope_type: script",
  "name: bert.internal.predictMaskedToken file_path: +bert/+internal/predictMaskedToken.m start_line: 1 end_line: 7 input_parameters: ['mdl', 'x', 'maskIdx', 'k'] output_variables: ['toks', 'probs'] code_snippet: function [toks,probs] = predictMaskedToken(mdl,x,maskIdx,k)\narguments\n    mdl\n    x\n    maskIdx\n    k (1,1) double {mustBePositive,mustBeInteger} = 1\nend scope_id: predictMaskedToken scope_type: script",
  "name: predictMaskedToken file_path: +bert/+internal/predictMaskedToken.m start_line: 1 end_line: 14 code_snippet: function [toks,probs] = predictMaskedToken(mdl,x,maskIdx,k)\narguments\n    mdl\n    x\n    maskIdx\n    k (1,1) double {mustBePositive,mustBeInteger} = 1\nend\nprobs = bert.languageModel(x,mdl.Parameters);\nprobs = extractdata(probs(:,maskIdx));\n[~,idx] = maxk(probs,k);\ntoks = mdl.Tokenizer.FullTokenizer.decode(idx);\nend\n    \n     scope_id: predictMaskedToken scope_type: script",
  "name: bert.internal.getSupportFilePath file_path: +bert/+internal/getSupportFilePath.m start_line: 1 end_line: 10 input_parameters: ['modelName', 'fileName'] code_snippet: function filePath = getSupportFilePath(modelName,fileName)\n% getSupportFilePath   This function is for converting any differences\n% between the model names presented to the user and the support files\n% URLs.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string\n    fileName (1,1) string\nend scope_id: getSupportFilePath scope_type: script",
  "name: getSupportFilePath file_path: +bert/+internal/getSupportFilePath.m start_line: 1 end_line: 19 code_snippet: function filePath = getSupportFilePath(modelName,fileName)\n% getSupportFilePath   This function is for converting any differences\n% between the model names presented to the user and the support files\n% URLs.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string\n    fileName (1,1) string\nend\ndirectory = bert.internal.convertModelNameToDirectories(modelName);\nsd = matlab.internal.examples.utils.getSupportFileDir();\nlocalFile = fullfile(sd,\"nnet\",directory{:},fileName);\nif exist(localFile,'file')~=2\n    disp(\"Downloading \"+fileName+\" to: \"+localFile);\nend\nfileURL = strjoin([directory,fileName],\"/\");\nfilePath = matlab.internal.examples.downloadSupportFile(\"nnet\",fileURL);\nend scope_id: getSupportFilePath scope_type: script",
  "name: bert.internal.convertModelNameToDirectories file_path: +bert/+internal/convertModelNameToDirectories.m start_line: 1 end_line: 8 input_parameters: ['name'] code_snippet: function dirpath = convertModelNameToDirectories(name)\n% convertModelNameToDirectories   Converts the user facing model name to\n% the directory name used by support files.\n\n% Copyright 2021-2023 The MathWorks, Inc.\narguments\n    name (1,1) string\nend scope_id: convertModelNameToDirectories scope_type: script",
  "name: bert.internal.userInputToSupportFileName file_path: +bert/+internal/convertModelNameToDirectories.m start_line: 16 end_line: 22 input_parameters: ['name'] code_snippet: \nfunction supportfileName = userInputToSupportFileName(name)\npersistent map;\nif isempty(map)\n    names = namesArray();\n    map = containers.Map(names(:,1),names(:,2));\nend scope_id: convertModelNameToDirectories scope_type: script",
  "name: bert.internal.namesArray file_path: +bert/+internal/convertModelNameToDirectories.m start_line: 25 end_line: 36 code_snippet: \nfunction names = namesArray()\nnames = [\n    \"base\",               \"uncased_L12_H768_A12\";\n    \"multilingual-cased\", \"multicased_L12_H768_A12\";\n    \"medium\",             \"uncased_L8_H512_A8\";\n    \"small\",              \"uncased_L4_H512_A8\";\n    \"mini\",               \"uncased_L4_H256_A4\";\n    \"tiny\",               \"uncased_L2_H128_A2\";\n    \"japanese-base-wwm\",  \"\";\n    \"japanese-base\",      \"\"];\nend scope_id: convertModelNameToDirectories scope_type: script",
  "name: convertModelNameToDirectories file_path: +bert/+internal/convertModelNameToDirectories.m start_line: 1 end_line: 36 code_snippet: function dirpath = convertModelNameToDirectories(name)\n% convertModelNameToDirectories   Converts the user facing model name to\n% the directory name used by support files.\n\n% Copyright 2021-2023 The MathWorks, Inc.\narguments\n    name (1,1) string\nend\nmodelName = userInputToSupportFileName(name);\nbertBaseLocation = \"bert\";\nif contains(name,\"japanese\")\n    bertBaseLocation = \"ja_\" + bertBaseLocation;\nend\ndirpath = {\"data\",\"networks\",bertBaseLocation,modelName};\nend\n\nfunction supportfileName = userInputToSupportFileName(name)\npersistent map;\nif isempty(map)\n    names = namesArray();\n    map = containers.Map(names(:,1),names(:,2));\nend\nsupportfileName = map(name);\nend\n\nfunction names = namesArray()\nnames = [\n    \"base\",               \"uncased_L12_H768_A12\";\n    \"multilingual-cased\", \"multicased_L12_H768_A12\";\n    \"medium\",             \"uncased_L8_H512_A8\";\n    \"small\",              \"uncased_L4_H512_A8\";\n    \"mini\",               \"uncased_L4_H256_A4\";\n    \"tiny\",               \"uncased_L2_H128_A2\";\n    \"japanese-base-wwm\",  \"\";\n    \"japanese-base\",      \"\"];\nend scope_id: convertModelNameToDirectories scope_type: script",
  "name: bert.internal.inferTypeID file_path: +bert/+internal/inferTypeID.m start_line: 1 end_line: 8 input_parameters: ['x', 'separatorCode'] code_snippet: function types = inferTypeID(x,separatorCode)\n% infer the typeIDs from a CTB unlabeled array x\nxsz = size(x);\ntypes = ones(xsz);\nsepId = x==separatorCode;\nif isa(sepId,'dlarray')\n    sepId = extractdata(sepId);\nend scope_id: inferTypeID scope_type: script",
  "name: inferTypeID file_path: +bert/+internal/inferTypeID.m start_line: 1 end_line: 17 code_snippet: function types = inferTypeID(x,separatorCode)\n% infer the typeIDs from a CTB unlabeled array x\nxsz = size(x);\ntypes = ones(xsz);\nsepId = x==separatorCode;\nif isa(sepId,'dlarray')\n    sepId = extractdata(sepId);\nend\n% Find which observations have >1 separator - when there is 1 separator,\n% any padding is considered \"type 1\".\ncs = cumsum(sepId,2);\nobsNeedsType2 = cs(:,end,:)>1;\n% Type 2 tokens are those between the first (exclusive) and second\n% separator (inclusive) if a second separator was present.\ntype2positions = circshift(cs==1,1) & obsNeedsType2;\ntypes(type2positions) = 2;\nend scope_id: inferTypeID scope_type: script",
  "name: BERTTokenizer file_path: +bert/+tokenizer/BERTTokenizer.m line_range: 1-51 scope_id: BERTTokenizer scope_type: script",
  "name: bert.tokenizer.BERTTokenizer file_path: +bert/+tokenizer/BERTTokenizer.m start_line: 62 end_line: 108 input_parameters: ['vocabFile', 'nvp'] code_snippet:         function this = BERTTokenizer(vocabFile,nvp)\n            % BERTTokenizer   Construct a tokenizer to use with BERT\n            % models.\n            %\n            %   tokenizer = BERTTokenizer()   Constructs a case-insensitive\n            %   BERTTokenizer using the BERT-Base vocabulary file.\n            %\n            %   tokenizer = BERTTokenizer(vocabFile)   Constructs a\n            %   case-insensitive BERTTokenizer using the file vocabFile as\n            %   the vocabulary.\n            %\n            %   tokenizer = BERTTokenizer(vocabFile,'PARAM1', VAL1, 'PARAM2', VAL2, ...) \n            %   specifies the optional parameter name/value pairs:\n            %\n            %   'IgnoreCase'           - A logical value to control if the\n            %                            BERTTokenizer is case sensitive or not.\n            %                            The default value is true.\n            %\n            %   'FullTokenizer'        - The underlying word-piece tokenizer.\n            %                            If not specified, a default\n            %                            FullTokenizer is constructed.\n            %\n            %   BERTTokenizer properties:\n            %     FullTokenizer  - The underlying word-piece tokenizer.\n            %     PaddingToken   - The string \"[PAD]\"\n            %     StartToken     - The string \"[CLS]\"\n            %     SeparatorToken - The string \"[SEP]\"\n            %     MaskToken      - The string \"[MASK]\"\n            %     PaddingCode    - The encoded PaddingToken\n            %     StartCode      - The encoded StartToken\n            %     SeparatorCode  - The encoded SeparatorToken\n            %     MaskCode       - The encoded MaskToken\n            %\n            %   BERTTokenizer methods:\n            %     tokenize     - Tokenize strings\n            %     encode       - Tokenize and encode strings\n            %     encodeTokens - Encode pre-tokenized token sequences\n            %     decode       - Decode an encoded sequence to string\n            %\n            % Example:\n            %    tokenizer = bert.tokenizer.BERTTokenizer();\n            %   sequences = tokenizer.encode(\"Hello World!\")\n            arguments\n                vocabFile (1,1) string {mustBeFile} = bert.internal.getSupportFilePath(\"base\",\"vocab.txt\")\n                nvp.IgnoreCase (1,1) logical = true\n                nvp.FullTokenizer = []\n            end scope_id: BERTTokenizer scope_type: script",
  "name: bert.tokenizer.tokenize file_path: +bert/+tokenizer/BERTTokenizer.m start_line: 121 end_line: 147 input_parameters: ['this', 'text_a', 'text_b'] code_snippet:         \n        function tokens = tokenize(this,text_a,text_b)\n            % tokenize   Tokenizes a batch of strings and adds special\n            % tokens for the BERT model.\n            %\n            %   tokens = tokenize(bertTokenizer,text) tokenizes text using\n            %   the BERTTokenizer specified by bertTokenizer. The input\n            %   text is a string array. The output tokens is a cell-array\n            %   where tokens{i} is a string array corresponding to the\n            %   tokenized text(i). The start token and separator token are\n            %   preprended and appended respectively.\n            %\n            %   tokens = tokenize(bertTokenizer,text_a,text_b) tokenizes\n            %   the sentence-pairs (text_a,text_b). Here tokens{i}\n            %   corresponds to the tokenized form of the sentence pair\n            %   (text_a(i),text_b(i)), including the separator token\n            %   between the tokenized text_a(i) and text_b(i). The inputs\n            %   text_a and text_b must have the same number of elements.\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer;\n            %   tokens = tokenizer.tokenize(\"Hello world!\")\n            arguments\n                this\n                text_a string\n                text_b string = string.empty()\n            end scope_id: BERTTokenizer scope_type: script",
  "name: bert.tokenizer.encodeTokens file_path: +bert/+tokenizer/BERTTokenizer.m start_line: 162 end_line: 178 input_parameters: ['this', 'toks'] code_snippet:         \n        function x = encodeTokens(this,toks)\n            % encodeTokens   Encodes pre-tokenized tokens.\n            %\n            %   seqs = encodeTokens(bertTokenizer,tokens) encodes the\n            %   cell-array tokens into a cell-array of sequences of\n            %   positive integers seqs. For a BERTTokenizer specified by\n            %   bertTokenizer the output of\n            %   encodeTokens(bertTokenizer,tokenize(bertTokenizer,text)) is\n            %   equivalent to encode(bertTokenizer,text).\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer;\n            %   tokens = tokenizer.tokenize(\"Hello world!\");\n            %   sequences = tokenizer.encodeTokens(tokens)\n            x = cellfun(@(tokens) this.FullTokenizer.encode(tokens), toks, 'UniformOutput', false);\n        end scope_id: BERTTokenizer scope_type: script",
  "name: bert.tokenizer.encode file_path: +bert/+tokenizer/BERTTokenizer.m start_line: 179 end_line: 203 input_parameters: ['this', 'text_a', 'text_b'] code_snippet:         \n        function x = encode(this,text_a,text_b)\n            % encode   Tokenizes and encodes strings.\n            %\n            %   x = encode(bertTokenizer,text) will tokenize\n            %   and encode the string array specified by text into a\n            %   cell-array of sequences of positive integers. The encoded\n            %   start token and separator token are prepended and appended\n            %   to each sequence respectively.\n            %\n            %   x = encode(tok,text_a,text_b) For a sentence-pair task\n            %   each input sentence text_a and text_b is encoded, then the\n            %   encoded sequences are joined on an encoded separator token.\n            %   The inputs text_a and text_b must have the same number of\n            %   elements.\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer;\n            %   sequences = tokenizer.encode([\"Hello world!\"; ...\n            %   \"I am a model.\"])\n            arguments\n                this\n                text_a string\n                text_b string = string.empty()\n            end scope_id: BERTTokenizer scope_type: script",
  "name: bert.tokenizer.decode file_path: +bert/+tokenizer/BERTTokenizer.m start_line: 210 end_line: 235 input_parameters: ['this', 'x'] code_snippet:         \n        function text = decode(this,x)\n            % decode   Decode sequences of encoded tokens back to their\n            % string form and join on a space.\n            %\n            %   text = decode(bertTokenizer,x) decodes an input of encoded\n            %   tokens x into string tokens and joins these on a space\n            %   character to create the output text. The input x can be a\n            %   cell-array of sequences of positive integers or an array\n            %   of positive integers with size\n            %   1-by-numInputSubWords-by-numObs. Note that decode is not\n            %   the inverse to encode as the output of decode joins tokens\n            %   on a space character, and the tokenization performed by\n            %   encode is not inverted by the decode method.\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer();\n            %   sequences = tokenizer.encode(\"Hello World!\");\n            %   decoded = tokenizer.decode(sequences)\n            \n            if ~iscell(x)\n                % assume x is CTB\n                x = mat2cell(x,size(x,1),size(x,2),ones(size(x,3),1));\n                % return as a column vector.\n                x = reshape(x,[],1);\n            end scope_id: BERTTokenizer scope_type: script",
  "name: BERTTokenizer file_path: +bert/+tokenizer/BERTTokenizer.m start_line: 1 end_line: 241 code_snippet: classdef BERTTokenizer\n    % BERTTokenizer   Construct a tokenizer to use with BERT\n    % models.\n    %\n    %   tokenizer = BERTTokenizer()   Constructs a case-insensitive\n    %   BERTTokenizer using the BERT-Base vocabulary file.\n    %\n    %   tokenizer = BERTTokenizer(vocabFile)   Constructs a\n    %   case-insensitive BERTTokenizer using the file vocabFile as\n    %   the vocabulary.\n    %\n    %   tokenizer = BERTTokenizer(vocabFile,'PARAM1', VAL1, 'PARAM2', VAL2, ...) \n    %   specifies the optional parameter name/value pairs:\n    %\n    %   'IgnoreCase'           - A logical value to control if the\n    %                            BERTTokenizer is case sensitive or not.\n    %                            The default value is true.\n    %\n    %   'FullTokenizer'        - The underlying word-piece tokenizer.\n    %                            If not specified, a default\n    %                            FullTokenizer is constructed.\n    %\n    %   BERTTokenizer properties:\n    %     FullTokenizer  - The underlying word-piece tokenizer.\n    %     PaddingToken   - The string \"[PAD]\"\n    %     StartToken     - The string \"[CLS]\"\n    %     SeparatorToken - The string \"[SEP]\"\n    %     MaskToken      - The string \"[MASK]\"\n    %     PaddingCode    - The encoded PaddingToken\n    %     StartCode      - The encoded StartToken\n    %     SeparatorCode  - The encoded SeparatorToken\n    %     MaskCode       - The encoded MaskToken\n    %\n    %   BERTTokenizer methods:\n    %     tokenize     - Tokenize strings\n    %     encode       - Tokenize and encode strings\n    %     encodeTokens - Encode pre-tokenized token sequences\n    %     decode       - Decode an encoded sequence to string\n    %\n    % Example:\n    %   tokenizer = bert.tokenizer.BERTTokenizer();\n    %   sequences = tokenizer.encode(\"Hello World!\")\n    \n    % Copyright 2021-2023 The MathWorks, Inc.\n    \n    properties(Constant)\n        PaddingToken = \"[PAD]\"\n        StartToken = \"[CLS]\"\n        SeparatorToken = \"[SEP]\"\n        MaskToken = \"[MASK]\"\n    end\n    \n    properties(GetAccess=public,SetAccess=private)\n        FullTokenizer\n        PaddingCode\n        SeparatorCode\n        StartCode\n        MaskCode\n    end\n    \n    methods\n        function this = BERTTokenizer(vocabFile,nvp)\n            % BERTTokenizer   Construct a tokenizer to use with BERT\n            % models.\n            %\n            %   tokenizer = BERTTokenizer()   Constructs a case-insensitive\n            %   BERTTokenizer using the BERT-Base vocabulary file.\n            %\n            %   tokenizer = BERTTokenizer(vocabFile)   Constructs a\n            %   case-insensitive BERTTokenizer using the file vocabFile as\n            %   the vocabulary.\n            %\n            %   tokenizer = BERTTokenizer(vocabFile,'PARAM1', VAL1, 'PARAM2', VAL2, ...) \n            %   specifies the optional parameter name/value pairs:\n            %\n            %   'IgnoreCase'           - A logical value to control if the\n            %                            BERTTokenizer is case sensitive or not.\n            %                            The default value is true.\n            %\n            %   'FullTokenizer'        - The underlying word-piece tokenizer.\n            %                            If not specified, a default\n            %                            FullTokenizer is constructed.\n            %\n            %   BERTTokenizer properties:\n            %     FullTokenizer  - The underlying word-piece tokenizer.\n            %     PaddingToken   - The string \"[PAD]\"\n            %     StartToken     - The string \"[CLS]\"\n            %     SeparatorToken - The string \"[SEP]\"\n            %     MaskToken      - The string \"[MASK]\"\n            %     PaddingCode    - The encoded PaddingToken\n            %     StartCode      - The encoded StartToken\n            %     SeparatorCode  - The encoded SeparatorToken\n            %     MaskCode       - The encoded MaskToken\n            %\n            %   BERTTokenizer methods:\n            %     tokenize     - Tokenize strings\n            %     encode       - Tokenize and encode strings\n            %     encodeTokens - Encode pre-tokenized token sequences\n            %     decode       - Decode an encoded sequence to string\n            %\n            % Example:\n            %    tokenizer = bert.tokenizer.BERTTokenizer();\n            %   sequences = tokenizer.encode(\"Hello World!\")\n            arguments\n                vocabFile (1,1) string {mustBeFile} = bert.internal.getSupportFilePath(\"base\",\"vocab.txt\")\n                nvp.IgnoreCase (1,1) logical = true\n                nvp.FullTokenizer = []\n            end\n            if isempty(nvp.FullTokenizer)\n                ignoreCase = nvp.IgnoreCase;\n                this.FullTokenizer = bert.tokenizer.internal.FullTokenizer(vocabFile,'IgnoreCase',ignoreCase);\n            else\n                mustBeA(nvp.FullTokenizer,'bert.tokenizer.internal.FullTokenizer');\n                this.FullTokenizer = nvp.FullTokenizer;\n            end\n            this.PaddingCode = this.FullTokenizer.encode(this.PaddingToken);\n            this.SeparatorCode = this.FullTokenizer.encode(this.SeparatorToken);\n            this.StartCode = this.FullTokenizer.encode(this.StartToken);\n            this.MaskCode = this.FullTokenizer.encode(this.MaskToken);\n        end\n        \n        function tokens = tokenize(this,text_a,text_b)\n            % tokenize   Tokenizes a batch of strings and adds special\n            % tokens for the BERT model.\n            %\n            %   tokens = tokenize(bertTokenizer,text) tokenizes text using\n            %   the BERTTokenizer specified by bertTokenizer. The input\n            %   text is a string array. The output tokens is a cell-array\n            %   where tokens{i} is a string array corresponding to the\n            %   tokenized text(i). The start token and separator token are\n            %   preprended and appended respectively.\n            %\n            %   tokens = tokenize(bertTokenizer,text_a,text_b) tokenizes\n            %   the sentence-pairs (text_a,text_b). Here tokens{i}\n            %   corresponds to the tokenized form of the sentence pair\n            %   (text_a(i),text_b(i)), including the separator token\n            %   between the tokenized text_a(i) and text_b(i). The inputs\n            %   text_a and text_b must have the same number of elements.\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer;\n            %   tokens = tokenizer.tokenize(\"Hello world!\")\n            arguments\n                this\n                text_a string\n                text_b string = string.empty()\n            end\n            if ~isempty(text_b) && numel(text_a)~=numel(text_b)\n                error(\"bert:tokenizer:SentencePairNumelMismatch\",\"For sentence-pairs, both inputs must have the same number of elements\");\n            end\n            inputShape = size(text_a);\n            text_a = reshape(text_a,[],1);\n            text_b = reshape(text_b,[],1);\n            tokens = this.FullTokenizer.tokenize(text_a);\n            if ~isempty(text_b)\n                tokens_b = this.FullTokenizer.tokenize(text_b);\n                tokens = cellfun(@(tokens_a,tokens_b) [tokens_a,this.SeparatorToken,tokens_b], tokens, tokens_b, 'UniformOutput', false);\n            end\n             tokens = cellfun(@(tokens) [this.SeparatorToken, tokens, this.StartToken], tokens, 'UniformOutput', false);\n            tokens = reshape(tokens,inputShape);\n        end\n        \n        function x = encodeTokens(this,toks)\n            % encodeTokens   Encodes pre-tokenized tokens.\n            %\n            %   seqs = encodeTokens(bertTokenizer,tokens) encodes the\n            %   cell-array tokens into a cell-array of sequences of\n            %   positive integers seqs. For a BERTTokenizer specified by\n            %   bertTokenizer the output of\n            %   encodeTokens(bertTokenizer,tokenize(bertTokenizer,text)) is\n            %   equivalent to encode(bertTokenizer,text).\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer;\n            %   tokens = tokenizer.tokenize(\"Hello world!\");\n            %   sequences = tokenizer.encodeTokens(tokens)\n            x = cellfun(@(tokens) this.FullTokenizer.encode(tokens), toks, 'UniformOutput', false);\n        end\n        \n        function x = encode(this,text_a,text_b)\n            % encode   Tokenizes and encodes strings.\n            %\n            %   x = encode(bertTokenizer,text) will tokenize\n            %   and encode the string array specified by text into a\n            %   cell-array of sequences of positive integers. The encoded\n            %   start token and separator token are prepended and appended\n            %   to each sequence respectively.\n            %\n            %   x = encode(tok,text_a,text_b) For a sentence-pair task\n            %   each input sentence text_a and text_b is encoded, then the\n            %   encoded sequences are joined on an encoded separator token.\n            %   The inputs text_a and text_b must have the same number of\n            %   elements.\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer;\n            %   sequences = tokenizer.encode([\"Hello world!\"; ...\n            %   \"I am a model.\"])\n            arguments\n                this\n                text_a string\n                text_b string = string.empty()\n            end\n            if ~isempty(text_b) && numel(text_a)~=numel(text_b)\n                error(\"bert:tokenizer:SentencePairNumelMismatch\",\"For sentence-pairs, both inputs must have the same number of elements\");\n            end\n            tokens = this.tokenize(text_b,text_a);\n            x = this.encodeTokens(tokens);\n        end\n        \n        function text = decode(this,x)\n            % decode   Decode sequences of encoded tokens back to their\n            % string form and join on a space.\n            %\n            %   text = decode(bertTokenizer,x) decodes an input of encoded\n            %   tokens x into string tokens and joins these on a space\n            %   character to create the output text. The input x can be a\n            %   cell-array of sequences of positive integers or an array\n            %   of positive integers with size\n            %   1-by-numInputSubWords-by-numObs. Note that decode is not\n            %   the inverse to encode as the output of decode joins tokens\n            %   on a space character, and the tokenization performed by\n            %   encode is not inverted by the decode method.\n            %\n            % Example:\n            %   tokenizer = bert.tokenizer.BERTTokenizer();\n            %   sequences = tokenizer.encode(\"Hello World!\");\n            %   decoded = tokenizer.decode(sequences)\n            \n            if ~iscell(x)\n                % assume x is CTB\n                x = mat2cell(x,size(x,1),size(x,2),ones(size(x,3),1));\n                % return as a column vector.\n                x = reshape(x,[],1);\n            end\n            tokens = cellfun(@(s) this.FullTokenizer.decode(s(2:end-1)), x, 'UniformOutput', false);\n            text = cellfun(@(x) join(x,\" \"), tokens);\n        end\n    end\nend\n scope_id: BERTTokenizer scope_type: script",
  "name: WordPieceTokenizer file_path: +bert/+tokenizer/+internal/WordPieceTokenizer.m line_range: 1-11 superclass: bert scope_id: WordPieceTokenizer scope_type: script",
  "name: bert.tokenizer.internal.mustBeFileOrEncoding file_path: +bert/+tokenizer/+internal/WordPieceTokenizer.m start_line: 107 end_line: 111 input_parameters: ['x'] code_snippet: \nfunction mustBeFileOrEncoding(x)\nif ~isa(x,'wordEncoding')\n    mustBeFile(x);\nend scope_id: WordPieceTokenizer scope_type: script",
  "name: bert.tokenizer.internal.WordPieceTokenizer file_path: +bert/+tokenizer/+internal/WordPieceTokenizer.m start_line: 18 end_line: 34 input_parameters: ['vocab', 'nvp'] code_snippet:         function this = WordPieceTokenizer(vocab,nvp)\n            % tok = WordPieceTokenizer(vocab)   Constructs a\n            % WordPieceTokenizer with vocabulary file vocab.\n            %\n            % Name-Value Pair Arguments:\n            %   'UnknownToken'   - String to use for unknown tokens.\n            %                      Default is \"[UNK]\".\n            %\n            %   'MaxTokenLength' - Maximum length of a token. Tokens longer\n            %                      than this are replaced by\n            %                      'UnknownToken'.\n            %                      Default is 200.\n            arguments\n                vocab {mustBeFileOrEncoding}\n                nvp.UnknownToken (1,1) string = \"[UNK]\"\n                nvp.MaxTokenLength (1,1) {mustBePositive,mustBeInteger} = 200\n            end scope_id: WordPieceTokenizer scope_type: script",
  "name: bert.tokenizer.internal.tokenize file_path: +bert/+tokenizer/+internal/WordPieceTokenizer.m start_line: 39 end_line: 44 input_parameters: ['this', 'utext'] code_snippet:         \n        function tokens = tokenize(this,utext)\n            arguments\n                this\n                utext\n            end scope_id: WordPieceTokenizer scope_type: script",
  "name: bert.tokenizer.internal.parseVocab file_path: +bert/+tokenizer/+internal/WordPieceTokenizer.m start_line: 89 end_line: 92 input_parameters: ['~', 'vocab'] code_snippet:         function vocab = parseVocab(~,vocab)\n            if isa(vocab,'wordEncoding')\n                return\n            end scope_id: WordPieceTokenizer scope_type: script",
  "name: WordPieceTokenizer file_path: +bert/+tokenizer/+internal/WordPieceTokenizer.m start_line: 1 end_line: 112 code_snippet: classdef WordPieceTokenizer < bert.tokenizer.internal.Tokenizer\n    % WordPieceTokenizer   Constructs a Word Piece Tokenizer from a given\n    %   vocab.txt file.\n    \n    % Copyright 2021 The MathWorks, Inc.\n    \n    properties(SetAccess=private)\n        Vocab\n        Unk\n        MaxChar\n    end\n    \n    properties(Constant,Access=private)\n        WhitespaceTokenizer = bert.tokenizer.internal.WhitespaceTokenizer()\n    end\n    \n    methods\n        function this = WordPieceTokenizer(vocab,nvp)\n            % tok = WordPieceTokenizer(vocab)   Constructs a\n            % WordPieceTokenizer with vocabulary file vocab.\n            %\n            % Name-Value Pair Arguments:\n            %   'UnknownToken'   - String to use for unknown tokens.\n            %                      Default is \"[UNK]\".\n            %\n            %   'MaxTokenLength' - Maximum length of a token. Tokens longer\n            %                      than this are replaced by\n            %                      'UnknownToken'.\n            %                      Default is 200.\n            arguments\n                vocab {mustBeFileOrEncoding}\n                nvp.UnknownToken (1,1) string = \"[UNK]\"\n                nvp.MaxTokenLength (1,1) {mustBePositive,mustBeInteger} = 200\n            end\n            this.Unk = nvp.UnknownToken;\n            this.MaxChar = nvp.MaxTokenLength;\n            this.Vocab = this.parseVocab(vocab);\n        end\n        \n        function tokens = tokenize(this,utext)\n            arguments\n                this\n                utext\n            end\n            tokens = string.empty();\n            sub = textanalytics.unicode.UTF32();\n            for i = 1:numel(utext)\n                token = utext(i);\n                if numel(token.Data)>this.MaxChar\n                    tokens = [tokens,this.Unk]; %#ok\n                    continue\n                end\n                isBad = false;\n                start = 1;\n                subTokens = string.empty();\n                while start<(numel(token.Data)+1)\n                    finish = numel(token.Data);\n                    currentSub = [];\n                    while start<finish+1                        \n                        sub.Data = token.Data(start:finish);\n                        if start>1\n                            sub.Data = [uint32('##'),sub.Data];\n                        end\n                        strForm = sub.string();\n                        if this.Vocab.isVocabularyWord(strForm)\n                            currentSub = strForm;\n                            break\n                        end\n                        finish = finish-1;\n                    end\n                    if isempty(currentSub)\n                        isBad = true;\n                        break\n                    end\n                    subTokens(end+1) = currentSub;%#ok\n                    start = finish+1;\n                end\n                \n                if isBad\n                    tokens = [tokens, this.Unk];%#ok\n                else\n                    tokens = [tokens, subTokens];%#ok\n                end\n            end\n        end\n    end\n    \n    methods(Access=private)\n        function vocab = parseVocab(~,vocab)\n            if isa(vocab,'wordEncoding')\n                return\n            end\n            if exist(vocab,'file')~=2\n                error(\"Unknown vocabulary file\");\n            end\n            fid = fopen(vocab,'r','n','utf-8');\n            c = fread(fid,Inf);\n            fclose(fid);\n            c = native2unicode(c,'utf-8');%#ok\n            words = splitlines(c').';\n            empties = cellfun(@isempty,words);\n            words(empties) = [];\n            vocab = wordEncoding(words);\n        end\n    end\nend\n\nfunction mustBeFileOrEncoding(x)\nif ~isa(x,'wordEncoding')\n    mustBeFile(x);\nend\nend scope_id: WordPieceTokenizer scope_type: script",
  "name: TokenizedDocumentTokenizer file_path: +bert/+tokenizer/+internal/TokenizedDocumentTokenizer.m line_range: 1-10 superclass: bert scope_id: TokenizedDocumentTokenizer scope_type: script",
  "name: bert.tokenizer.internal.TokenizedDocumentTokenizer file_path: +bert/+tokenizer/+internal/TokenizedDocumentTokenizer.m start_line: 13 end_line: 16 input_parameters: ['varargin', 'args'] code_snippet:         function this = TokenizedDocumentTokenizer(varargin,args)\n            arguments(Repeating)\n                varargin\n            end scope_id: TokenizedDocumentTokenizer scope_type: script",
  "name: bert.tokenizer.internal.tokenize file_path: +bert/+tokenizer/+internal/TokenizedDocumentTokenizer.m start_line: 23 end_line: 28 input_parameters: ['this', 'txt'] code_snippet:         \n        function toks = tokenize(this,txt)\n            arguments\n                this\n                txt (1,:) string\n            end scope_id: TokenizedDocumentTokenizer scope_type: script",
  "name: TokenizedDocumentTokenizer file_path: +bert/+tokenizer/+internal/TokenizedDocumentTokenizer.m start_line: 1 end_line: 36 code_snippet: classdef TokenizedDocumentTokenizer < bert.tokenizer.internal.Tokenizer\n    % TokenizedDocumentTokenizer   Implements a word-level tokenizer using\n    % tokenizedDocument. \n\n     % Copyright 2023 The MathWorks, Inc.\n    \n    properties\n        TokenizedDocumentOptions\n        IgnoreCase\n    end\n    \n    methods\n        function this = TokenizedDocumentTokenizer(varargin,args)\n            arguments(Repeating)\n                varargin\n            end\n            arguments\n                args.IgnoreCase (1,1) logical = true\n            end\n            this.IgnoreCase = args.IgnoreCase;\n            this.TokenizedDocumentOptions = varargin;\n        end\n        \n        function toks = tokenize(this,txt)\n            arguments\n                this\n                txt (1,:) string\n            end\n            if this.IgnoreCase\n                txt = lower(txt);\n            end\n            t = tokenizedDocument(txt,this.TokenizedDocumentOptions{:});\n            toks = doc2cell(t);\n        end\n    end\nend scope_id: TokenizedDocumentTokenizer scope_type: script",
  "name: BasicTokenizer file_path: +bert/+tokenizer/+internal/BasicTokenizer.m line_range: 1-8 superclass: bert scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.stripAccents file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 134 end_line: 140 input_parameters: ['~', 'u', 'cats'] output_variables: ['u', 'cats'] code_snippet:         \n        function [u,cats] = stripAccents(~,u,cats)\n            for i = 1:numel(u)\n                isMn = cats{i}==\"Mn\";\n                u(i).Data(isMn) = [];\n                cats{i}(isMn) = [];\n            end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.BasicTokenizer file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 15 end_line: 24 input_parameters: ['nvp'] code_snippet:         function this = BasicTokenizer(nvp)\n            % BasicTokenizer()   Constructs a BasicTokenizer.\n            %\n            % Name-Value Pair Arguments:\n            %   'IgnoreCase'   - When set to true the text to be tokenized\n            %                    is lower cased during tokenization.\n            %                    Default is true.\n            arguments\n                nvp.IgnoreCase (1,1) logical = true\n            end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.tokenize file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 27 end_line: 32 input_parameters: ['this', 'text'] code_snippet:         \n        function tokens = tokenize(this,text)\n            arguments\n                this (1,1) bert.tokenizer.internal.BasicTokenizer\n                text (1,:) string\n            end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.whiteSpaceTokenize file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 58 end_line: 61 input_parameters: ['this', 'text'] code_snippet:         function tok = whiteSpaceTokenize(this,text)\n            % Simple whitespace tokenization\n            tok = this.WhiteSpaceTokenizer.tokenize(text);\n        end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.cleanText file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 62 end_line: 73 input_parameters: ['this', 'u'] code_snippet:         \n        function u = cleanText(this,u)\n            % normalize whitespace, remove control chars.\n            udata = u.Data;\n            ucats = u.characterCategories('Granularity','detailed');\n            ucats = ucats{1};\n            isWhitespace = this.whitespaceIdx(udata,ucats);\n            isControl = this.controlIdx(udata,ucats);\n            udata(isWhitespace) = uint32(' ');\n            udata(isControl) = [];\n            u.Data = udata;\n        end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.whitespaceIdx file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 74 end_line: 79 input_parameters: ['~', 'udata', 'cats'] code_snippet:         \n        function tf = whitespaceIdx(~,udata,cats)\n            whitespaceChar = uint32(sprintf(' \\n\\r\\t')).';\n            whitespaceCat = 'Zs';\n            tf = (any(udata==whitespaceChar,1)|any(string(cats)==whitespaceCat,1));\n        end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.controlIdx file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 80 end_line: 88 input_parameters: ['~', 'udata', 'cats'] code_snippet:         \n        function tf = controlIdx(~,udata,cats)\n            % We want to keep certain whitespace control characters\n            controlExceptions = uint32(sprintf('\\t\\n\\r')).';\n            controlChar = uint32([0;0xfffd]);\n            controlCat = [\"Cc\";\"Cf\"];\n            isControlCat = ~any(udata==controlExceptions,1) & any(string(cats)==controlCat,1);\n            tf = (any(udata==controlChar,1)|isControlCat);\n        end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.splitOnPunc file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 89 end_line: 96 input_parameters: ['~', 'uarray', 'catsCell'] code_snippet:         \n        function out = splitOnPunc(~,uarray,catsCell)\n            % splits on punctuation characters\n            out = cell(numel(uarray),1);\n            function u = createUTF32FromData(data)\n                u = textanalytics.unicode.UTF32();\n                u.Data = data;\n            end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.createUTF32FromData file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 93 end_line: 96 input_parameters: ['data'] code_snippet:             function u = createUTF32FromData(data)\n                u = textanalytics.unicode.UTF32();\n                u.Data = data;\n            end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.tokenizeCJK file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 117 end_line: 122 input_parameters: ['~', 'u'] code_snippet:         \n        function u = tokenizeCJK(~,u)\n            udata = u.Data;\n            if isempty(udata)\n                return\n            end scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.inRange file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 144 end_line: 147 input_parameters: ['num', 'lower', 'upper'] code_snippet: \nfunction tf = inRange(num,lower,upper)\ntf = num>=lower & num<=upper;\nend scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.isCJK file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 148 end_line: 157 input_parameters: ['udata'] code_snippet: \nfunction tf = isCJK(udata)\ntf = inRange(udata,0x4E00,0x9FFF)|...\n    inRange(udata,0x3400,0x4DBF)|...\n    inRange(udata,0x20000,0x2A6DF)|...\n    inRange(udata,0x2A700,0x2B73F)|...\n    inRange(udata,0x2B820,0x2CEAF)|...\n    inRange(udata,0xF900,0xFAFF)|...\n    inRange(udata,0x2F800,0x2FA1F);\nend scope_id: BasicTokenizer scope_type: script",
  "name: bert.tokenizer.internal.isPunctuation file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 158 end_line: 168 input_parameters: ['u', 'cats'] code_snippet: \nfunction tf = isPunctuation(u,cats)\nudata = u.Data;\ntf = ...\n    inRange(udata,33,47)|...\n    inRange(udata,58,64)|...\n    inRange(udata,91,96)|...\n    inRange(udata,123,126);\ncats = string(cats);\ntf = (tf)|(cats.startsWith(\"P\"));\nend scope_id: BasicTokenizer scope_type: script",
  "name: BasicTokenizer file_path: +bert/+tokenizer/+internal/BasicTokenizer.m start_line: 1 end_line: 169 code_snippet: classdef BasicTokenizer < bert.tokenizer.internal.Tokenizer\n    % BasicTokenizer   Perform basic tokenization.\n    \n    % Copyright 2020-2023 The MathWorks, Inc.\n    \n    properties(SetAccess=private)\n        IgnoreCase\n    end\n    \n    properties(Constant,Access=private)\n        WhiteSpaceTokenizer = bert.tokenizer.internal.WhitespaceTokenizer()\n    end\n    \n    methods\n        function this = BasicTokenizer(nvp)\n            % BasicTokenizer()   Constructs a BasicTokenizer.\n            %\n            % Name-Value Pair Arguments:\n            %   'IgnoreCase'   - When set to true the text to be tokenized\n            %                    is lower cased during tokenization.\n            %                    Default is true.\n            arguments\n                nvp.IgnoreCase (1,1) logical = true\n            end\n            this.IgnoreCase = nvp.IgnoreCase;\n        end\n        \n        function tokens = tokenize(this,text)\n            arguments\n                this (1,1) bert.tokenizer.internal.BasicTokenizer\n                text (1,:) string\n            end\n            tokens = cell(1,numel(string));\n            for i = 1:numel(text)\n                thisText = text(i);\n                u = textanalytics.unicode.UTF32(thisText);\n                u = this.cleanText(u);\n                u = this.tokenizeCJK(u);\n                thisText = u.string();\n                if this.IgnoreCase\n                    thisText = lower(thisText);\n                    thisText = textanalytics.unicode.nfd(thisText);\n                end\n                u = textanalytics.unicode.UTF32(thisText);\n                cats = u.characterCategories('Granularity','detailed');\n                if this.IgnoreCase\n                    [u,cats] = this.stripAccents(u,cats);\n                end\n                theseTokens = this.splitOnPunc(u,cats);\n                theseTokens = join(cat(2,theseTokens{:}),\" \");\n                theseTokens = this.whiteSpaceTokenize(theseTokens);\n                tokens{i} = theseTokens;\n            end\n        end\n    end\n    \n    methods(Access=private)\n        function tok = whiteSpaceTokenize(this,text)\n            % Simple whitespace tokenization\n            tok = this.WhiteSpaceTokenizer.tokenize(text);\n        end\n        \n        function u = cleanText(this,u)\n            % normalize whitespace, remove control chars.\n            udata = u.Data;\n            ucats = u.characterCategories('Granularity','detailed');\n            ucats = ucats{1};\n            isWhitespace = this.whitespaceIdx(udata,ucats);\n            isControl = this.controlIdx(udata,ucats);\n            udata(isWhitespace) = uint32(' ');\n            udata(isControl) = [];\n            u.Data = udata;\n        end\n        \n        function tf = whitespaceIdx(~,udata,cats)\n            whitespaceChar = uint32(sprintf(' \\n\\r\\t')).';\n            whitespaceCat = 'Zs';\n            tf = (any(udata==whitespaceChar,1)|any(string(cats)==whitespaceCat,1));\n        end\n        \n        function tf = controlIdx(~,udata,cats)\n            % We want to keep certain whitespace control characters\n            controlExceptions = uint32(sprintf('\\t\\n\\r')).';\n            controlChar = uint32([0;0xfffd]);\n            controlCat = [\"Cc\";\"Cf\"];\n            isControlCat = ~any(udata==controlExceptions,1) & any(string(cats)==controlCat,1);\n            tf = (any(udata==controlChar,1)|isControlCat);\n        end\n        \n        function out = splitOnPunc(~,uarray,catsCell)\n            % splits on punctuation characters\n            out = cell(numel(uarray),1);\n            function u = createUTF32FromData(data)\n                u = textanalytics.unicode.UTF32();\n                u.Data = data;\n            end\n            for i = 1:numel(uarray)\n                u = uarray(i);\n                cats = catsCell{i};\n                isPunc = isPunctuation(u,cats);\n                if ~any(isPunc)\n                    % early return\n                    out{i} = u.string();                    \n                else\n                    isBeforePunc = circshift(isPunc,-1);\n                    isBeforePunc(end) = 0;\n                    splitBoundary = isPunc | isBeforePunc;\n                    % We always split at the end if it's not already a boundary\n                    splitBoundary(end) = 1;\n                    splitLens = diff([0,find(splitBoundary)]);\n                    newdata = mat2cell(u.Data,1,splitLens);\n                    newU = cellfun(@(data) createUTF32FromData(data), newdata);\n                    out{i} = newU.string();\n                end\n            end\n        end\n        \n        function u = tokenizeCJK(~,u)\n            udata = u.Data;\n            if isempty(udata)\n                return\n            end\n            cjkCodepoints = isCJK(udata);\n            % For each CJK codepoint we add a preceding and succeeding space\n            newIdxDelta = 2*cumsum(cjkCodepoints);\n            % But only the preceding space affects the position of the CJK\n            % codepoint.\n            newIdxDelta(cjkCodepoints)=newIdxDelta(cjkCodepoints)-1;\n            newIdx = (1:numel(udata)) + newIdxDelta;\n            newdata = uint32(' ').*ones(1,newIdx(end),'uint32');\n            newdata(newIdx) = udata;\n            u.Data = newdata;\n        end\n        \n        function [u,cats] = stripAccents(~,u,cats)\n            for i = 1:numel(u)\n                isMn = cats{i}==\"Mn\";\n                u(i).Data(isMn) = [];\n                cats{i}(isMn) = [];\n            end\n        end\n    end\nend\n\nfunction tf = inRange(num,lower,upper)\ntf = num>=lower & num<=upper;\nend\n\nfunction tf = isCJK(udata)\ntf = inRange(udata,0x4E00,0x9FFF)|...\n    inRange(udata,0x3400,0x4DBF)|...\n    inRange(udata,0x20000,0x2A6DF)|...\n    inRange(udata,0x2A700,0x2B73F)|...\n    inRange(udata,0x2B820,0x2CEAF)|...\n    inRange(udata,0xF900,0xFAFF)|...\n    inRange(udata,0x2F800,0x2FA1F);\nend\n\nfunction tf = isPunctuation(u,cats)\nudata = u.Data;\ntf = ...\n    inRange(udata,33,47)|...\n    inRange(udata,58,64)|...\n    inRange(udata,91,96)|...\n    inRange(udata,123,126);\ncats = string(cats);\ntf = (tf)|(cats.startsWith(\"P\"));\nend\n scope_id: BasicTokenizer scope_type: script",
  "name: FullTokenizer file_path: +bert/+tokenizer/+internal/FullTokenizer.m line_range: 1-46 superclass: bert scope_id: FullTokenizer scope_type: script",
  "name: bert.tokenizer.internal.FullTokenizer file_path: +bert/+tokenizer/+internal/FullTokenizer.m start_line: 49 end_line: 91 input_parameters: ['vocab', 'nvp'] code_snippet:         function this = FullTokenizer(vocab,nvp)\n            % FullTokenizer   A tokenizer based on word-piece tokenization.\n            %\n            %   tokenizer = FullTokenizer(vocabFile) constructs a FullTokenizer\n            %   using the vocabulary specified in the newline delimited txt file\n            %   vocabFile.\n            %\n            %   tokenizer = FullTokenizer(vocabFile,'PARAM1', VAL1, 'PARAM2', VAL2, ...) specifies\n            %   the optional parameter name/value pairs:\n            %\n            %   'BasicTokenizer'       - Tokenizer used to split text into words.\n            %                            If not specified, a default\n            %                            BasicTokenizer is constructed.\n            %\n            %   'IgnoreCase'           - A logical value to control if the\n            %                            FullTokenizer is case sensitive or not.\n            %                            The default value is true.\n            %\n            %   FullTokenizer methods:\n            %     tokenize - tokenize text\n            %     encode   - encode tokens\n            %     decode   - decode encoded tokens\n            %\n            % Example:\n            %   % Save a file named fakeVocab.txt with the text on the next 3 lines:\n            %   fake\n            %   vo\n            %   ##cab\n            %\n            %   % Now create a FullTokenizer\n            %   tokenizer = bert.tokenizer.internal.FullTokenizer('fakeVocab.txt');\n            %   tokens = tokenizer.tokenize(\"This tokenizer has a fake vocab\")\n            %   % Note that most tokens are unknown as they are not in the\n            %   % vocabulary and neither are any sub-tokens. However \"fake\" is\n            %   % detected and \"vocab\" is split into \"vo\" and \"##cab\".\n            %   tokenizer.encode(tokens)\n            %   % This returns the encoded form of the tokens - each token is\n            %   % replaced by its corresponding line number in the fakeVocab.txt\n            arguments\n                vocab\n                nvp.BasicTokenizer = []\n                nvp.IgnoreCase = true\n            end scope_id: FullTokenizer scope_type: script",
  "name: bert.tokenizer.internal.tokenize file_path: +bert/+tokenizer/+internal/FullTokenizer.m start_line: 102 end_line: 118 input_parameters: ['this', 'txt'] code_snippet:         \n        function toks = tokenize(this,txt)\n            % tokenize   Tokenizes text.\n            % \n            %   tokens = tokenize(tokenizer,text) tokenizes the input\n            %   string text using the FullTokenizer specified by tokenizer.\n            basicToks = this.Basic.tokenize(txt);\n            toks = cell(numel(txt),1);\n            for i = 1:numel(txt)\n                theseBasicToks = textanalytics.unicode.UTF32(basicToks{i});\n                theseSubToks = cell(numel(theseBasicToks),1);\n                for j = 1:numel(theseBasicToks)\n                    if mod(j,2) == 0 || isempty(theseBasicToks(j))\n                        theseSubToks{j} = this.WordPiece.tokenize(theseBasicToks(j));\n                    else\n                        theseSubToks{j} = theseBasicToks(j);\n                    end scope_id: FullTokenizer scope_type: script",
  "name: bert.tokenizer.internal.encode file_path: +bert/+tokenizer/+internal/FullTokenizer.m start_line: 123 end_line: 130 input_parameters: ['this', 'tokens'] code_snippet:         \n        function idx = encode(this,tokens)\n            % encode   Encodes tokens.\n            %\n            %   encoded = encode(tokenizer,tokens) encodes the string array\n            %   tokens using the FullTokenizer specified by tokenizer.\n            idx = this.Encoding.word2ind(tokens);\n        end scope_id: FullTokenizer scope_type: script",
  "name: bert.tokenizer.internal.decode file_path: +bert/+tokenizer/+internal/FullTokenizer.m start_line: 131 end_line: 138 input_parameters: ['this', 'x'] code_snippet:         \n        function tokens = decode(this,x)\n            % decode   Decodes tokens.\n            %\n            %   decoded = decode(tokenizer,x) decodes the array of positive\n            %   integers x into the string array decoded.\n            tokens = this.Encoding.ind2word(x);\n        end scope_id: FullTokenizer scope_type: script",
  "name: FullTokenizer file_path: +bert/+tokenizer/+internal/FullTokenizer.m start_line: 1 end_line: 141 code_snippet: classdef FullTokenizer < bert.tokenizer.internal.Tokenizer\n    % FullTokenizer   A tokenizer based on word-piece tokenization.\n    %\n    %   tokenizer = FullTokenizer(vocabFile) constructs a FullTokenizer\n    %   using the vocabulary specified in the newline delimited txt file\n    %   vocabFile.\n    %\n    %   tokenizer = FullTokenizer(vocabFile,'PARAM1', VAL1, 'PARAM2', VAL2, ...)\n    %   specifies the optional parameter name/value pairs:\n    %\n    %   'BasicTokenizer'       - Tokenizer used to split text into words.\n    %                            If not specified, a default\n    %                            BasicTokenizer is constructed.\n    %\n    %   'IgnoreCase'           - A logical value to control if the\n    %                            FullTokenizer is case sensitive or not.\n    %                            The default value is true.\n    %\n    %   FullTokenizer methods:\n    %     tokenize - tokenize text\n    %     encode   - encode tokens\n    %     decode   - decode encoded tokens\n    %\n    % Example:\n    %   % Save a file named fakeVocab.txt with the text on the next 3 lines:\n    %   fake\n    %   vo\n    %   ##cab\n    %\n    %   % Now create a FullTokenizer\n    %   tokenizer = bert.tokenizer.internal.FullTokenizer('fakeVocab.txt');\n    %   tokens = tokenizer.tokenize(\"This tokenizer has a fake vocab\")\n    %   % Note that most tokens are unknown as they are not in the\n    %   % vocabulary and neither are any sub-tokens. However \"fake\" is\n    %   % detected and \"vocab\" is split into \"vo\" and \"##cab\".\n    %   tokenizer.encode(tokens)\n    %   % This returns the encoded form of the tokens - each token is\n    %   % replaced by its corresponding line number in the fakeVocab.txt\n    \n    % Copyright 2021-2023 The MathWorks, Inc.\n    \n    properties(Access=private)\n        Basic\n        WordPiece\n        Encoding\n    end\n    \n    methods\n        function this = FullTokenizer(vocab,nvp)\n            % FullTokenizer   A tokenizer based on word-piece tokenization.\n            %\n            %   tokenizer = FullTokenizer(vocabFile) constructs a FullTokenizer\n            %   using the vocabulary specified in the newline delimited txt file\n            %   vocabFile.\n            %\n            %   tokenizer = FullTokenizer(vocabFile,'PARAM1', VAL1, 'PARAM2', VAL2, ...) specifies\n            %   the optional parameter name/value pairs:\n            %\n            %   'BasicTokenizer'       - Tokenizer used to split text into words.\n            %                            If not specified, a default\n            %                            BasicTokenizer is constructed.\n            %\n            %   'IgnoreCase'           - A logical value to control if the\n            %                            FullTokenizer is case sensitive or not.\n            %                            The default value is true.\n            %\n            %   FullTokenizer methods:\n            %     tokenize - tokenize text\n            %     encode   - encode tokens\n            %     decode   - decode encoded tokens\n            %\n            % Example:\n            %   % Save a file named fakeVocab.txt with the text on the next 3 lines:\n            %   fake\n            %   vo\n            %   ##cab\n            %\n            %   % Now create a FullTokenizer\n            %   tokenizer = bert.tokenizer.internal.FullTokenizer('fakeVocab.txt');\n            %   tokens = tokenizer.tokenize(\"This tokenizer has a fake vocab\")\n            %   % Note that most tokens are unknown as they are not in the\n            %   % vocabulary and neither are any sub-tokens. However \"fake\" is\n            %   % detected and \"vocab\" is split into \"vo\" and \"##cab\".\n            %   tokenizer.encode(tokens)\n            %   % This returns the encoded form of the tokens - each token is\n            %   % replaced by its corresponding line number in the fakeVocab.txt\n            arguments\n                vocab\n                nvp.BasicTokenizer = []\n                nvp.IgnoreCase = true\n            end\n            if isempty(nvp.BasicTokenizer)\n                % Default case\n                this.Basic = bert.tokenizer.internal.BasicTokenizer('IgnoreCase',nvp.IgnoreCase);\n            else\n                mustBeA(nvp.BasicTokenizer,'bert.tokenizer.internal.Tokenizer');\n                this.Basic = nvp.BasicTokenizer;\n            end\n            this.WordPiece = bert.tokenizer.internal.WordPieceTokenizer(vocab);\n            this.Encoding = this.WordPiece.Vocab;\n        end\n        \n        function toks = tokenize(this,txt)\n            % tokenize   Tokenizes text.\n            % \n            %   tokens = tokenize(tokenizer,text) tokenizes the input\n            %   string text using the FullTokenizer specified by tokenizer.\n            basicToks = this.Basic.tokenize(txt);\n            toks = cell(numel(txt),1);\n            for i = 1:numel(txt)\n                theseBasicToks = textanalytics.unicode.UTF32(basicToks{i});\n                theseSubToks = cell(numel(theseBasicToks),1);\n                for j = 1:numel(theseBasicToks)\n                    if mod(j,2) == 0 || isempty(theseBasicToks(j))\n                        theseSubToks{j} = this.WordPiece.tokenize(theseBasicToks(j));\n                    else\n                        theseSubToks{j} = theseBasicToks(j);\n                    end\n                end\n                toks{i} = cat(2,theseSubToks{:});\n            end\n        end\n        \n        function idx = encode(this,tokens)\n            % encode   Encodes tokens.\n            %\n            %   encoded = encode(tokenizer,tokens) encodes the string array\n            %   tokens using the FullTokenizer specified by tokenizer.\n            idx = this.Encoding.word2ind(tokens);\n        end\n        \n        function tokens = decode(this,x)\n            % decode   Decodes tokens.\n            %\n            %   decoded = decode(tokenizer,x) decodes the array of positive\n            %   integers x into the string array decoded.\n            tokens = this.Encoding.ind2word(x);\n        end\n    end\nend\n scope_id: FullTokenizer scope_type: script",
  "name: WhitespaceTokenizer file_path: +bert/+tokenizer/+internal/WhitespaceTokenizer.m line_range: 1-14 superclass: bert scope_id: WhitespaceTokenizer scope_type: script",
  "name: bert.tokenizer.internal.tokenize file_path: +bert/+tokenizer/+internal/WhitespaceTokenizer.m start_line: 8 end_line: 14 input_parameters: ['~', 'text'] code_snippet:         function text = tokenize(~,text)\n            % tokens = tokenize(tok,str)   Returns an array of tokens formed\n            %                              by splitting str on whitespace.\n            arguments\n                ~\n                text\n            end scope_id: WhitespaceTokenizer scope_type: script",
  "name: WhitespaceTokenizer file_path: +bert/+tokenizer/+internal/WhitespaceTokenizer.m start_line: 1 end_line: 20 code_snippet: classdef WhitespaceTokenizer < bert.tokenizer.internal.Tokenizer\n    % WhitespaceTokenizer   The simplest type of tokenization, split on\n    % whitespace characters.\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    methods\n        function text = tokenize(~,text)\n            % tokens = tokenize(tok,str)   Returns an array of tokens formed\n            %                              by splitting str on whitespace.\n            arguments\n                ~\n                text\n            end\n            text = strip(text);\n            text = split(text).';\n        end\n    end\nend\n scope_id: WhitespaceTokenizer scope_type: script",
  "name: Tokenizer file_path: +bert/+tokenizer/+internal/Tokenizer.m start_line: 1 end_line: 6 code_snippet: classdef(Abstract) Tokenizer\n    %Tokenizer  Interface for tokenizer classes\n    methods(Abstract)\n        tokenize(this,text)\n    end\nend scope_id: Tokenizer scope_type: script",
  "name: finbert file_path: test/finbert/tsentimentModel.m start_line: 9 end_line: 22 code_snippet:         function canUseSentimentAnalysisModel(test)\n            % Verify we can use finbert.sentiment_analysis with the\n            % finbert() model.\n            \n            mdl = finbert();\n            strs = [\"Good day on the markets.\";\"My portfolio is down by 10%\"];\n            seqs = mdl.Tokenizer.encode(strs);\n            x = padsequences(seqs,2,'PaddingValue',mdl.Tokenizer.PaddingCode);\n            [sentClass,sentScore] = finbert.sentimentModel(x,mdl.Parameters);\n            % Regression test against some current values.\n            test.verifyEqual(sentClass,categorical([\"neutral\",\"negative\"]));\n            test.verifyEqual(string(categories(sentClass)),[\"positive\";\"negative\";\"neutral\"]);            \n            test.verifyEqual(extractdata(sentScore),single([0.4561, -0.9558]),'AbsTol',single(1e-4));\n        end         scope_id: tsentimentModel scope_type: script",
  "name: tsentimentModel file_path: test/finbert/tsentimentModel.m start_line: 1 end_line: 24 code_snippet: classdef(SharedTestFixtures={\n        DownloadFinBERTFixture}) ...\n        tsentimentModel < matlab.unittest.TestCase\n    % tsentimentAnalysis   Unit test for finbert.sentiment_analysis\n    \n    % Copyright 2021 The MathWorks, Inc.\n    \n    methods(Test)        \n        function canUseSentimentAnalysisModel(test)\n            % Verify we can use finbert.sentiment_analysis with the\n            % finbert() model.\n            \n            mdl = finbert();\n            strs = [\"Good day on the markets.\";\"My portfolio is down by 10%\"];\n            seqs = mdl.Tokenizer.encode(strs);\n            x = padsequences(seqs,2,'PaddingValue',mdl.Tokenizer.PaddingCode);\n            [sentClass,sentScore] = finbert.sentimentModel(x,mdl.Parameters);\n            % Regression test against some current values.\n            test.verifyEqual(sentClass,categorical([\"neutral\",\"negative\"]));\n            test.verifyEqual(string(categories(sentClass)),[\"positive\";\"negative\";\"neutral\"]);            \n            test.verifyEqual(extractdata(sentScore),single([0.4561, -0.9558]),'AbsTol',single(1e-4));\n        end        \n    end    \nend scope_id: tsentimentModel scope_type: script",
  "name: finbert file_path: test/finbert/tlanguageModel.m start_line: 12 end_line: 18 input_parameters: [\"'Model'\", '\"language-model\"'] code_snippet:         \n        function canBatch(test)\n            mdl = finbert('Model', \"language-model\");\n            x = dlarray(repmat(1:10,[1,1,2]));\n            probs = extractdata(finbert.languageModel(x,mdl.Parameters));\n            test.verifyEqual(probs(:,:,1),probs(:,:,2),'AbsTol',single(1e-6));\n        end scope_id: tlanguageModel scope_type: script",
  "name: tlanguageModel file_path: test/finbert/tlanguageModel.m start_line: 1 end_line: 49 code_snippet: classdef(SharedTestFixtures={DownloadFinBERTFixture}) ...\n        tlanguageModel < matlab.unittest.TestCase\n    % tlanguageModel   Unit test for finbert.languageModel\n    \n    % Copyright 2021 The MathWorks, Inc.  \n    \n    properties(Constant)\n        FunctionUnderTest = @(x, p)finbert.languageModel(x, p);\n    end\n    \n    methods(Test)\n        \n        function canBatch(test)\n            mdl = finbert('Model', \"language-model\");\n            x = dlarray(repmat(1:10,[1,1,2]));\n            probs = extractdata(finbert.languageModel(x,mdl.Parameters));\n            test.verifyEqual(probs(:,:,1),probs(:,:,2),'AbsTol',single(1e-6));\n        end\n        \n        function canPredictMaskedTokens(test)\n            mdl = finbert('Model','language-model');\n            str = \"This investment has no return on value.\";\n            seq = mdl.Tokenizer.encode(str);\n            x = dlarray(seq{1});\n            % Replace some sequence element with a mask\n            toMask = size(x,2)-2;\n            x(toMask) = mdl.Tokenizer.MaskCode;\n            z = finbert.languageModel(x,mdl.Parameters);\n            k = 3;\n            maskProbabilities = z(:,toMask);\n            [~,topk] = maxk(extractdata(maskProbabilities),k);\n            toks = arrayfun(@(idx) mdl.Tokenizer.decode(idx), topk);\n            % Regression test against hard coded values.\n            test.verifyEqual(toks,[\"investment\";\"value\";\"equity\"]);\n        end\n        \n        function languageModelErrorsWithSAParams(test)\n            % Test that bert.languageModel errors when using a sentiment\n            % analysis input model parameters.\n            \n            mdl = finbert('Model', 'sentiment-model');\n            str = \"This investment has no return on value.\";    \n            lmFunction = @() finbert.languageModel(str, mdl.Parameters);\n            \n            test.verifyError(lmFunction, 'bert:languageModel:MissingLMWeights');\n        end\n        \n    end\nend scope_id: tlanguageModel scope_type: script",
  "name: DownloadBERTFixture file_path: test/tools/DownloadBERTFixture.m line_range: 1-13 superclass: matlab scope_id: DownloadBERTFixture scope_type: script",
  "name: pathToSupportFile file_path: test/tools/DownloadBERTFixture.m start_line: 43 end_line: 46 input_parameters: ['this', 'model'] code_snippet:         function path = pathToSupportFile(this,model)\n            modelDir = this.convertModelNameToDirectories(model);\n            path = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",modelDir);\n        end scope_id: DownloadBERTFixture scope_type: script",
  "name: convertModelNameToDirectories file_path: test/tools/DownloadBERTFixture.m start_line: 47 end_line: 51 input_parameters: ['~', 'modelNames'] code_snippet:         \n        function modelDirs = convertModelNameToDirectories(~,modelNames)\n            modelDirs = arrayfun(@(model) bert.internal.convertModelNameToDirectories(model), modelNames, 'UniformOutput', false);\n            modelDirs = cellfun(@(dirCell) fullfile(dirCell{:}), modelDirs);\n        end scope_id: DownloadBERTFixture scope_type: script",
  "name: DownloadBERTFixture file_path: test/tools/DownloadBERTFixture.m start_line: 1 end_line: 53 code_snippet: classdef DownloadBERTFixture < matlab.unittest.fixtures.Fixture\n    % DownloadBERTFixture   A fixture for downloading the BERT models and\n    % clearing them out after tests finish if they were not previously\n    % downloaded.\n    \n    % Copyright 2021 The MathWorks, Inc\n    \n    properties(Constant)\n        Models = [\n            \"base\"\n            \"multilingual-cased\"\n            \"tiny\"]\n    end\n    \n    properties\n        DataDirExists\n    end    \n    \n    methods        \n        function setup(this)\n            dirs = this.pathToSupportFile(this.Models);\n            dataDirsExist = arrayfun(@(dir) exist(dir,'dir')==7, dirs);\n            this.DataDirExists = containers.Map(this.Models,dataDirsExist);\n            for i=1:numel(this.Models)\n                model = this.Models(i);\n                if ~this.DataDirExists(model)\n                    bert('Model',model);\n                end\n            end\n        end\n        \n        function teardown(this)\n            for i=1:numel(this.Models)\n                model = this.Models(i);\n                if ~this.DataDirExists(model)\n                    rmdir(this.pathToSupportFile(model),'s');\n                end\n            end\n        end\n    end    \n    \n    methods(Access=private)\n        function path = pathToSupportFile(this,model)\n            modelDir = this.convertModelNameToDirectories(model);\n            path = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",modelDir);\n        end\n        \n        function modelDirs = convertModelNameToDirectories(~,modelNames)\n            modelDirs = arrayfun(@(model) bert.internal.convertModelNameToDirectories(model), modelNames, 'UniformOutput', false);\n            modelDirs = cellfun(@(dirCell) fullfile(dirCell{:}), modelDirs);\n        end\n    end\nend scope_id: DownloadBERTFixture scope_type: script",
  "name: DownloadFinBERTFixture file_path: test/tools/DownloadFinBERTFixture.m line_range: 1-11 superclass: matlab scope_id: DownloadFinBERTFixture scope_type: script",
  "name: pathToSupportFile file_path: test/tools/DownloadFinBERTFixture.m start_line: 41 end_line: 44 input_parameters: ['this', 'model'] code_snippet:         function path = pathToSupportFile(this,model)\n            modelDir = this.convertModelNameToDirectories(model);\n            path = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",modelDir);\n        end scope_id: DownloadFinBERTFixture scope_type: script",
  "name: convertModelNameToDirectories file_path: test/tools/DownloadFinBERTFixture.m start_line: 45 end_line: 49 input_parameters: ['~', 'modelNames'] code_snippet:         \n        function modelDirs = convertModelNameToDirectories(~,modelNames)\n            modelDirs = arrayfun(@(model) finbert.internal.convertModelNameToDirectories(model), modelNames, 'UniformOutput', false);\n            modelDirs = cellfun(@(dirCell) fullfile(dirCell{:}), modelDirs);\n        end scope_id: DownloadFinBERTFixture scope_type: script",
  "name: DownloadFinBERTFixture file_path: test/tools/DownloadFinBERTFixture.m start_line: 1 end_line: 51 code_snippet: classdef DownloadFinBERTFixture < matlab.unittest.fixtures.Fixture\n    % DownloadFinBERTFixture   A fixture for calling finbert.download if\n    % necessary. This is to ensure that this function is only called once\n    % and only when tests need it. It also provides a teardown to return\n    % the test environment to the expected state before testing.\n    \n    % Copyright 2021 The MathWorks, Inc\n    \n    properties(Constant)\n        Models = [\"sentiment-model\",\"language-model\"]\n    end\n    \n    properties\n        DataDirExists\n    end\n    \n    methods\n        function setup(this)\n            dirs = this.pathToSupportFile(this.Models);\n            dataDirsExist = arrayfun(@(dir) exist(dir,'dir')==7, dirs);\n            this.DataDirExists = containers.Map(this.Models,dataDirsExist);\n            for i=1:numel(this.Models)\n                model = this.Models(i);\n                if ~this.DataDirExists(model)\n                    finbert('Model',model);\n                end\n            end\n        end\n        \n        function teardown(this)\n            for i=1:numel(this.Models)\n                model = this.Models(i);\n                if ~this.DataDirExists(model)\n                    rmdir(this.pathToSupportFile(model),'s');\n                end\n            end\n        end\n    end\n    \n    methods(Access=private)\n        function path = pathToSupportFile(this,model)\n            modelDir = this.convertModelNameToDirectories(model);\n            path = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",modelDir);\n        end\n        \n        function modelDirs = convertModelNameToDirectories(~,modelNames)\n            modelDirs = arrayfun(@(model) finbert.internal.convertModelNameToDirectories(model), modelNames, 'UniformOutput', false);\n            modelDirs = cellfun(@(dirCell) fullfile(dirCell{:}), modelDirs);\n        end\n    end\nend scope_id: DownloadFinBERTFixture scope_type: script",
  "name: DownloadJPBERTFixture file_path: test/tools/DownloadJPBERTFixture.m line_range: 1-11 superclass: matlab scope_id: DownloadJPBERTFixture scope_type: script",
  "name: pathToSupportFile file_path: test/tools/DownloadJPBERTFixture.m start_line: 43 end_line: 46 input_parameters: ['~', 'model'] code_snippet:         function path = pathToSupportFile(~,model)\n            modelDir = fullfile(\"data\", \"networks\", \"ja_bert\", model);\n            path = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",modelDir);\n        end scope_id: DownloadJPBERTFixture scope_type: script",
  "name: DownloadJPBERTFixture file_path: test/tools/DownloadJPBERTFixture.m start_line: 1 end_line: 48 code_snippet: classdef DownloadJPBERTFixture < matlab.unittest.fixtures.Fixture\n    % DownloadJPBERTFixture   A fixture for downloading the Japanese BERT models and\n    % clearing them out after tests finish if they were not previously\n    % downloaded.\n    \n    % Copyright 2023 The MathWorks, Inc\n    \n    properties(Constant)\n        Models = dictionary([\"japanese-base\", \"japanese-base-wwm\"], ...\n            [\"bert-base-japanese\", \"bert-base-japanese-whole-word-masking\"]);\n    end\n    \n    properties\n        DataDirExists\n    end    \n    \n    methods        \n        function setup(this)\n            dirs = this.pathToSupportFile(this.Models.values);\n            dataDirsExist = arrayfun(@(dir) exist(dir,'dir')==7, dirs);\n            this.DataDirExists = dictionary(this.Models.keys,dataDirsExist);\n            modelNames = this.Models.keys;\n            for i=1:numel(modelNames)\n                model = modelNames(i);\n                if ~this.DataDirExists(model)\n                    bert('Model',model);\n                end\n            end\n        end\n        \n        function teardown(this)\n            modelNames = this.Models.keys;\n            for i=1:numel(modelNames)\n                model = modelNames(i);\n                if ~this.DataDirExists(model)\n                    rmdir(this.pathToSupportFile(this.Models(model)),'s');\n                end\n            end\n        end\n    end    \n    \n    methods(Access=private)\n        function path = pathToSupportFile(~,model)\n            modelDir = fullfile(\"data\", \"networks\", \"ja_bert\", model);\n            path = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",modelDir);\n        end\n    end\nend scope_id: DownloadJPBERTFixture scope_type: script",
  "name: DownloadGPT2Fixture file_path: test/tools/DownloadGPT2Fixture.m line_range: 1-12 superclass: matlab scope_id: DownloadGPT2Fixture scope_type: script",
  "name: exist file_path: test/tools/DownloadGPT2Fixture.m start_line: 20 end_line: 28 input_parameters: ['this.GPT2DataDir', \"'dir'\"] code_snippet:         function setup(this)\n            this.DataDirExists = exist(this.GPT2DataDir,'dir')==7;\n            dataFileExists = arrayfun(@(file) exist(fullfile(this.GPT2DataDir,file),'file')==2, this.GPT2DataFiles);\n            this.DataFileExists = containers.Map(this.GPT2DataFiles,dataFileExists);\n            if ~this.DataDirExists || any(~dataFileExists)\n                % Call this in eval to capture and drop any standard output\n                % that we don't want polluting the test logs.\n                evalc('gpt2();');\n            end scope_id: DownloadGPT2Fixture scope_type: script",
  "name: DownloadGPT2Fixture file_path: test/tools/DownloadGPT2Fixture.m start_line: 1 end_line: 42 code_snippet: classdef DownloadGPT2Fixture < matlab.unittest.fixtures.Fixture\n    % DownloadGPT2Fixture   A fixture for calling gpt2.download if\n    % necessary. This is to ensure that this function is only called once\n    % and only when tests need it. It also provides a teardown to return\n    % the test environment to the expected state before testing.\n    \n    % Copyright 2020 The MathWorks, Inc\n    \n    properties(Constant)\n        GPT2DataDir = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",\"data\",\"networks\")\n        GPT2DataFiles = [\"gpt2_355M_params.mat\",\"gpt2_encoder.txt\",\"gpt2_vocab.bpe\"]\n    end\n    \n    properties\n        DataDirExists (1,1) logical\n        DataFileExists\n    end\n    \n    methods\n        function setup(this)\n            this.DataDirExists = exist(this.GPT2DataDir,'dir')==7;\n            dataFileExists = arrayfun(@(file) exist(fullfile(this.GPT2DataDir,file),'file')==2, this.GPT2DataFiles);\n            this.DataFileExists = containers.Map(this.GPT2DataFiles,dataFileExists);\n            if ~this.DataDirExists || any(~dataFileExists)\n                % Call this in eval to capture and drop any standard output\n                % that we don't want polluting the test logs.\n                evalc('gpt2();');\n            end\n        end\n        \n        function teardown(this)\n            if ~this.DataDirExists\n                rmdir(this.GPT2DataDir,'s');\n            end\n            for file = this.GPT2DataFiles\n                if ~this.DataFileExists(file)\n                    delete(fullfile(this.GPT2DataDir,file));\n                end\n            end\n        end\n    end\nend scope_id: DownloadGPT2Fixture scope_type: script",
  "name: getRepoRoot file_path: test/tools/getRepoRoot.m start_line: 1 end_line: 11 code_snippet: function path = getRepoRoot()\n% getRepoRoot   Return a path to the repository's root directory.\n\n% Copyright 2020 The MathWorks, Inc.\n\nthisFile = mfilename('fullpath');\nthisDir = fileparts(thisFile);\n\n% the root is up two directories (<root>/test/tools/getRepoRoot.m)\npath = fullfile(thisDir,'..','..');\nend scope_id: getRepoRoot scope_type: script",
  "name: getRepoRoot file_path: test/tools/getRepoRoot.m start_line: 1 end_line: 11 code_snippet: function path = getRepoRoot()\n% getRepoRoot   Return a path to the repository's root directory.\n\n% Copyright 2020 The MathWorks, Inc.\n\nthisFile = mfilename('fullpath');\nthisDir = fileparts(thisFile);\n\n% the root is up two directories (<root>/test/tools/getRepoRoot.m)\npath = fullfile(thisDir,'..','..');\nend scope_id: getRepoRoot scope_type: script",
  "name: canEncodeSentencePairs file_path: test/bert/tmodel.m start_line: 59 end_line: 80 input_parameters: ['test'] code_snippet:         \n        function canEncodeSentencePairs(test)\n            % Test that multiple sequence pairs pass gives the same results\n            % with single sequence pair passes.\n            \n            txt_a = [\"blah blah\"; \"foo bar baz\"];\n            txt_b = [\"hello world\"; \"another nonsense string\"];\n            \n            mdl = bert();\n            seqs = mdl.Tokenizer.encode(txt_a, txt_b);\n            \n            paddingValue = mdl.Tokenizer.PaddingCode;\n            x = dlarray(padsequences(seqs, 2, 'PaddingValue', paddingValue));\n            \n            y = bert.model(x, mdl.Parameters);\n            y1 = bert.model(x(:, :, 1), mdl.Parameters);\n            y2 = bert.model(x(:, :, 2), mdl.Parameters);\n            \n            tol = matlab.unittest.constraints.AbsoluteTolerance(dlarray(single(1e-5)));\n            test.verifyThat(y(:, :, 1), iIsEqualTo(y1,'Within',tol));\n            test.verifyThat(y(:, :, 2), iIsEqualTo(y2,'Within',tol));\n        end scope_id: tmodel scope_type: script",
  "name: checkDuplicateSentence file_path: test/bert/tmodel.m start_line: 81 end_line: 98 input_parameters: ['test'] code_snippet:         \n        function checkDuplicateSentence(test)\n            % Check that the results per observation dimension match since\n            % the two input data observations are equal.\n            \n            txt = \"nipson anomemata me monan opsin\";\n            \n            mdl = bert();\n            seq = mdl.Tokenizer.encode(txt);\n            x1 = dlarray(seq{1});\n            \n            % Duplicate x1 in observation dimension\n            x = cat(3, x1, x1);\n            y = bert.model(x, mdl.Parameters);\n            \n            tol = matlab.unittest.constraints.AbsoluteTolerance(dlarray(single(1e-5)));\n            test.verifyThat(y(:, :, 1), iIsEqualTo(y(:, :, 2),'Within',tol));\n        end scope_id: tmodel scope_type: script",
  "name: defaultOutputsIsLastLayer file_path: test/bert/tmodel.m start_line: 99 end_line: 107 input_parameters: ['test'] code_snippet:         \n        function defaultOutputsIsLastLayer(test)\n            % Verify the Outputs NVP default is the last layer.\n            mdl = bert();\n            x = dlarray(1:10);\n            z1 = bert.model(x,mdl.Parameters);\n            z2 = bert.model(x,mdl.Parameters,'Outputs',mdl.Parameters.Hyperparameters.NumLayers);\n            test.verifyEqual(z1,z2);\n        end scope_id: tmodel scope_type: script",
  "name: outputsCanBeUsed file_path: test/bert/tmodel.m start_line: 108 end_line: 126 input_parameters: ['test'] code_snippet:         \n        function outputsCanBeUsed(test)\n            % Verify the Outputs NVP can be used.\n            mdl = bert();\n            seqlen = 10;\n            batchsize = 3;\n            x = dlarray(randi([1,100],[1,seqlen,batchsize]));\n            outputs = [1,3,5];\n            [z1,z3,z5] = bert.model(x,mdl.Parameters,'Outputs',outputs);\n            expSize = [mdl.Parameters.Hyperparameters.HiddenSize, seqlen, batchsize];\n            test.verifySize(z1,expSize);\n            test.verifySize(z3,expSize);\n            test.verifySize(z5,expSize);\n            % A final neat check is to modify the NumLayers and check the\n            % value of z5\n            mdl.Parameters.Hyperparameters.NumLayers = 5;\n            z5_exp = bert.model(x,mdl.Parameters);\n            test.verifyEqual(z5,z5_exp);\n        end scope_id: tmodel scope_type: script",
  "name: outputsCanDuplicateAndBeOutOfOrder file_path: test/bert/tmodel.m start_line: 127 end_line: 138 input_parameters: ['test'] code_snippet:         \n        function outputsCanDuplicateAndBeOutOfOrder(test)\n            % Subtle edge case\n            mdl = bert();\n            x = dlarray(1:10);\n            outputs = [5,12,2,5,1,2];\n            [z1,z2,z3,z4,~,z6] = bert.model(x,mdl.Parameters,'Outputs',outputs);\n            test.verifyEqual(z1,z4);\n            test.verifyEqual(z3,z6);\n            z_exp = bert.model(x,mdl.Parameters);\n            test.verifyEqual(z2,z_exp);\n        end scope_id: tmodel scope_type: script",
  "name: negativeTestOutputs file_path: test/bert/tmodel.m start_line: 139 end_line: 145 input_parameters: ['test', 'InvalidOutputs'] code_snippet:         \n        function negativeTestOutputs(test,InvalidOutputs)\n            % Check the Outputs NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'Outputs',InvalidOutputs.Value), InvalidOutputs.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: negativeTestSeparatorCode file_path: test/bert/tmodel.m start_line: 146 end_line: 152 input_parameters: ['test', 'InvalidCodes'] code_snippet:         \n        function negativeTestSeparatorCode(test, InvalidCodes)\n            % Check the SeparatorCode NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'SeparatorCode',InvalidCodes.Value), InvalidCodes.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: negativeTestPaddingCode file_path: test/bert/tmodel.m start_line: 153 end_line: 159 input_parameters: ['test', 'InvalidCodes'] code_snippet:         \n        function negativeTestPaddingCode(test, InvalidCodes)\n            % Check the PaddingCode NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'PaddingCode',InvalidCodes.Value), InvalidCodes.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: negativeTestDropoutProb file_path: test/bert/tmodel.m start_line: 160 end_line: 166 input_parameters: ['test', 'InvalidDropoutProb'] code_snippet:         \n        function negativeTestDropoutProb(test, InvalidDropoutProb)\n            % Check the DropoutProb NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'DropoutProb',InvalidDropoutProb.Value), InvalidDropoutProb.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: negativeTestAttentionDropoutProb file_path: test/bert/tmodel.m start_line: 167 end_line: 173 input_parameters: ['test', 'InvalidDropoutProb'] code_snippet:         \n        function negativeTestAttentionDropoutProb(test, InvalidDropoutProb)\n            % Check the AttentionDropoutProb NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'AttentionDropoutProb',InvalidDropoutProb.Value), InvalidDropoutProb.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: negativeInputMask file_path: test/bert/tmodel.m start_line: 174 end_line: 180 input_parameters: ['test', 'InvalidInputMask'] code_snippet:         \n        function negativeInputMask(test, InvalidInputMask)\n            % Check the InputMask NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'InputMask',InvalidInputMask.Value), InvalidInputMask.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: negativeInputX file_path: test/bert/tmodel.m start_line: 181 end_line: 186 input_parameters: ['test', 'InvalidInputX'] code_snippet:         \n        function negativeInputX(test, InvalidInputX)\n            % Check the input X errors in expected scenarios\n            mdl = bert();\n            test.verifyError(@() bert.model(InvalidInputX.Value,mdl.Parameters), InvalidInputX.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: negativeModelParameters file_path: test/bert/tmodel.m start_line: 187 end_line: 192 input_parameters: ['test', 'InvalidModelParams'] code_snippet:         \n        function negativeModelParameters(test, InvalidModelParams)\n            % Check the input model paramters errors in expected scenarios\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,InvalidModelParams.Value), InvalidModelParams.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: errorsForIncorrectModelName file_path: test/bert/tmodel.m start_line: 193 end_line: 197 input_parameters: ['test', 'InvalidModelName'] code_snippet:         \n        function errorsForIncorrectModelName(test, InvalidModelName)\n            % Check that 'Model' NVP errors in expected scenarios\n            test.verifyError(@() bert('Model', InvalidModelName.Value), InvalidModelName.ErrorID);\n        end scope_id: tmodel scope_type: script",
  "name: canUseInputMask file_path: test/bert/tmodel.m start_line: 198 end_line: 216 input_parameters: ['test'] code_snippet:         \n        function canUseInputMask(test)\n            % Verify the InputMask NVP works as expected\n            mdl = bert();\n            strs = [\"foo bar baz\";\"another exciting string with many words\"];\n            seqs = mdl.Tokenizer.encode(strs);\n            [x,xmask] = padsequences(seqs,2,'PaddingValue',mdl.Tokenizer.PaddingCode);\n            x = dlarray(x);\n            % Specifying the mask should match the default when the mask\n            % corresponds to padding tokens only.\n            y_default = bert.model(x,mdl.Parameters);\n            y_inputmask = bert.model(x,mdl.Parameters,'InputMask',xmask);\n            test.verifyEqual(y_default,y_inputmask);\n            % However the InputMask can be any logical with same size as x,\n            % e.g. you may want to attend to padding for some reason.\n            nomask = true(size(x));\n            y_nomask = bert.model(x,mdl.Parameters,'InputMask',nomask);\n            test.verifyNotEqual(y_inputmask,y_nomask);\n        end scope_id: tmodel scope_type: script",
  "name: dlarray file_path: test/bert/tmodel.m start_line: 18 end_line: 35 input_parameters: ['fake_input_ids('] code_snippet:         function matchesExpectedValue(test)\n            x = dlarray(fake_input_ids());\n            params = bert();\n            params = params.Parameters;\n            % Regression test against some expected values\n            v1 = -0.4484;\n            vend = 0.0752;\n            vmiddle = 0.3042;\n            z = bert.model(x,params);\n            z = extractdata(z);\n            z1 = z(1);\n            zend = z(end);\n            zmiddle = z(numel(z)/2);\n            tol = single(7e-3);\n            test.verifyEqual(z1,single(v1),'AbsTol',tol);\n            test.verifyEqual(zend,single(vend),'AbsTol',tol);\n            test.verifyEqual(zmiddle,single(vmiddle),'AbsTol',tol);\n        end scope_id: tmodel scope_type: script",
  "name: bert file_path: test/bert/tmodel.m start_line: 36 end_line: 58 code_snippet:         \n        function canBatch(test)\n            % Check that BERT returns the same result when either given\n            % a batch of sequences or sequence by sequence.\n            mdl = bert();\n            params = mdl.Parameters;\n            txt = [\"nipson anomemata\"; \"memonan opsin\"];\n            seqs = mdl.Tokenizer.encode(txt);\n            paddingValue = mdl.Tokenizer.PaddingCode;\n            seqsPadded = padsequences(seqs, 2, \"PaddingValue\", paddingValue);\n            \n            x = dlarray(seqsPadded);\n            x1 = x(:, :, 1);\n            x2 = x(:, :, 2);\n            \n            y1 = bert.model(x1, params);\n            y2 = bert.model(x2, params);\n            y = test.verifyWarningFree(@() bert.model(x, params));\n            \n            tolerance = iAbsoluteTolerance(single(1e-5));\n            test.verifyThat(cat(3, extractdata(y1), extractdata(y2)), ...\n                iIsEqualTo(extractdata(y), 'Within', tolerance));\n        end scope_id: tmodel scope_type: script",
  "name: fake_input_ids file_path: test/bert/tmodel.m start_line: 219 end_line: 225 code_snippet: \nfunction ids = fake_input_ids()\n% Who was Jim Henson ? ||| Jim Henson was a puppeteer\nids = [101,2040,2001,3958,27227,1029,1064,1064,1064,3958,27227,2001,1037,13997,11510,102];\n% matlab indexes from 1\nids = ids+1;\nend scope_id: tmodel scope_type: script",
  "name: iIsEqualTo file_path: test/bert/tmodel.m start_line: 226 end_line: 229 input_parameters: ['varargin'] code_snippet: \nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend scope_id: tmodel scope_type: script",
  "name: iAbsoluteTolerance file_path: test/bert/tmodel.m start_line: 230 end_line: 233 input_parameters: ['tol'] code_snippet: \nfunction constraint = iAbsoluteTolerance(tol)\nconstraint = matlab.unittest.constraints.AbsoluteTolerance(tol);\nend scope_id: tmodel scope_type: script",
  "name: iInvalidOutputs file_path: test/bert/tmodel.m start_line: 234 end_line: 241 code_snippet: \nfunction s = iInvalidOutputs()\ns = struct(...\n    'NonInteger',iInvalidInputCase(1.1,'MATLAB:validators:mustBeInteger'),...\n    'NonPositive', iInvalidInputCase(0,'MATLAB:validators:mustBePositive'),...\n    'NonDouble', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'),...\n    'TooBig', iInvalidInputCase([1,2,25],'MATLAB:validators:mustBeLessThanOrEqual'));\nend scope_id: tmodel scope_type: script",
  "name: iInvalidCodes file_path: test/bert/tmodel.m start_line: 242 end_line: 250 code_snippet: \nfunction s = iInvalidCodes()\ns = struct(...\n    'NonInteger', iInvalidInputCase(1.1,'MATLAB:validators:mustBeInteger'),...\n    'NonPositive', iInvalidInputCase(0,'MATLAB:validators:mustBePositive'),...\n    'NonNumeric', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'),...\n    'InvalidDims', iInvalidInputCase([1,2,25],'MATLAB:validation:IncompatibleSize'),...\n    'NonReal', iInvalidInputCase(1i,'MATLAB:validators:mustBeReal'));\nend scope_id: tmodel scope_type: script",
  "name: iInvalidDropoutProb file_path: test/bert/tmodel.m start_line: 251 end_line: 259 code_snippet: \nfunction s = iInvalidDropoutProb()\ns = struct(...\n    'NonPositive', iInvalidInputCase(-1,'MATLAB:validators:mustBeNonnegative'), ...\n    'Larger', iInvalidInputCase(1.5,'MATLAB:validators:mustBeLessThanOrEqual'), ...\n    'InvalidDims', iInvalidInputCase([1,2],'MATLAB:validation:IncompatibleSize'), ...\n    'NonReal', iInvalidInputCase(1i,'MATLAB:validators:mustBeReal'), ...\n    'NonNumeric', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'));\nend scope_id: tmodel scope_type: script",
  "name: iInvalidInputMask file_path: test/bert/tmodel.m start_line: 260 end_line: 266 code_snippet: \nfunction s = iInvalidInputMask()\ns = struct(...\n    'InvalidDims', iInvalidInputCase([true; true; false],\"bert:model:InvalidMaskSize\"), ...\n    'InvalidTypeString', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'), ...\n    'InvalidTypeCell', iInvalidInputCase({{1, 1, 0}},'MATLAB:validators:mustBeNumericOrLogical'));\nend scope_id: tmodel scope_type: script",
  "name: iInvalidInputX file_path: test/bert/tmodel.m start_line: 267 end_line: 275 code_snippet: \nfunction s = iInvalidInputX()\ns = struct(...\n    'InvalidTypeString', iInvalidInputCase(\"foo\",'MATLAB:validation:UnableToConvert'), ...\n    'InvalidTypeCell', iInvalidInputCase({{1}},'MATLAB:validation:UnableToConvert'), ...\n    'Logical', iInvalidInputCase(true,'MATLAB:validators:mustBeNumeric'), ...\n    'Complex', iInvalidInputCase(1 + 1i,'deep:dlarray:ComplexNotSupported'), ...\n    'IsEmpty', iInvalidInputCase([], 'MATLAB:validators:mustBeNonempty'));\nend scope_id: tmodel scope_type: script",
  "name: iInvalidModelParams file_path: test/bert/tmodel.m start_line: 276 end_line: 283 code_snippet: \nfunction s = iInvalidModelParams()\ns = struct(...\n    'NonStructNumerical', iInvalidInputCase(10,'MATLAB:validators:mustBeA'), ...\n    'NonStructString', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeA'), ...\n    'NonStructCell', iInvalidInputCase({10},'MATLAB:validators:mustBeA'), ...\n    'IsEmpty', iInvalidInputCase([], 'MATLAB:validators:mustBeA'));\nend scope_id: tmodel scope_type: script",
  "name: iInvalidModelName file_path: test/bert/tmodel.m start_line: 284 end_line: 290 code_snippet: \nfunction s = iInvalidModelName()\ns = struct(...\n    'NonStructNumerical', iInvalidInputCase(10,'MATLAB:validators:mustBeMember'), ...\n    'NonStructString', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeMember'), ...\n    'NonStructCell', iInvalidInputCase({10},'MATLAB:validators:mustBeMember'));\nend scope_id: tmodel scope_type: script",
  "name: iInvalidInputCase file_path: test/bert/tmodel.m start_line: 291 end_line: 296 input_parameters: ['val', 'id'] code_snippet: \nfunction s = iInvalidInputCase(val,id)\ns = struct(...\n    'Value',val,...\n    'ErrorID',id);\nend scope_id: tmodel scope_type: script",
  "name: tmodel file_path: test/bert/tmodel.m start_line: 1 end_line: 296 code_snippet: classdef(SharedTestFixtures = {\n        DownloadBERTFixture}) tmodel < matlab.unittest.TestCase\n    % Unit tests for bert.model\n    \n    % Copyright 2021 The MathWorks, Inc.\n    \n    properties(TestParameter)\n        InvalidOutputs = iInvalidOutputs()\n        InvalidCodes = iInvalidCodes()\n        InvalidDropoutProb = iInvalidDropoutProb()\n        InvalidInputMask = iInvalidInputMask()\n        InvalidInputX = iInvalidInputX()\n        InvalidModelParams = iInvalidModelParams()\n        InvalidModelName = iInvalidModelName()\n    end\n    \n    methods(Test)\n        function matchesExpectedValue(test)\n            x = dlarray(fake_input_ids());\n            params = bert();\n            params = params.Parameters;\n            % Regression test against some expected values\n            v1 = -0.4484;\n            vend = 0.0752;\n            vmiddle = 0.3042;\n            z = bert.model(x,params);\n            z = extractdata(z);\n            z1 = z(1);\n            zend = z(end);\n            zmiddle = z(numel(z)/2);\n            tol = single(7e-3);\n            test.verifyEqual(z1,single(v1),'AbsTol',tol);\n            test.verifyEqual(zend,single(vend),'AbsTol',tol);\n            test.verifyEqual(zmiddle,single(vmiddle),'AbsTol',tol);\n        end\n        \n        function canBatch(test)\n            % Check that BERT returns the same result when either given\n            % a batch of sequences or sequence by sequence.\n            mdl = bert();\n            params = mdl.Parameters;\n            txt = [\"nipson anomemata\"; \"memonan opsin\"];\n            seqs = mdl.Tokenizer.encode(txt);\n            paddingValue = mdl.Tokenizer.PaddingCode;\n            seqsPadded = padsequences(seqs, 2, \"PaddingValue\", paddingValue);\n            \n            x = dlarray(seqsPadded);\n            x1 = x(:, :, 1);\n            x2 = x(:, :, 2);\n            \n            y1 = bert.model(x1, params);\n            y2 = bert.model(x2, params);\n            y = test.verifyWarningFree(@() bert.model(x, params));\n            \n            tolerance = iAbsoluteTolerance(single(1e-5));\n            test.verifyThat(cat(3, extractdata(y1), extractdata(y2)), ...\n                iIsEqualTo(extractdata(y), 'Within', tolerance));\n        end\n        \n        function canEncodeSentencePairs(test)\n            % Test that multiple sequence pairs pass gives the same results\n            % with single sequence pair passes.\n            \n            txt_a = [\"blah blah\"; \"foo bar baz\"];\n            txt_b = [\"hello world\"; \"another nonsense string\"];\n            \n            mdl = bert();\n            seqs = mdl.Tokenizer.encode(txt_a, txt_b);\n            \n            paddingValue = mdl.Tokenizer.PaddingCode;\n            x = dlarray(padsequences(seqs, 2, 'PaddingValue', paddingValue));\n            \n            y = bert.model(x, mdl.Parameters);\n            y1 = bert.model(x(:, :, 1), mdl.Parameters);\n            y2 = bert.model(x(:, :, 2), mdl.Parameters);\n            \n            tol = matlab.unittest.constraints.AbsoluteTolerance(dlarray(single(1e-5)));\n            test.verifyThat(y(:, :, 1), iIsEqualTo(y1,'Within',tol));\n            test.verifyThat(y(:, :, 2), iIsEqualTo(y2,'Within',tol));\n        end\n        \n        function checkDuplicateSentence(test)\n            % Check that the results per observation dimension match since\n            % the two input data observations are equal.\n            \n            txt = \"nipson anomemata me monan opsin\";\n            \n            mdl = bert();\n            seq = mdl.Tokenizer.encode(txt);\n            x1 = dlarray(seq{1});\n            \n            % Duplicate x1 in observation dimension\n            x = cat(3, x1, x1);\n            y = bert.model(x, mdl.Parameters);\n            \n            tol = matlab.unittest.constraints.AbsoluteTolerance(dlarray(single(1e-5)));\n            test.verifyThat(y(:, :, 1), iIsEqualTo(y(:, :, 2),'Within',tol));\n        end\n        \n        function defaultOutputsIsLastLayer(test)\n            % Verify the Outputs NVP default is the last layer.\n            mdl = bert();\n            x = dlarray(1:10);\n            z1 = bert.model(x,mdl.Parameters);\n            z2 = bert.model(x,mdl.Parameters,'Outputs',mdl.Parameters.Hyperparameters.NumLayers);\n            test.verifyEqual(z1,z2);\n        end\n        \n        function outputsCanBeUsed(test)\n            % Verify the Outputs NVP can be used.\n            mdl = bert();\n            seqlen = 10;\n            batchsize = 3;\n            x = dlarray(randi([1,100],[1,seqlen,batchsize]));\n            outputs = [1,3,5];\n            [z1,z3,z5] = bert.model(x,mdl.Parameters,'Outputs',outputs);\n            expSize = [mdl.Parameters.Hyperparameters.HiddenSize, seqlen, batchsize];\n            test.verifySize(z1,expSize);\n            test.verifySize(z3,expSize);\n            test.verifySize(z5,expSize);\n            % A final neat check is to modify the NumLayers and check the\n            % value of z5\n            mdl.Parameters.Hyperparameters.NumLayers = 5;\n            z5_exp = bert.model(x,mdl.Parameters);\n            test.verifyEqual(z5,z5_exp);\n        end\n        \n        function outputsCanDuplicateAndBeOutOfOrder(test)\n            % Subtle edge case\n            mdl = bert();\n            x = dlarray(1:10);\n            outputs = [5,12,2,5,1,2];\n            [z1,z2,z3,z4,~,z6] = bert.model(x,mdl.Parameters,'Outputs',outputs);\n            test.verifyEqual(z1,z4);\n            test.verifyEqual(z3,z6);\n            z_exp = bert.model(x,mdl.Parameters);\n            test.verifyEqual(z2,z_exp);\n        end\n        \n        function negativeTestOutputs(test,InvalidOutputs)\n            % Check the Outputs NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'Outputs',InvalidOutputs.Value), InvalidOutputs.ErrorID);\n        end\n        \n        function negativeTestSeparatorCode(test, InvalidCodes)\n            % Check the SeparatorCode NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'SeparatorCode',InvalidCodes.Value), InvalidCodes.ErrorID);\n        end\n        \n        function negativeTestPaddingCode(test, InvalidCodes)\n            % Check the PaddingCode NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'PaddingCode',InvalidCodes.Value), InvalidCodes.ErrorID);\n        end\n        \n        function negativeTestDropoutProb(test, InvalidDropoutProb)\n            % Check the DropoutProb NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'DropoutProb',InvalidDropoutProb.Value), InvalidDropoutProb.ErrorID);\n        end\n        \n        function negativeTestAttentionDropoutProb(test, InvalidDropoutProb)\n            % Check the AttentionDropoutProb NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'AttentionDropoutProb',InvalidDropoutProb.Value), InvalidDropoutProb.ErrorID);\n        end\n        \n        function negativeInputMask(test, InvalidInputMask)\n            % Check the InputMask NVP errors in expected scenarios\n            mdl = bert();\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,mdl.Parameters,'InputMask',InvalidInputMask.Value), InvalidInputMask.ErrorID);\n        end\n        \n        function negativeInputX(test, InvalidInputX)\n            % Check the input X errors in expected scenarios\n            mdl = bert();\n            test.verifyError(@() bert.model(InvalidInputX.Value,mdl.Parameters), InvalidInputX.ErrorID);\n        end\n        \n        function negativeModelParameters(test, InvalidModelParams)\n            % Check the input model paramters errors in expected scenarios\n            x = dlarray([1,2,3]);\n            test.verifyError(@() bert.model(x,InvalidModelParams.Value), InvalidModelParams.ErrorID);\n        end\n        \n        function errorsForIncorrectModelName(test, InvalidModelName)\n            % Check that 'Model' NVP errors in expected scenarios\n            test.verifyError(@() bert('Model', InvalidModelName.Value), InvalidModelName.ErrorID);\n        end\n        \n        function canUseInputMask(test)\n            % Verify the InputMask NVP works as expected\n            mdl = bert();\n            strs = [\"foo bar baz\";\"another exciting string with many words\"];\n            seqs = mdl.Tokenizer.encode(strs);\n            [x,xmask] = padsequences(seqs,2,'PaddingValue',mdl.Tokenizer.PaddingCode);\n            x = dlarray(x);\n            % Specifying the mask should match the default when the mask\n            % corresponds to padding tokens only.\n            y_default = bert.model(x,mdl.Parameters);\n            y_inputmask = bert.model(x,mdl.Parameters,'InputMask',xmask);\n            test.verifyEqual(y_default,y_inputmask);\n            % However the InputMask can be any logical with same size as x,\n            % e.g. you may want to attend to padding for some reason.\n            nomask = true(size(x));\n            y_nomask = bert.model(x,mdl.Parameters,'InputMask',nomask);\n            test.verifyNotEqual(y_inputmask,y_nomask);\n        end\n    end\nend\n\nfunction ids = fake_input_ids()\n% Who was Jim Henson ? ||| Jim Henson was a puppeteer\nids = [101,2040,2001,3958,27227,1029,1064,1064,1064,3958,27227,2001,1037,13997,11510,102];\n% matlab indexes from 1\nids = ids+1;\nend\n\nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend\n\nfunction constraint = iAbsoluteTolerance(tol)\nconstraint = matlab.unittest.constraints.AbsoluteTolerance(tol);\nend\n\nfunction s = iInvalidOutputs()\ns = struct(...\n    'NonInteger',iInvalidInputCase(1.1,'MATLAB:validators:mustBeInteger'),...\n    'NonPositive', iInvalidInputCase(0,'MATLAB:validators:mustBePositive'),...\n    'NonDouble', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'),...\n    'TooBig', iInvalidInputCase([1,2,25],'MATLAB:validators:mustBeLessThanOrEqual'));\nend\n\nfunction s = iInvalidCodes()\ns = struct(...\n    'NonInteger', iInvalidInputCase(1.1,'MATLAB:validators:mustBeInteger'),...\n    'NonPositive', iInvalidInputCase(0,'MATLAB:validators:mustBePositive'),...\n    'NonNumeric', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'),...\n    'InvalidDims', iInvalidInputCase([1,2,25],'MATLAB:validation:IncompatibleSize'),...\n    'NonReal', iInvalidInputCase(1i,'MATLAB:validators:mustBeReal'));\nend\n\nfunction s = iInvalidDropoutProb()\ns = struct(...\n    'NonPositive', iInvalidInputCase(-1,'MATLAB:validators:mustBeNonnegative'), ...\n    'Larger', iInvalidInputCase(1.5,'MATLAB:validators:mustBeLessThanOrEqual'), ...\n    'InvalidDims', iInvalidInputCase([1,2],'MATLAB:validation:IncompatibleSize'), ...\n    'NonReal', iInvalidInputCase(1i,'MATLAB:validators:mustBeReal'), ...\n    'NonNumeric', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'));\nend\n\nfunction s = iInvalidInputMask()\ns = struct(...\n    'InvalidDims', iInvalidInputCase([true; true; false],\"bert:model:InvalidMaskSize\"), ...\n    'InvalidTypeString', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeNumericOrLogical'), ...\n    'InvalidTypeCell', iInvalidInputCase({{1, 1, 0}},'MATLAB:validators:mustBeNumericOrLogical'));\nend\n\nfunction s = iInvalidInputX()\ns = struct(...\n    'InvalidTypeString', iInvalidInputCase(\"foo\",'MATLAB:validation:UnableToConvert'), ...\n    'InvalidTypeCell', iInvalidInputCase({{1}},'MATLAB:validation:UnableToConvert'), ...\n    'Logical', iInvalidInputCase(true,'MATLAB:validators:mustBeNumeric'), ...\n    'Complex', iInvalidInputCase(1 + 1i,'deep:dlarray:ComplexNotSupported'), ...\n    'IsEmpty', iInvalidInputCase([], 'MATLAB:validators:mustBeNonempty'));\nend\n\nfunction s = iInvalidModelParams()\ns = struct(...\n    'NonStructNumerical', iInvalidInputCase(10,'MATLAB:validators:mustBeA'), ...\n    'NonStructString', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeA'), ...\n    'NonStructCell', iInvalidInputCase({10},'MATLAB:validators:mustBeA'), ...\n    'IsEmpty', iInvalidInputCase([], 'MATLAB:validators:mustBeA'));\nend\n\nfunction s = iInvalidModelName()\ns = struct(...\n    'NonStructNumerical', iInvalidInputCase(10,'MATLAB:validators:mustBeMember'), ...\n    'NonStructString', iInvalidInputCase(\"foo\",'MATLAB:validators:mustBeMember'), ...\n    'NonStructCell', iInvalidInputCase({10},'MATLAB:validators:mustBeMember'));\nend\n\nfunction s = iInvalidInputCase(val,id)\ns = struct(...\n    'Value',val,...\n    'ErrorID',id);\nend scope_id: tmodel scope_type: script",
  "name: checkProbDistrOverChannelDimBatches file_path: test/bert/tlanguageModel.m start_line: 68 end_line: 84 input_parameters: ['test'] code_snippet:         \n        function checkProbDistrOverChannelDimBatches(test)\n            % Verify that the output of bert.languageModel corresponds to\n            % a probability distribution over the implicit C dimension\n            % summing up to 1staking into account a batched input.\n            \n            mdl = bert('Model', 'tiny');\n            x = dlarray(repmat(1:10, [1, 1, 2]));\n            z = bert.languageModel(x, mdl.Parameters);\n            \n            actualProbSum = extractdata(sum(z, 1));\n            expectedProbSum = single(ones(1, size(z, 2), 2));\n            tolerance = iAbsoluteTolerance(single(1e-6));\n            \n            test.verifyThat(actualProbSum, ...\n                iIsEqualTo(expectedProbSum, 'Within', tolerance));\n        end scope_id: tlanguageModel scope_type: script",
  "name: bert file_path: test/bert/tlanguageModel.m start_line: 13 end_line: 18 input_parameters: [\"'Model'\", \"'tiny'\"] code_snippet:         function canBatch(test)\n            mdl = bert('Model','tiny');\n            x = dlarray(repmat(1:10,[1,1,2]));\n            probs = extractdata(bert.languageModel(x,mdl.Parameters));\n            test.verifyEqual(probs(:,:,1),probs(:,:,2),'AbsTol',single(1e-6));\n        end scope_id: tlanguageModel scope_type: script",
  "name: iIsEqualTo file_path: test/bert/tlanguageModel.m start_line: 87 end_line: 90 input_parameters: ['varargin'] code_snippet: \nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend scope_id: tlanguageModel scope_type: script",
  "name: iAbsoluteTolerance file_path: test/bert/tlanguageModel.m start_line: 91 end_line: 94 input_parameters: ['tol'] code_snippet: \nfunction constraint = iAbsoluteTolerance(tol)\nconstraint = matlab.unittest.constraints.AbsoluteTolerance(tol);\nend scope_id: tlanguageModel scope_type: script",
  "name: tlanguageModel file_path: test/bert/tlanguageModel.m start_line: 1 end_line: 94 code_snippet: classdef(SharedTestFixtures={\n        DownloadBERTFixture}) tlanguageModel < matlab.unittest.TestCase\n        \n    % tlanguageModel   Unit test for bert.languageModel\n    \n    % Copyright 2021 The MathWorks, Inc.\n    \n    properties(Constant)\n        FunctionUnderTest = @(x, p)bert.languageModel(x, p);\n    end\n    \n    methods(Test)\n        function canBatch(test)\n            mdl = bert('Model','tiny');\n            x = dlarray(repmat(1:10,[1,1,2]));\n            probs = extractdata(bert.languageModel(x,mdl.Parameters));\n            test.verifyEqual(probs(:,:,1),probs(:,:,2),'AbsTol',single(1e-6));\n        end\n        \n        function verifyOutputDimSizes(test)\n            % Test that the output dimension sizes of bert.languageModel \n            % have expected values.\n            \n            txt = [\"nipson anomemata\"; \"memonan opsin\"];\n            numObs = numel(txt);\n            mdl = bert('Model', \"base\");\n            params = mdl.Parameters;\n            seqs = mdl.Tokenizer.encode(txt);\n            paddingValue = mdl.Tokenizer.PaddingCode;\n            seqsPadded = padsequences(seqs, 2, \"PaddingValue\", paddingValue);\n            vocabularySize = size(params.Weights.masked_LM.output.bias, 1);\n\n            x = dlarray(seqsPadded);\n            \n            z = bert.languageModel(seqsPadded, params);\n            \n            test.verifyNotEmpty(z);\n            test.verifyThat(size(z, 1), iIsEqualTo(vocabularySize), ...\n                            'Wrong first output dim size.');\n            test.verifyThat(size(z, 2), iIsEqualTo(size(x, 2)), ...\n                            'Wrong second output dim size.');            \n            test.verifyThat(size(z, 3), iIsEqualTo(numObs), ...\n                            'Wrong third output dim size.');\n        end\n        \n        function canPredictMaskedTokens(test)\n            % Test that BERT can predict masked tokens.\n            \n            mdl = bert('Model', 'base');\n            str = \"Today it is warm and sunny so I have to drink cold water.\";\n            seq = mdl.Tokenizer.encode(str);\n            x = dlarray(seq{1});\n            \n            % Replace some sequence element with a mask. Specifically the\n            % first word before full-stop.\n            toMask = size(x, 2) - 2;\n            x(toMask) = mdl.Tokenizer.MaskCode;\n            z = bert.languageModel(x, mdl.Parameters);\n            \n            k = 5;\n            maskProbabilities = z(:, toMask);\n            [~, topk] = maxk(extractdata(maskProbabilities), k);\n            toks = arrayfun(@(idx) mdl.Tokenizer.decode(idx), topk);\n            \n            % Regression test against hard coded values.\n            test.verifyThat(toks, iIsEqualTo([\"water\"; \"coffee\"; \"tea\"; \"beer\"; \"milk\"]));        \n        end\n        \n        function checkProbDistrOverChannelDimBatches(test)\n            % Verify that the output of bert.languageModel corresponds to\n            % a probability distribution over the implicit C dimension\n            % summing up to 1staking into account a batched input.\n            \n            mdl = bert('Model', 'tiny');\n            x = dlarray(repmat(1:10, [1, 1, 2]));\n            z = bert.languageModel(x, mdl.Parameters);\n            \n            actualProbSum = extractdata(sum(z, 1));\n            expectedProbSum = single(ones(1, size(z, 2), 2));\n            tolerance = iAbsoluteTolerance(single(1e-6));\n            \n            test.verifyThat(actualProbSum, ...\n                iIsEqualTo(expectedProbSum, 'Within', tolerance));\n        end\n    end\nend\n\nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend\n\nfunction constraint = iAbsoluteTolerance(tol)\nconstraint = matlab.unittest.constraints.AbsoluteTolerance(tol);\nend scope_id: tlanguageModel scope_type: script",
  "name: tload file_path: test/gpt2/tload.m start_line: 1 end_line: 55 code_snippet: classdef(SharedTestFixtures = {DownloadGPT2Fixture}) tload < matlab.unittest.TestCase\n    % tload   Test for gpt2.load\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    properties(Constant)\n        ExpectedNumHeads = 16\n        ExpectedNumLayers = 24\n        ExpectedContext = 1024\n    end\n    \n    properties\n        Parameters\n    end\n    \n    methods(TestClassSetup)\n        function loadParameters(test)\n            % Load the parameters once for all tests\n            test.Parameters = gpt2.load(gpt2.internal.getSupportFilePath('gpt2_355M_params.mat'));\n        end\n    end\n    \n    methods(Test)\n        function verifyLoadStructFields(test)\n            % Verify the expected fieldnames of the loaded struct\n            import matlab.unittest.constraints.IsSameSetAs\n            expected = [\"Hyperparameters\",\"Weights\"];\n            test.verifyThat(fieldnames(test.Parameters), IsSameSetAs(expected));\n        end\n        \n        function verifyHyperparameters(test)\n            % Verify the 355M config\n            hyperParams = test.Parameters.Hyperparameters;\n            test.verifyEqual(hyperParams.NumHeads,test.ExpectedNumHeads,...\n                \"Unexpected value for Hyperparameters.NumHeads\");\n            test.verifyEqual(hyperParams.NumLayers,test.ExpectedNumLayers,...\n                \"Unexpected value for Hyperparameters.NumLayers\");\n            test.verifyEqual(hyperParams.NumContext,test.ExpectedContext,...\n                \"Unexpected value for Hyperparameters.NumContext\");\n        end\n        \n        function verifyWeights(test)\n            % Verify the structure of the Weights field and check some\n            % particular weight has the expected type.\n            \n            % Here there is an implicit check that \"model_\" has been\n            % removed from the weight names and the flat parameters.mat has\n            % been organised into a heirarchy for each gpt2.block\n            w = test.assertWarningFree(@() test.Parameters.Weights.h0.ln_1_g_0);\n            import matlab.unittest.constraints.IsOfClass\n            test.verifyThat(w,IsOfClass('dlarray'));\n            test.verifyThat(extractdata(w),IsOfClass('single'));\n        end\n    end    \nend scope_id: tload scope_type: script",
  "name: tdownload file_path: test/gpt2/tdownload.m start_line: 1 end_line: 29 code_snippet: classdef(SharedTestFixtures = {DownloadGPT2Fixture}) tdownload < matlab.unittest.TestCase\n    % tdownload   Tests for gpt2.download\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    % downloadGPT2Fixture.setup calls gpt2.download so this test is just a\n    % sanity check that the required files are downloaded.\n    \n    properties(Constant)\n        ExpectedFiles = [\"gpt2_355M_params.mat\",\"gpt2_vocab.bpe\",\"gpt2_encoder.txt\"]\n    end\n    \n    methods(Test)\n        function verifyFilesExist(test)\n            aFile = gpt2.internal.getSupportFilePath('gpt2_vocab.bpe');\n            directory = fileparts(aFile);\n            files = dir(directory);\n            filenames = {files.name};\n            % Check that the expected files were downloaded by the fixture.\n            % Do not test these are the only files since we now use the\n            % support files location, and what files exist there depend on\n            % the user.\n            import matlab.unittest.constraints.IsSupersetOf\n            test.verifyThat(filenames,IsSupersetOf(test.ExpectedFiles),...\n                \"Expected files not downloaded for gpt2-355M.\");\n        end\n    end\nend\n         scope_id: tdownload scope_type: script",
  "name: prepareInputs file_path: test/gpt2/tmodel.m start_line: 48 end_line: 53 input_parameters: ['test'] code_snippet:         function [pasts, parameters] = prepareInputs(test)\n            % Convenience method to setup inputs for\n            % transformer.model\n            parameters = test.prepareParameters();\n            pasts = test.preparePasts(parameters.Hyperparameters.NumLayers);\n        end scope_id: tmodel scope_type: script",
  "name: preparePasts file_path: test/gpt2/tmodel.m start_line: 54 end_line: 57 input_parameters: ['~', 'numLayers'] code_snippet:         \n        function pasts = preparePasts(~,numLayers)\n            pasts = cell(numLayers,1);\n        end scope_id: tmodel scope_type: script",
  "name: prepareParameters file_path: test/gpt2/tmodel.m start_line: 58 end_line: 62 input_parameters: ['~'] code_snippet:         \n        function parameters = prepareParameters(~)\n            parametersFile = gpt2.internal.getSupportFilePath(\"gpt2_355M_params.mat\");\n            parameters = gpt2.load(parametersFile);\n        end scope_id: tmodel scope_type: script",
  "name: iGetInputData file_path: test/gpt2/tmodel.m start_line: 65 end_line: 72 code_snippet: \nfunction s = iGetInputData()\ns = struct( ...\n    'SingleToken', dlarray(1), ...\n    'MultiSeqLen', dlarray([1 7 2 9]), ...\n    'MultiSeqLenAndObs', dlarray( permute([1 7 2 9; 7 2 1 9], [3 2 1]) ) ...\n);\nend scope_id: tmodel scope_type: script",
  "name: tmodel file_path: test/gpt2/tmodel.m start_line: 1 end_line: 72 code_snippet: classdef(SharedTestFixtures = {DownloadGPT2Fixture}) tmodel < matlab.unittest.TestCase\n    % tmodel   Tests for gpt2.model\n    \n    % Copyright 2020-2021 The MathWorks, Inc.\n    \n    properties(Constant)\n        model = @gpt2.model\n    end\n    \n    properties(TestParameter)\n        InputData = iGetInputData();\n    end\n    \n    methods(Test)\n        function canUseModel(test, InputData)\n            X = InputData;\n            [pasts, parameters] = test.prepareInputs();\n            test.verifyWarningFree(@() test.model(X, pasts, parameters));\n        end\n        \n        function canAcceptBatches(test)\n            % gpt2.model should be able to accept multiple observations\n            % with the same sequence length\n            \n            % Create inputs\n            [pasts, parameters] = test.prepareInputs();\n            numObs = 4;\n            seqLen = 5;\n            vocabSize = size( parameters.Weights.wte_0, 2 );\n            X = randi(vocabSize, [1 seqLen numObs]);\n            \n            % Get batch results\n            Ybatch = test.model(X, pasts, parameters);\n            \n            % Iterate over batch\n            YperObs = dlarray(zeros([vocabSize seqLen numObs], 'single'));\n            for i = 1:numObs\n                YperObs(:, :, i) = test.model(X(:, :, i), pasts, parameters);\n            end\n            \n            % Verify the results are within a relative tolerance for single\n            % precision data\n            test.verifyEqual(extractdata(Ybatch), extractdata(YperObs), 'RelTol', single(1e-5));\n        end\n    end\n    \n    methods(Access=private)\n        function [pasts, parameters] = prepareInputs(test)\n            % Convenience method to setup inputs for\n            % transformer.model\n            parameters = test.prepareParameters();\n            pasts = test.preparePasts(parameters.Hyperparameters.NumLayers);\n        end\n        \n        function pasts = preparePasts(~,numLayers)\n            pasts = cell(numLayers,1);\n        end\n        \n        function parameters = prepareParameters(~)\n            parametersFile = gpt2.internal.getSupportFilePath(\"gpt2_355M_params.mat\");\n            parameters = gpt2.load(parametersFile);\n        end\n    end\nend\n\nfunction s = iGetInputData()\ns = struct( ...\n    'SingleToken', dlarray(1), ...\n    'MultiSeqLen', dlarray([1 7 2 9]), ...\n    'MultiSeqLenAndObs', dlarray( permute([1 7 2 9; 7 2 1 9], [3 2 1]) ) ...\n);\nend scope_id: tmodel scope_type: script",
  "name: tsampleFromCategorical file_path: test/sampling/tsampleFromCategorical.m line_range: 1-15 superclass: matlab scope_id: tsampleFromCategorical scope_type: script",
  "name: canSampleDeterministic file_path: test/sampling/tsampleFromCategorical.m line_range: 8-21 parameters: test scope_id: tsampleFromCategorical scope_type: class",
  "name: tsampleFromCategorical file_path: test/sampling/tsampleFromCategorical.m start_line: 1 end_line: 30 code_snippet: classdef tsampleFromCategorical < matlab.unittest.TestCase\n    % tsampleFromCategorical   Unit tests for\n    % sampling.sampleFromCategorical\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    methods(Test)\n        function canSampleDeterministic(test)\n            numClasses = 100;\n            class = randi(numClasses);\n            probabilities = zeros([numClasses,1]);\n            probabilities(class) = 1;\n            sample = sampling.sampleFromCategorical(probabilities);\n            test.verifyEqual(sample,class);\n        end\n        \n        function canSampleUniform(test)\n            numClasses = 10;\n            numSamples = 1000;\n            uniformProbabilities = ones([numClasses,1])./numClasses;\n            samples = arrayfun(...\n                @(i) sampling.sampleFromCategorical(uniformProbabilities),1:numSamples);\n            counts = groupcounts(samples');\n            % Verify the counts for each class are about right\n            expMean = numSamples/numClasses;\n            tol = 2;\n            test.verifyEqual(mean(counts),expMean,'AbsTol',tol);\n        end\n    end\nend scope_id: tsampleFromCategorical scope_type: script",
  "name: ttopKLogits file_path: test/sampling/ttopKLogits.m line_range: 1-27 superclass: matlab scope_id: ttopKLogits scope_type: script",
  "name: testForKIsOne file_path: test/sampling/ttopKLogits.m line_range: 7-31 parameters: test scope_id: ttopKLogits scope_type: class",
  "name: ttopKLogits file_path: test/sampling/ttopKLogits.m start_line: 1 end_line: 58 code_snippet: classdef ttopKLogits < matlab.unittest.TestCase\n    % ttopKLogits   Unit tests for sampling.topKLogits\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    methods(Test)\n        function testForKIsOne(test)\n            % When K=1 topKLogits followed by softmax is just the one-hot\n            % for the argmax.\n            numClasses = 100;\n            % Make an arbitrary class the largest logit.\n            class = randi([1,numClasses]);\n            x = rand([numClasses,1]);\n            x(class) = max(x,[],'all')+1;\n            % Call topKLogits with K=1\n            topK = 1;\n            logits = sampling.topKLogits(x,topK);\n            logits = dlarray(logits);\n            % Apply softmax to get a probability distribution over the\n            % classes.\n            actProb = softmax(logits,'DataFormat','C');\n            % The expected output is a delta distribution on the arbitrary\n            % class.\n            expProb = zeros([numClasses,1]);\n            expProb(class) = 1;\n            test.verifyEqual(extractdata(actProb),expProb);\n        end\n        \n        function testForLargeK(test)\n            % When K>1 the topKLogits followed by softmax are only\n            % non-zero at the largest K classes. If those classes have\n            % equal logits the distribution is uniform.\n            numClasses = 100;\n            K = 10;\n            classes = (1:K)+randi([1,numClasses-K]);\n            x = rand([numClasses,1]);\n            x(classes) = max(x,[],'all')+1;\n            logits = sampling.topKLogits(x,K);\n            logits = dlarray(logits);\n            actProb = softmax(logits,'DataFormat','C');\n            expProb = zeros([numClasses,1]);\n            expProb(classes) = 1/K;\n            test.verifyEqual(extractdata(actProb),expProb);\n        end\n        \n        function dlarrayIsSupported(test)\n            % Test we can call topKLogits on a dlarray.\n            numClasses = 10;\n            k = 5;\n            x = dlarray(rand([numClasses,1]));\n            yAct = sampling.topKLogits(x,k);\n            yExp = sampling.topKLogits(extractdata(x),k);\n            test.verifyEqual(extractdata(yAct),yExp);\n        end\n    end\n    \n    \nend scope_id: ttopKLogits scope_type: script",
  "name: tconvolution1d file_path: test/transformer/layer/tconvolution1d.m line_range: 1-8 superclass: matlab scope_id: tconvolution1d scope_type: script",
  "name: dlarray file_path: test/transformer/layer/tconvolution1d.m start_line: 15 end_line: 22 input_parameters: ['Input.x'] code_snippet:         function hasExpectedValue(test,Input)\n            x = dlarray(Input.x);\n            W = dlarray(Input.W);\n            b = dlarray(Input.b);\n            z_exp = fullyconnect(x,W,b,'DataFormat','CBT');\n            z_act = test.convolution1d(x,W,b);\n            test.verifyEqual(extractdata(z_act), extractdata(z_exp), 'AbsTol', 1e-10);\n        end scope_id: tconvolution1d scope_type: script",
  "name: iInput file_path: test/transformer/layer/tconvolution1d.m start_line: 25 end_line: 32 code_snippet: \nfunction s = iInput()\ns = struct(...\n    'OneC',iInputCase(1,2,0),...\n    'TwoC',iInputCase(ones(2,1),rand(3,2),rand(3,1)),...\n    'CT',iInputCase(rand(3,4),rand(5,3),rand(5,1)), ...\n    'CBT',iInputCase(rand(3,4,2),rand(5,3),rand(5,1)));\nend scope_id: tconvolution1d scope_type: script",
  "name: iInputCase file_path: test/transformer/layer/tconvolution1d.m start_line: 33 end_line: 39 input_parameters: ['x', 'w', 'b'] code_snippet: \nfunction s = iInputCase(x,w,b)\ns = struct(...\n    'x',x,...\n    'W',w,...\n    'b',b);\nend scope_id: tconvolution1d scope_type: script",
  "name: tconvolution1d file_path: test/transformer/layer/tconvolution1d.m start_line: 1 end_line: 39 code_snippet: classdef tconvolution1d < matlab.unittest.TestCase\n    % tconvolution1d   Tests for the convolution1d function\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    properties(Constant,Access=private)\n        convolution1d = @transformer.layer.convolution1d\n    end\n    \n    properties(TestParameter)\n        Input = iInput\n    end\n    \n    methods(Test)\n        function hasExpectedValue(test,Input)\n            x = dlarray(Input.x);\n            W = dlarray(Input.W);\n            b = dlarray(Input.b);\n            z_exp = fullyconnect(x,W,b,'DataFormat','CBT');\n            z_act = test.convolution1d(x,W,b);\n            test.verifyEqual(extractdata(z_act), extractdata(z_exp), 'AbsTol', 1e-10);\n        end\n    end\nend\n\nfunction s = iInput()\ns = struct(...\n    'OneC',iInputCase(1,2,0),...\n    'TwoC',iInputCase(ones(2,1),rand(3,2),rand(3,1)),...\n    'CT',iInputCase(rand(3,4),rand(5,3),rand(5,1)), ...\n    'CBT',iInputCase(rand(3,4,2),rand(5,3),rand(5,1)));\nend\n\nfunction s = iInputCase(x,w,b)\ns = struct(...\n    'x',x,...\n    'W',w,...\n    'b',b);\nend scope_id: tconvolution1d scope_type: script",
  "name: tnormalization file_path: test/transformer/layer/tnormalization.m line_range: 1-12 superclass: matlab scope_id: tnormalization scope_type: script",
  "name: Precision file_path: test/transformer/layer/tnormalization.m start_line: 31 end_line: 39 input_parameters: ['Value.input'] code_snippet:         function computesExpectedValue(test,Precision,InputClass,Value)\n            x = Precision(Value.input);\n            x = InputClass(x);\n            y = test.normalization(x,1,0);\n            % Because normalization uses single(1e-5) as epsilon it always\n            % casts to single.\n            if isa(y,'dlarray')\n                y = extractdata(y);\n            end scope_id: tnormalization scope_type: script",
  "name: referenceImplementation file_path: test/transformer/layer/tnormalization.m start_line: 44 end_line: 50 input_parameters: ['x', 'g', 'b', 'normDim'] code_snippet: \nfunction y = referenceImplementation(x,g,b,normDim)\nmu = mean(x,normDim);\nsig = std(x,1,normDim);\nxhat = (x-mu)./(sig+eps(class(sig)));\ny = g.*xhat + b;\nend scope_id: tnormalization scope_type: script",
  "name: tnormalization file_path: test/transformer/layer/tnormalization.m start_line: 1 end_line: 50 code_snippet: classdef tnormalization < matlab.unittest.TestCase\n    % tnormalization   Tests for transformer.layer.normalization\n    % The expected behaviour is to normalize the mean to zero and standard\n    % deviation to 1, then perform an affine linear transform.\n    % Normalization occurs along the 1st dimension.\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    properties(Constant)\n        normalization = @transformer.layer.normalization\n        Eps = single(1e-5)\n    end\n    \n    properties(TestParameter)\n        Precision = struct( ...\n            'double',@double, ...\n            'single',@single )\n        InputClass = struct( ...\n            'identity', @(x) x, ...\n            'dlarray', @(x) dlarray(x) )\n        Value = struct( ...\n            'scalarOne', struct('input',1, 'expected', 0), ...\n            'scalarTwo', struct('input',2, 'expected', 0), ...\n            'scalarThree', struct('input',0, 'expected', 0), ...\n            'vectorSymmetric', struct('input',[-1/sqrt(2);0;1/sqrt(2)],'expected',[-sqrt(3)/sqrt(2);0;sqrt(3)/sqrt(2)]), ...\n            'vectorSymmetricTwo', struct('input',[1;2;3],'expected',[-sqrt(3)/sqrt(2);0;sqrt(3)/sqrt(2)]), ...\n            'matrix', struct('input',reshape(1:9,[3,3]),'expected',referenceImplementation(reshape(1:9,[3,3]),1,0,1)) )\n    end\n    \n    methods(Test)\n        function computesExpectedValue(test,Precision,InputClass,Value)\n            x = Precision(Value.input);\n            x = InputClass(x);\n            y = test.normalization(x,1,0);\n            % Because normalization uses single(1e-5) as epsilon it always\n            % casts to single.\n            if isa(y,'dlarray')\n                y = extractdata(y);\n            end\n            test.verifyEqual(y,single(Value.expected),'AbsTol',2*test.Eps);\n        end\n    end\nend\n\nfunction y = referenceImplementation(x,g,b,normDim)\nmu = mean(x,normDim);\nsig = std(x,1,normDim);\nxhat = (x-mu)./(sig+eps(class(sig)));\ny = g.*xhat + b;\nend scope_id: tnormalization scope_type: script",
  "name: tgelu file_path: test/transformer/layer/tgelu.m line_range: 1-11 superclass: matlab scope_id: tgelu scope_type: script",
  "name: mixedTolerance file_path: test/transformer/layer/tgelu.m start_line: 45 end_line: 49 input_parameters: ['test'] code_snippet:         function tol = mixedTolerance(test)\n            absTol = matlab.unittest.constraints.AbsoluteTolerance(test.Tolerance);\n            relTol = matlab.unittest.constraints.RelativeTolerance(test.Tolerance);\n            tol = absTol | relTol;\n        end scope_id: tgelu scope_type: script",
  "name: normcdf file_path: test/transformer/layer/tgelu.m start_line: 52 end_line: 56 input_parameters: ['x'] code_snippet: \nfunction y = normcdf(x)\n% reimplement normcdf for mean 0 std 1\ny = (1+erf(x/sqrt(2)))/2;\nend scope_id: tgelu scope_type: script",
  "name: tgelu file_path: test/transformer/layer/tgelu.m start_line: 1 end_line: 56 code_snippet: classdef tgelu < matlab.unittest.TestCase\n    % tgelu   Tests for transformer.layer.gelu\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    % Reference: https://arxiv.org/abs/1606.08415\n    \n    properties(Constant)\n        gelu = @transformer.layer.gelu\n        Tolerance = 3e-4\n    end\n    \n    properties(TestParameter)\n        Value = {0,1,-5,10,1.23,100}\n        Cast = struct(...\n            'double', @double, ...\n            'single', @single, ...\n            'dlarray', @dlarray)\n    end\n    \n    methods(Test)\n        function matchesExpectedValue(test,Value)\n            % Check gelu matches the non-approximate version within some\n            % tolerance.\n            actual = test.gelu(Value);\n            expected = Value * normcdf(Value);\n            tol = test.mixedTolerance();\n            constraint = matlab.unittest.constraints.IsEqualTo(expected,'Within',tol);\n            test.verifyThat(actual,constraint);\n        end\n        \n        function supportsMultipleTypes(test,Cast)\n            % Check gelu supports various input types and has the expected\n            % value.\n            num = pi;\n            casted = Cast(num);\n            expected = test.gelu(num);\n            actual = test.gelu(casted);\n            test.verifyEqual(actual,Cast(expected));\n        end\n        \n    end\n    \n    methods(Access=private)\n        function tol = mixedTolerance(test)\n            absTol = matlab.unittest.constraints.AbsoluteTolerance(test.Tolerance);\n            relTol = matlab.unittest.constraints.RelativeTolerance(test.Tolerance);\n            tol = absTol | relTol;\n        end\n    end\nend\n\nfunction y = normcdf(x)\n% reimplement normcdf for mean 0 std 1\ny = (1+erf(x/sqrt(2)))/2;\nend scope_id: tgelu scope_type: script",
  "name: tmultiLayerPerceptron file_path: test/transformer/layer/tmultiLayerPerceptron.m line_range: 1-15 superclass: matlab scope_id: tmultiLayerPerceptron scope_type: script",
  "name: dlarray file_path: test/transformer/layer/tmultiLayerPerceptron.m start_line: 22 end_line: 32 input_parameters: ['Input.x'] code_snippet:         function hasExpectedValue(test,Input)\n            x = dlarray(Input.x);\n            W1 = dlarray(Input.W1);\n            b1 = dlarray(Input.b1);\n            W2 = dlarray(Input.W2);\n            b2 = dlarray(Input.b2);\n            z_exp = test.expectedValue(x,W1,b1,W2,b2);\n            s = test.weightsToStruct(W1,b1,W2,b2);\n            z_act = test.multiLayerPerceptron(x,s);\n            test.verifyEqual(z_act,z_exp);\n        end scope_id: tmultiLayerPerceptron scope_type: script",
  "name: expectedValue file_path: test/transformer/layer/tmultiLayerPerceptron.m start_line: 36 end_line: 43 input_parameters: ['test', 'x', 'W1', 'b1', 'W2', 'b2'] code_snippet:         function z = expectedValue(test,x,W1,b1,W2,b2)\n            % Computes the expected value of multiLayerPerceptron using Wi\n            % and bi as the weights and bias of the i-th fully-connected\n            % layer.\n            z = fullyconnect(x,W1,b1,'DataFormat','CB');\n            z = test.Activation(z);\n            z = fullyconnect(z,W2,b2,'DataFormat','CB');\n        end scope_id: tmultiLayerPerceptron scope_type: script",
  "name: weightsToStruct file_path: test/transformer/layer/tmultiLayerPerceptron.m start_line: 44 end_line: 53 input_parameters: ['~', 'W1', 'b1', 'W2', 'b2'] code_snippet:         \n        function s = weightsToStruct(~,W1,b1,W2,b2)\n            % Create a struct of weights to be consumed by\n            % transformer.layer.multiLayerPerceptron\n            s = struct(...\n                'mlp_c_fc_w_0',W1,...\n                'mlp_c_fc_b_0',b1,...\n                'mlp_c_proj_w_0',W2,...\n                'mlp_c_proj_b_0',b2);\n        end scope_id: tmultiLayerPerceptron scope_type: script",
  "name: iInput file_path: test/transformer/layer/tmultiLayerPerceptron.m start_line: 56 end_line: 62 code_snippet: \nfunction s = iInput()\ns = struct( ...\n    'OneD', iInputCase(1,1,0,1,0),...\n    'TwoD', iInputCase([1;2],eye(2),zeros(2,1),eye(2),zeros(2,1)),...\n    'Random', iInputCase(rand([10,1]),rand(15,10),rand(15,1),rand(10,15),rand(10,1)));\nend scope_id: tmultiLayerPerceptron scope_type: script",
  "name: iInputCase file_path: test/transformer/layer/tmultiLayerPerceptron.m start_line: 63 end_line: 71 input_parameters: ['x', 'w1', 'b1', 'w2', 'b2'] code_snippet: \nfunction s = iInputCase(x,w1,b1,w2,b2)\ns = struct(...\n    'x',x,...\n    'W1',w1,...\n    'b1',b1,...\n    'W2',w2,...\n    'b2',b2);\nend scope_id: tmultiLayerPerceptron scope_type: script",
  "name: tmultiLayerPerceptron file_path: test/transformer/layer/tmultiLayerPerceptron.m start_line: 1 end_line: 71 code_snippet: classdef tmultiLayerPerceptron < matlab.unittest.TestCase\n    % tmultiLayerPerceptron   Tests for the\n    % transformer.layer.multiLayerPerceptron function\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    % The feed-forward network used in a transformer originally has 2\n    % layers with ReLU activation https://arxiv.org/abs/1706.03762\n    % However gpt-2 https://openai.com/blog/better-language-models/\n    % modifies this to use GeLU activation https://arxiv.org/abs/1606.08415\n    \n    properties(Constant,Access=private)\n        Activation = @transformer.layer.gelu\n        multiLayerPerceptron = @transformer.layer.multiLayerPerceptron\n    end\n    \n    properties(TestParameter)\n        Input = iInput\n    end\n    \n    methods(Test)\n        function hasExpectedValue(test,Input)\n            x = dlarray(Input.x);\n            W1 = dlarray(Input.W1);\n            b1 = dlarray(Input.b1);\n            W2 = dlarray(Input.W2);\n            b2 = dlarray(Input.b2);\n            z_exp = test.expectedValue(x,W1,b1,W2,b2);\n            s = test.weightsToStruct(W1,b1,W2,b2);\n            z_act = test.multiLayerPerceptron(x,s);\n            test.verifyEqual(z_act,z_exp);\n        end\n    end\n    \n    methods(Access=private)\n        function z = expectedValue(test,x,W1,b1,W2,b2)\n            % Computes the expected value of multiLayerPerceptron using Wi\n            % and bi as the weights and bias of the i-th fully-connected\n            % layer.\n            z = fullyconnect(x,W1,b1,'DataFormat','CB');\n            z = test.Activation(z);\n            z = fullyconnect(z,W2,b2,'DataFormat','CB');\n        end\n        \n        function s = weightsToStruct(~,W1,b1,W2,b2)\n            % Create a struct of weights to be consumed by\n            % transformer.layer.multiLayerPerceptron\n            s = struct(...\n                'mlp_c_fc_w_0',W1,...\n                'mlp_c_fc_b_0',b1,...\n                'mlp_c_proj_w_0',W2,...\n                'mlp_c_proj_b_0',b2);\n        end\n    end\nend\n\nfunction s = iInput()\ns = struct( ...\n    'OneD', iInputCase(1,1,0,1,0),...\n    'TwoD', iInputCase([1;2],eye(2),zeros(2,1),eye(2),zeros(2,1)),...\n    'Random', iInputCase(rand([10,1]),rand(15,10),rand(15,1),rand(10,15),rand(10,1)));\nend\n\nfunction s = iInputCase(x,w1,b1,w2,b2)\ns = struct(...\n    'x',x,...\n    'W1',w1,...\n    'b1',b1,...\n    'W2',w2,...\n    'b2',b2);\nend scope_id: tmultiLayerPerceptron scope_type: script",
  "name: tmultiheadAttention file_path: test/transformer/layer/tmultiheadAttention.m line_range: 1-26 superclass: matlab scope_id: tmultiheadAttention scope_type: script",
  "name: isExpectedValue file_path: test/transformer/layer/tmultiheadAttention.m start_line: 63 end_line: 81 input_parameters: ['test', 'Dim'] code_snippet:         \n        function isExpectedValue(test,Dim)\n            % Verify that multiheadAttention is the expected value of the\n            % value matrix V over the distribution given by scaled dot\n            % product attention with softmax.\n            % Since isScaledDotProduct ensures the scaledDotProduct method\n            % is used, we can apply softmax to that and compute the\n            % expected value directly.\n            keyDim = 4;\n            numKeys = 5;\n            valDim = Dim;\n            q = dlarray(rand(keyDim,1));\n            K = dlarray(rand(keyDim,numKeys));\n            V = dlarray(rand(valDim,numKeys));\n            attnAct = test.multiheadAttention(q,K,V);\n            attnProbs = softmax(test.scaledDotProduct(q,K),'DataFormat','CT');\n            attnExp = test.expectedValue(attnProbs,V);\n            test.verifyDlarrayEqual(attnAct,attnExp,'AbsTol',test.Tolerance);\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: multipleQueries file_path: test/transformer/layer/tmultiheadAttention.m start_line: 82 end_line: 101 input_parameters: ['test'] code_snippet:         \n        function multipleQueries(test)\n            % Verify that multiheadAttention works on multiple queries.\n            % This is almost independently, except for masking.\n            keyDim = 6;\n            numKeys = 7;\n            numQueries = 3;\n            Q = dlarray(rand(keyDim,numQueries));\n            K = dlarray(rand(keyDim,numKeys));\n            % Use V = Identity to get the probabilities out.\n            V = dlarray(eye(numKeys));\n            attnAct = test.multiheadAttention(Q,K,V);\n            % Only the last query can attend to every value, then each\n            % preceding query can attend to one less value.\n            attnScores = test.scaledDotProduct(Q,K);\n            for i = 0:(numQueries-1)\n                attnExp = softmax(attnScores(1:(end-i),numQueries-i),'DataFormat','CT');\n                attnExp = cat(1,attnExp,dlarray(zeros(i,1)));\n                test.verifyDlarrayEqual(attnAct(:,numQueries-i),attnExp,'AbsTol',test.Tolerance);\n            end scope_id: tmultiheadAttention scope_type: script",
  "name: multipleHeads file_path: test/transformer/layer/tmultiheadAttention.m start_line: 103 end_line: 123 input_parameters: ['test'] code_snippet:         \n        function multipleHeads(test)\n            % Verify multiheadAttention works with multiple attention heads\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            numHeads = 6;\n            Q = dlarray(rand(keyDim,numQueries,numHeads));\n            K = dlarray(rand(keyDim,numKeys,numHeads));\n            V = dlarray(rand(valDim,numKeys,numHeads));\n            attnAct = test.multiheadAttention(Q,K,V);\n            attnExp = dlarray(zeros(valDim,numQueries,numHeads));\n            for h = 1:numHeads\n                attnScores = test.scaledDotProduct(Q(:,:,h),K(:,:,h));\n                for i = 0:(numQueries-1)\n                    maskedAttnScores = attnScores(1:(end-i),numQueries-i);\n                    maskedAttnScores = cat(1,maskedAttnScores,-1e10*dlarray(ones(i,1)));\n                    probs = softmax(maskedAttnScores,'DataFormat','CT');\n                    attnExp(:,numQueries-i,h) = V(:,:,h)*probs;                    \n                end scope_id: tmultiheadAttention scope_type: script",
  "name: defaultIsMasked file_path: test/transformer/layer/tmultiheadAttention.m start_line: 127 end_line: 141 input_parameters: ['test'] code_snippet: \n        function defaultIsMasked(test)\n            % Verify the 'CausalMask' NVP\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            batchSize = 6;\n            Q = dlarray(rand(keyDim,numQueries,batchSize));\n            K = dlarray(rand(keyDim,numKeys,batchSize));\n            V = dlarray(rand(valDim,numKeys,batchSize));\n            default = test.multiheadAttention(Q,K,V);\n            masked = test.multiheadAttention(Q,K,V,'CausalMask',true);\n            test.verifyDlarrayEqual(default,masked)\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: canTurnOffMask file_path: test/transformer/layer/tmultiheadAttention.m start_line: 142 end_line: 158 input_parameters: ['test'] code_snippet:         \n        function canTurnOffMask(test)\n            % Verify the 'CausalMask' NVP can be false - no value should be\n            % \"very small\"\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            batchSize = 6;\n            Q = dlarray(rand(keyDim,numQueries,batchSize));\n            K = dlarray(rand(keyDim,numKeys,batchSize));\n            V = dlarray(rand(valDim,numKeys,batchSize));\n            A = test.multiheadAttention(Q,K,V,'CausalMask',false);\n            import matlab.unittest.constraints.EveryElementOf\n            import matlab.unittest.constraints.IsGreaterThan\n            test.verifyThat(EveryElementOf(extractdata(A)),IsGreaterThan(-1e-9));\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: canDropout file_path: test/transformer/layer/tmultiheadAttention.m start_line: 159 end_line: 177 input_parameters: ['test'] code_snippet:         \n        function canDropout(test)\n            % By setting the value matrix V to the identity, the output of\n            % multiheadAttention with dropout is simply the output without\n            % dropout with dropout applied separately.\n            keyDim = 5;\n            numKeys = 3;\n            q = dlarray(rand(keyDim,1));\n            K = dlarray(rand(keyDim,numKeys));\n            V = dlarray(eye(numKeys));\n            attnNoDropout = test.multiheadAttention(q,K,V,'Dropout',0);\n            % Reset global seed between non-deterministic calls\n            p = 0.5;\n            rng(0);\n            attnWithDropout = test.multiheadAttention(q,K,V,'Dropout',p);\n            rng(0);\n            attnExp = transformer.layer.dropout(attnNoDropout,p);\n            test.verifyEqual(attnWithDropout,attnExp);\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: defaultIsNoDropout file_path: test/transformer/layer/tmultiheadAttention.m start_line: 178 end_line: 193 input_parameters: ['test'] code_snippet:         \n        function defaultIsNoDropout(test)\n            % Verify the default is no dropout applied to\n            % multiheadAttention\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            batchSize = 6;\n            Q = dlarray(rand(keyDim,numQueries,batchSize));\n            K = dlarray(rand(keyDim,numKeys,batchSize));\n            V = dlarray(rand(valDim,numKeys,batchSize));\n            A_default = test.multiheadAttention(Q,K,V);\n            A_exp = test.multiheadAttention(Q,K,V,'Dropout',0);\n            test.verifyEqual(A_default,A_exp);\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: multipleObservations file_path: test/transformer/layer/tmultiheadAttention.m start_line: 194 end_line: 211 input_parameters: ['test'] code_snippet:         \n        function multipleObservations(test)\n            % Verify multiheadAttention works with batches, i.e. across\n            % independent observations in a single batch\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            numHeads = 6;\n            numObs = 2;\n            Q = dlarray(rand(keyDim,numQueries,numHeads,numObs));\n            K = dlarray(rand(keyDim,numKeys,numHeads,numObs));\n            V = dlarray(rand(valDim,numKeys,numHeads,numObs));\n            attnAct = test.multiheadAttention(Q,K,V);\n            attnExp = dlarray(zeros(valDim,numQueries,numHeads,numObs));\n            for n = 1:numObs\n                attnExp(:,:,:,n) = test.multiheadAttention(Q(:,:,:,n),K(:,:,:,n),V(:,:,:,n));\n            end scope_id: tmultiheadAttention scope_type: script",
  "name: verifyDlarrayEqual file_path: test/transformer/layer/tmultiheadAttention.m start_line: 230 end_line: 235 input_parameters: ['test', 'x', 'y', 'varargin'] code_snippet:         \n        function verifyDlarrayEqual(test,x,y,varargin)\n            test.verifyClass(x,'dlarray');\n            test.verifyClass(y,'dlarray');\n            test.verifyEqual(extractdata(x),extractdata(y),varargin{:});\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: scaledDotProduct file_path: test/transformer/layer/tmultiheadAttention.m start_line: 217 end_line: 222 input_parameters: ['~', 'q', 'k'] code_snippet:         function w = scaledDotProduct(~,q,k)\n            % The scaled dot product attention for vector or matrices q\n            % and k.\n            keyDim = size(q,1);\n            w = (k'*q)./sqrt(keyDim);\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: expectedValue file_path: test/transformer/layer/tmultiheadAttention.m start_line: 223 end_line: 229 input_parameters: ['~', 'p', 'V'] code_snippet:         \n        function a = expectedValue(~,p,V)\n            % Expected value in this context is simply the weighted sum of\n            % the value vectors (columns of V) with weights p.\n            % This is the same as a matrix multiply.\n            a = V*p;\n        end scope_id: tmultiheadAttention scope_type: script",
  "name: tmultiheadAttention file_path: test/transformer/layer/tmultiheadAttention.m start_line: 1 end_line: 237 code_snippet: classdef tmultiheadAttention < matlab.unittest.TestCase\n    % tmultiheadAttention   Tests for transformer.layer.multiheadAttention\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    % Multi-head attention (https://arxiv.org/abs/1706.03762) simply\n    % computes a scaled dot product between a query-vector q against a\n    % number of keys k1,k2,... that are batched into a matrix K.\n    %\n    % These projections of the query onto the key vectors are converted to\n    % a probability distribution over the keys via softmax.\n    %\n    % Finally the output is the expected value according to this\n    % distribution of the values v1,v2,... associated to keys k1,k2,...\n    % which are also typically represented as a matrix V.\n    %\n    % A batch of queries, q1,q2,... can be merged into a query matrix Q and\n    % the whole operation becomes a matrix product.\n    %\n    % Masking can be applied to prevent the predictions at time t depending\n    % on inputs at times s > t.\n    \n    properties(Constant,Access=private)\n        multiheadAttention = @transformer.layer.multiheadAttention\n        Tolerance = 1e-6\n    end\n    \n    properties(TestParameter)\n        Dim = struct(...\n            'scalar',1,...\n            'multi',6)\n    end\n    \n    methods(Test)\n        function returnsValueWhenOneKey(test)\n            % Since softmax is applied over the channel dimension (key\n            % dimension), if there is only one key we always expect the\n            % result of the softmax to be 1, and so the multiheadAttention\n            % returns the value input.\n            keyDim = 10;\n            numKeys = 1;\n            q = dlarray(rand(keyDim,1));\n            k = dlarray(rand(keyDim,numKeys));\n            v = dlarray(rand);\n            attnAct = test.multiheadAttention(q,k,v);\n            test.verifyEqual(attnAct,v);\n        end\n        \n        function isScaledDotProduct(test)\n            % Verify that multiheadAttention uses the scaled dot-product.\n            % This can be seen by setting the value matrix to the identity\n            % and verifying the output is the softmax of scaled dot product\n            % attention.\n            keyDim = 5;\n            numKeys = 3;\n            q = dlarray(rand(keyDim,1));\n            K = dlarray(rand(keyDim,numKeys));\n            V = dlarray(eye(numKeys));\n            attnAct = test.multiheadAttention(q,K,V);\n            attnExp = softmax(test.scaledDotProduct(q,K),'DataFormat','CT');\n            test.verifyDlarrayEqual(attnAct,attnExp,'AbsTol',test.Tolerance);\n        end\n        \n        function isExpectedValue(test,Dim)\n            % Verify that multiheadAttention is the expected value of the\n            % value matrix V over the distribution given by scaled dot\n            % product attention with softmax.\n            % Since isScaledDotProduct ensures the scaledDotProduct method\n            % is used, we can apply softmax to that and compute the\n            % expected value directly.\n            keyDim = 4;\n            numKeys = 5;\n            valDim = Dim;\n            q = dlarray(rand(keyDim,1));\n            K = dlarray(rand(keyDim,numKeys));\n            V = dlarray(rand(valDim,numKeys));\n            attnAct = test.multiheadAttention(q,K,V);\n            attnProbs = softmax(test.scaledDotProduct(q,K),'DataFormat','CT');\n            attnExp = test.expectedValue(attnProbs,V);\n            test.verifyDlarrayEqual(attnAct,attnExp,'AbsTol',test.Tolerance);\n        end\n        \n        function multipleQueries(test)\n            % Verify that multiheadAttention works on multiple queries.\n            % This is almost independently, except for masking.\n            keyDim = 6;\n            numKeys = 7;\n            numQueries = 3;\n            Q = dlarray(rand(keyDim,numQueries));\n            K = dlarray(rand(keyDim,numKeys));\n            % Use V = Identity to get the probabilities out.\n            V = dlarray(eye(numKeys));\n            attnAct = test.multiheadAttention(Q,K,V);\n            % Only the last query can attend to every value, then each\n            % preceding query can attend to one less value.\n            attnScores = test.scaledDotProduct(Q,K);\n            for i = 0:(numQueries-1)\n                attnExp = softmax(attnScores(1:(end-i),numQueries-i),'DataFormat','CT');\n                attnExp = cat(1,attnExp,dlarray(zeros(i,1)));\n                test.verifyDlarrayEqual(attnAct(:,numQueries-i),attnExp,'AbsTol',test.Tolerance);\n            end\n        end\n        \n        function multipleHeads(test)\n            % Verify multiheadAttention works with multiple attention heads\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            numHeads = 6;\n            Q = dlarray(rand(keyDim,numQueries,numHeads));\n            K = dlarray(rand(keyDim,numKeys,numHeads));\n            V = dlarray(rand(valDim,numKeys,numHeads));\n            attnAct = test.multiheadAttention(Q,K,V);\n            attnExp = dlarray(zeros(valDim,numQueries,numHeads));\n            for h = 1:numHeads\n                attnScores = test.scaledDotProduct(Q(:,:,h),K(:,:,h));\n                for i = 0:(numQueries-1)\n                    maskedAttnScores = attnScores(1:(end-i),numQueries-i);\n                    maskedAttnScores = cat(1,maskedAttnScores,-1e10*dlarray(ones(i,1)));\n                    probs = softmax(maskedAttnScores,'DataFormat','CT');\n                    attnExp(:,numQueries-i,h) = V(:,:,h)*probs;                    \n                end\n            end\n            test.verifyDlarrayEqual(attnAct,attnExp,'AbsTol',test.Tolerance);\n        end        \n\n        function defaultIsMasked(test)\n            % Verify the 'CausalMask' NVP\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            batchSize = 6;\n            Q = dlarray(rand(keyDim,numQueries,batchSize));\n            K = dlarray(rand(keyDim,numKeys,batchSize));\n            V = dlarray(rand(valDim,numKeys,batchSize));\n            default = test.multiheadAttention(Q,K,V);\n            masked = test.multiheadAttention(Q,K,V,'CausalMask',true);\n            test.verifyDlarrayEqual(default,masked)\n        end\n        \n        function canTurnOffMask(test)\n            % Verify the 'CausalMask' NVP can be false - no value should be\n            % \"very small\"\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            batchSize = 6;\n            Q = dlarray(rand(keyDim,numQueries,batchSize));\n            K = dlarray(rand(keyDim,numKeys,batchSize));\n            V = dlarray(rand(valDim,numKeys,batchSize));\n            A = test.multiheadAttention(Q,K,V,'CausalMask',false);\n            import matlab.unittest.constraints.EveryElementOf\n            import matlab.unittest.constraints.IsGreaterThan\n            test.verifyThat(EveryElementOf(extractdata(A)),IsGreaterThan(-1e-9));\n        end\n        \n        function canDropout(test)\n            % By setting the value matrix V to the identity, the output of\n            % multiheadAttention with dropout is simply the output without\n            % dropout with dropout applied separately.\n            keyDim = 5;\n            numKeys = 3;\n            q = dlarray(rand(keyDim,1));\n            K = dlarray(rand(keyDim,numKeys));\n            V = dlarray(eye(numKeys));\n            attnNoDropout = test.multiheadAttention(q,K,V,'Dropout',0);\n            % Reset global seed between non-deterministic calls\n            p = 0.5;\n            rng(0);\n            attnWithDropout = test.multiheadAttention(q,K,V,'Dropout',p);\n            rng(0);\n            attnExp = transformer.layer.dropout(attnNoDropout,p);\n            test.verifyEqual(attnWithDropout,attnExp);\n        end\n        \n        function defaultIsNoDropout(test)\n            % Verify the default is no dropout applied to\n            % multiheadAttention\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            batchSize = 6;\n            Q = dlarray(rand(keyDim,numQueries,batchSize));\n            K = dlarray(rand(keyDim,numKeys,batchSize));\n            V = dlarray(rand(valDim,numKeys,batchSize));\n            A_default = test.multiheadAttention(Q,K,V);\n            A_exp = test.multiheadAttention(Q,K,V,'Dropout',0);\n            test.verifyEqual(A_default,A_exp);\n        end\n        \n        function multipleObservations(test)\n            % Verify multiheadAttention works with batches, i.e. across\n            % independent observations in a single batch\n            keyDim = 3;\n            valDim = 7;\n            numKeys = 4;\n            numQueries = 5;\n            numHeads = 6;\n            numObs = 2;\n            Q = dlarray(rand(keyDim,numQueries,numHeads,numObs));\n            K = dlarray(rand(keyDim,numKeys,numHeads,numObs));\n            V = dlarray(rand(valDim,numKeys,numHeads,numObs));\n            attnAct = test.multiheadAttention(Q,K,V);\n            attnExp = dlarray(zeros(valDim,numQueries,numHeads,numObs));\n            for n = 1:numObs\n                attnExp(:,:,:,n) = test.multiheadAttention(Q(:,:,:,n),K(:,:,:,n),V(:,:,:,n));\n            end\n            test.verifyDlarrayEqual(attnAct,attnExp,'AbsTol',test.Tolerance);\n        end\n    end\n    \n    methods(Access=private)\n        function w = scaledDotProduct(~,q,k)\n            % The scaled dot product attention for vector or matrices q\n            % and k.\n            keyDim = size(q,1);\n            w = (k'*q)./sqrt(keyDim);\n        end\n        \n        function a = expectedValue(~,p,V)\n            % Expected value in this context is simply the weighted sum of\n            % the value vectors (columns of V) with weights p.\n            % This is the same as a matrix multiply.\n            a = V*p;\n        end\n        \n        function verifyDlarrayEqual(test,x,y,varargin)\n            test.verifyClass(x,'dlarray');\n            test.verifyClass(y,'dlarray');\n            test.verifyEqual(extractdata(x),extractdata(y),varargin{:});\n        end\n    end\nend scope_id: tmultiheadAttention scope_type: script",
  "name: tmaskAttentionWeights file_path: test/transformer/layer/tmaskAttentionWeights.m line_range: 1-14 superclass: matlab scope_id: tmaskAttentionWeights scope_type: script",
  "name: dlarray file_path: test/transformer/layer/tmaskAttentionWeights.m start_line: 21 end_line: 34 input_parameters: ['Input'] code_snippet:         function hasExpectedValue(test,Input)\n            W = dlarray(Input);\n            act_masked = test.maskAttentionWeights(W);\n            [m,n] = size(W,[1,2]);\n            % Reimplement the triu call\n            mask = ((m-n)+(1:n))>=(1:m)';\n            test.verifyEqual(W(mask),act_masked(mask));\n            import matlab.unittest.constraints.EveryElementOf\n            import matlab.unittest.constraints.IsLessThan\n            act_masked = extractdata(act_masked);\n            masked_vals = act_masked(~mask);\n            if ~isempty(masked_vals)\n                test.verifyThat(EveryElementOf(masked_vals),IsLessThan(-1e9));\n            end scope_id: tmaskAttentionWeights scope_type: script",
  "name: iInput file_path: test/transformer/layer/tmaskAttentionWeights.m start_line: 38 end_line: 46 code_snippet: \nfunction s = iInput()\ns = struct(...\n    'One', 1,...\n    'Two', magic(2),...\n    'ThreeTwo', rand(3,2),...\n    'TwoThree', rand(2,3),...\n    'MultiObs', rand(5,4,3));\nend scope_id: tmaskAttentionWeights scope_type: script",
  "name: tmaskAttentionWeights file_path: test/transformer/layer/tmaskAttentionWeights.m start_line: 1 end_line: 46 code_snippet: classdef tmaskAttentionWeights < matlab.unittest.TestCase\n    % tmaskAttentionWeights   Tests for\n    % transformer.layer.maskAttentionWeights\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    % The expected behaviour is a temporal mask - states a(t) should\n    % not attend to future states a(s) for any s>t. Since a softmax is\n    % applied after this step it suffices to mask with a large negative\n    % value.\n    \n    properties(Constant,Access=private)\n        maskAttentionWeights = @transformer.layer.maskAttentionWeights\n    end\n    \n    properties(TestParameter)\n        Input = iInput\n    end\n    \n    methods(Test)\n        function hasExpectedValue(test,Input)\n            W = dlarray(Input);\n            act_masked = test.maskAttentionWeights(W);\n            [m,n] = size(W,[1,2]);\n            % Reimplement the triu call\n            mask = ((m-n)+(1:n))>=(1:m)';\n            test.verifyEqual(W(mask),act_masked(mask));\n            import matlab.unittest.constraints.EveryElementOf\n            import matlab.unittest.constraints.IsLessThan\n            act_masked = extractdata(act_masked);\n            masked_vals = act_masked(~mask);\n            if ~isempty(masked_vals)\n                test.verifyThat(EveryElementOf(masked_vals),IsLessThan(-1e9));\n            end\n        end\n    end\nend\n\nfunction s = iInput()\ns = struct(...\n    'One', 1,...\n    'Two', magic(2),...\n    'ThreeTwo', rand(3,2),...\n    'TwoThree', rand(2,3),...\n    'MultiObs', rand(5,4,3));\nend scope_id: tmaskAttentionWeights scope_type: script",
  "name: tattention file_path: test/transformer/layer/tattention.m line_range: 1-15 superclass: matlab scope_id: tattention scope_type: script",
  "name: checkMultiHeadNoFullyConnected file_path: test/transformer/layer/tattention.m start_line: 47 end_line: 67 input_parameters: ['test', 'NumQueries', 'NumObs'] code_snippet:         \n        function checkMultiHeadNoFullyConnected(test,NumQueries,NumObs)\n            % Verify the multiple head case. Multi head attention is\n            % achieved by simply creating a \"head dimension\" and pushing it\n            % to the back so that it is treated as a \"batch\"\n            % dimension for matrix multiplication.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,NumQueries,NumObs));\n            yAct = test.attention(x,past,weights,hyperParams);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify against the multiheadAttention function.\n            yExp = test.multiheadAttention(Q,K,V);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(yAct,yExp);\n        end scope_id: tattention scope_type: script",
  "name: checkPastPresentCaching file_path: test/transformer/layer/tattention.m start_line: 68 end_line: 104 input_parameters: ['test', 'NumQueries', 'NumObs'] code_snippet:         \n        function checkPastPresentCaching(test,NumQueries,NumObs)\n            % Verify the 2nd input and output of attention - the pasts\n            % passed in are the keys and values for the previous time step.\n            % These are concatenated to the keys and values for the current\n            % time step before the multiheadAttention call, and these\n            % concatenated key and values are passed out as a second\n            % output.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            % Set up a fake past by making an initial call to attention.\n            past = [];\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,NumQueries,NumObs));\n            [~,past] = test.attention(x,past,weights,hyperParams);\n            % Verify the expected value of past - it is the key and values\n            % concatenated on the 4th dimension.\n            [~,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            test.verifyEqual(past,cat(5,K,V));\n            % Now verify second call to attention is possible with the first \n            % past as input - and verify the value of the attention output.\n            [yAct,present] = test.attention(x,past,weights,hyperParams);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify the correct value for present.\n            pastK = past(:,:,:,:,1);\n            pastV = past(:,:,:,:,2);\n            test.verifyEqual(extractdata(present),extractdata(cat(5,cat(2,pastK,K),cat(2,pastV,V))),'AbsTol',1e-5);\n            % To compute the expected value, concatenate the pasts\n            K = cat(2,K,pastK);\n            V = cat(2,V,pastV);\n            yExp = test.multiheadAttention(Q,K,V);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(extractdata(yAct),extractdata(yExp),'AbsTol',1e-5);\n        end scope_id: tattention scope_type: script",
  "name: checkInputOutputFC file_path: test/transformer/layer/tattention.m start_line: 105 end_line: 125 input_parameters: ['test', 'NumQueries', 'NumObs'] code_snippet:         \n        function checkInputOutputFC(test,NumQueries,NumObs)\n            % The tests above ensure multiheadAttention is used by\n            % attention. Now verify the input and output FC operations are\n            % used.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            % Set up a fake past by making an initial call to attention.\n            past = [];\n            params = test.prepareWeightsStructUsingGenerators(hyperParams.NumHeads,latentDim,@randn,@randn);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,NumQueries,NumObs));\n            yAct = test.attention(x,past,params,hyperParams);\n            % Verify this matches the full attention implementation\n            z = fullyconnect(x,params.attn_c_attn_w_0,params.attn_c_attn_b_0,'DataFormat','CBT');\n            [Q,K,V] = iSplitQKV(z,hyperParams.NumHeads,latentDim);\n            z = test.multiheadAttention(Q,K,V);\n            z = iMergeHeads(z);\n            yExp = fullyconnect(z,params.attn_c_proj_w_0,params.attn_c_proj_b_0,'DataFormat','CBT');\n            test.verifyEqual(extractdata(yAct),extractdata(yExp),'AbsTol',1e-5);\n        end scope_id: tattention scope_type: script",
  "name: defaultIsMaksed file_path: test/transformer/layer/tattention.m start_line: 126 end_line: 141 input_parameters: ['test'] code_snippet:         \n        function defaultIsMaksed(test)\n            % Verify the default for the 'CausalMask' NVP is true\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            numQueries = 2;\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            default = test.attention(x,past,weights,hyperParams);\n            masked = test.attention(x,past,weights,hyperParams,'CausalMask',true);\n            test.verifyEqual(extractdata(default),extractdata(masked));            \n        end scope_id: tattention scope_type: script",
  "name: canTurnOffMask file_path: test/transformer/layer/tattention.m start_line: 142 end_line: 163 input_parameters: ['test'] code_snippet:         \n        function canTurnOffMask(test)\n            % Verify the 'CausalMask' NVP can be set to false - the expectation\n            % is this simply sets 'Masked' to false for the\n            % multiheadAttention call.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            numQueries = 2;\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            yAct = test.attention(x,past,weights,hyperParams,'CausalMask',false);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify against the multiheadAttention function with 'Masked'\n            % set to false.\n            yExp = test.multiheadAttention(Q,K,V,'CausalMask',false);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(yAct,yExp);\n        end scope_id: tattention scope_type: script",
  "name: canDropout file_path: test/transformer/layer/tattention.m start_line: 164 end_line: 190 input_parameters: ['test'] code_snippet:         \n        function canDropout(test)\n            % Verify dropout can be applied to attention - the expectation\n            % is that this matches simply setting the Dropout NVP in the\n            % multiheadAttention call\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            numQueries = 2;\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            p = 0.5;\n            % Set global rng between non-deterministic calls.\n            rng(0);\n            yAct = test.attention(x,past,weights,hyperParams,'Dropout',p);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify against the multiheadAttention function with 'Dropout'\n            % set to p\n            % Set global rng between non-deterministic calls.\n            rng(0);\n            yExp = test.multiheadAttention(Q,K,V,'Dropout',p);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(yAct,yExp);\n        end scope_id: tattention scope_type: script",
  "name: defaultIsNoDropout file_path: test/transformer/layer/tattention.m start_line: 191 end_line: 205 input_parameters: ['test'] code_snippet:         \n        function defaultIsNoDropout(test)\n            % Verify the default Dropout is 0.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            numQueries = 2;\n            % Set up a fake past by making an initial call to attention.\n            past = [];\n            params = test.prepareWeightsStructUsingGenerators(hyperParams.NumHeads,latentDim,@randn,@randn);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            yDefault = test.attention(x,past,params,hyperParams);\n            yNoDropout = test.attention(x,past,params,hyperParams,'Dropout',0);\n            test.verifyEqual(yDefault,yNoDropout);\n        end scope_id: tattention scope_type: script",
  "name: iSplitQKV file_path: test/transformer/layer/tattention.m start_line: 249 end_line: 256 input_parameters: ['X', 'nHeads', 'latentDim'] output_variables: ['Q', 'K', 'V'] code_snippet: \nfunction [Q,K,V] = iSplitQKV(X, nHeads, latentDim)\n% Split an input array X into the corresponding Q, K and V arrays.\nsplitSize = latentDim*nHeads;\nQ = iSplitHeads(X(1:splitSize,:,:),splitSize,nHeads);\nK = iSplitHeads(X((splitSize+1):2*splitSize,:,:),splitSize,nHeads);\nV = iSplitHeads(X((2*splitSize+1):3*splitSize,:,:),splitSize,nHeads);\nend scope_id: tattention scope_type: script",
  "name: prepareWeightsStruct file_path: test/transformer/layer/tattention.m start_line: 209 end_line: 217 input_parameters: ['~', 'W1', 'b1', 'W2', 'b2'] code_snippet:         function s = prepareWeightsStruct(~,W1,b1,W2,b2)\n            % Prepare a struct compatible with the weights input of\n            % attention. These are for the fully connected layers.\n            s = struct(...\n                'attn_c_attn_w_0',W1,...\n                'attn_c_attn_b_0',b1,...\n                'attn_c_proj_w_0',W2,...\n                'attn_c_proj_b_0',b2);\n        end scope_id: tattention scope_type: script",
  "name: prepareWeightsStructUsingGenerators file_path: test/transformer/layer/tattention.m start_line: 218 end_line: 240 input_parameters: ['test', 'nHeads', 'latentDim', 'weightGenerator', 'biasGenerator'] code_snippet:         \n        function s = prepareWeightsStructUsingGenerators(test,nHeads,latentDim,weightGenerator,biasGenerator)\n            % Use function handles weightGenerator and biasGenerator to\n            % create weight and bias values for the fc layers in attention.\n            % The expectation is these function handles map a size vector\n            % to an array of that size.\n            \n            % The query, key and value vectors are flattened into 1\n            % dimension for the input.\n            % There are nHeads of each, and each is a latentDim\n            % dimensional vector   \n            inputDim = 3*nHeads*latentDim;\n            W1 = dlarray(weightGenerator([inputDim,inputDim]));\n            b1 = dlarray(biasGenerator([inputDim,1]));\n            \n            % For the output we don't need the 3 - the multiheadAttention\n            % has taken an expectation of the value vectors under some\n            % probability distribution.            \n            outputDim = nHeads*latentDim;\n            W2 = dlarray(weightGenerator([outputDim,outputDim]));\n            b2 = dlarray(biasGenerator([outputDim,1]));\n            s = test.prepareWeightsStruct(W1,b1,W2,b2);\n        end scope_id: tattention scope_type: script",
  "name: prepareWeightsStructWithIdentityFC file_path: test/transformer/layer/tattention.m start_line: 241 end_line: 246 input_parameters: ['test', 'nHeads', 'latentDim'] code_snippet:         \n        function s = prepareWeightsStructWithIdentityFC(test,nHeads,latentDim)\n            % Prepare a struct compatible with the weights input of\n            % attention such that the fc layers are identity operations.\n            s = test.prepareWeightsStructUsingGenerators(nHeads,latentDim,@eye,@zeros);\n        end scope_id: tattention scope_type: script",
  "name: iSplitHeads file_path: test/transformer/layer/tattention.m start_line: 257 end_line: 261 input_parameters: ['X', 'splitSize', 'numHeads'] code_snippet: \nfunction X = iSplitHeads(X, splitSize, numHeads)\nX = reshape(X, splitSize/numHeads, numHeads, [], size(X,3));   % Split states\nX = permute(X,[1 3 2 4]);\nend scope_id: tattention scope_type: script",
  "name: iMergeHeads file_path: test/transformer/layer/tattention.m start_line: 262 end_line: 266 input_parameters: ['X'] code_snippet: \nfunction X = iMergeHeads(X)\nX = permute(X, [1 3 2 4]);\nX = reshape(X, size(X,1)*size(X,2), [], size(X,4));            % Merge states\nend scope_id: tattention scope_type: script",
  "name: tattention file_path: test/transformer/layer/tattention.m start_line: 1 end_line: 266 code_snippet: classdef tattention < matlab.unittest.TestCase\n    % tattention   Tests for transformer.layer.attention\n    \n    % Copyright 2020 The Mathworks, Inc.\n    \n    % The attention function has 3 purposes:\n    % 1. Apply the pre and post attention fully connected layers.\n    % 2. Rearrange data format for applying multi-head attention.\n    % 3. Preserve the key and value matrices, and append to them when using\n    % values from the past. This is done pre-attention.\n    \n    properties(Constant,Access=private)\n        attention = @transformer.layer.attention\n        multiheadAttention = @transformer.layer.multiheadAttention\n    end\n    \n    properties(TestParameter)\n        NumQueries = {1,2}\n        NumObs = {1,3}\n    end\n    \n    methods(Test)\n        function checkSingleHeadNoFullyConnected(test,NumQueries,NumObs)\n            % Verify that multiheadAttention is used.\n            % Do this by setting the fully connected weights to identity\n            % and the bias to 0.\n            % With a single head we do not need split and merge heads.\n            latentDim = 10;\n            hyperParams.NumHeads = 1;\n            past = [];\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0.\n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*3,NumQueries,NumObs));\n            yAct = test.attention(x,past,weights,hyperParams);\n            % Verify against multiheadAttention by splitting the arbitrary\n            % input into query, key and value.\n            [q,k,v] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            yExp = test.multiheadAttention(q,k,v);\n            % Note that for numObs > 1 we need to merge heads. Since we\n            % have a single head here, no merger needs to take place -- we\n            % just need to permute the observation dimension from 4 to 3\n            yExp = permute(yExp, [1 2 4 3]);\n            test.verifyEqual(yAct,yExp);\n        end\n        \n        function checkMultiHeadNoFullyConnected(test,NumQueries,NumObs)\n            % Verify the multiple head case. Multi head attention is\n            % achieved by simply creating a \"head dimension\" and pushing it\n            % to the back so that it is treated as a \"batch\"\n            % dimension for matrix multiplication.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,NumQueries,NumObs));\n            yAct = test.attention(x,past,weights,hyperParams);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify against the multiheadAttention function.\n            yExp = test.multiheadAttention(Q,K,V);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(yAct,yExp);\n        end\n        \n        function checkPastPresentCaching(test,NumQueries,NumObs)\n            % Verify the 2nd input and output of attention - the pasts\n            % passed in are the keys and values for the previous time step.\n            % These are concatenated to the keys and values for the current\n            % time step before the multiheadAttention call, and these\n            % concatenated key and values are passed out as a second\n            % output.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            % Set up a fake past by making an initial call to attention.\n            past = [];\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,NumQueries,NumObs));\n            [~,past] = test.attention(x,past,weights,hyperParams);\n            % Verify the expected value of past - it is the key and values\n            % concatenated on the 4th dimension.\n            [~,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            test.verifyEqual(past,cat(5,K,V));\n            % Now verify second call to attention is possible with the first \n            % past as input - and verify the value of the attention output.\n            [yAct,present] = test.attention(x,past,weights,hyperParams);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify the correct value for present.\n            pastK = past(:,:,:,:,1);\n            pastV = past(:,:,:,:,2);\n            test.verifyEqual(extractdata(present),extractdata(cat(5,cat(2,pastK,K),cat(2,pastV,V))),'AbsTol',1e-5);\n            % To compute the expected value, concatenate the pasts\n            K = cat(2,K,pastK);\n            V = cat(2,V,pastV);\n            yExp = test.multiheadAttention(Q,K,V);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(extractdata(yAct),extractdata(yExp),'AbsTol',1e-5);\n        end\n        \n        function checkInputOutputFC(test,NumQueries,NumObs)\n            % The tests above ensure multiheadAttention is used by\n            % attention. Now verify the input and output FC operations are\n            % used.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            % Set up a fake past by making an initial call to attention.\n            past = [];\n            params = test.prepareWeightsStructUsingGenerators(hyperParams.NumHeads,latentDim,@randn,@randn);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,NumQueries,NumObs));\n            yAct = test.attention(x,past,params,hyperParams);\n            % Verify this matches the full attention implementation\n            z = fullyconnect(x,params.attn_c_attn_w_0,params.attn_c_attn_b_0,'DataFormat','CBT');\n            [Q,K,V] = iSplitQKV(z,hyperParams.NumHeads,latentDim);\n            z = test.multiheadAttention(Q,K,V);\n            z = iMergeHeads(z);\n            yExp = fullyconnect(z,params.attn_c_proj_w_0,params.attn_c_proj_b_0,'DataFormat','CBT');\n            test.verifyEqual(extractdata(yAct),extractdata(yExp),'AbsTol',1e-5);\n        end\n        \n        function defaultIsMaksed(test)\n            % Verify the default for the 'CausalMask' NVP is true\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            numQueries = 2;\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            default = test.attention(x,past,weights,hyperParams);\n            masked = test.attention(x,past,weights,hyperParams,'CausalMask',true);\n            test.verifyEqual(extractdata(default),extractdata(masked));            \n        end\n        \n        function canTurnOffMask(test)\n            % Verify the 'CausalMask' NVP can be set to false - the expectation\n            % is this simply sets 'Masked' to false for the\n            % multiheadAttention call.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            numQueries = 2;\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            yAct = test.attention(x,past,weights,hyperParams,'CausalMask',false);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify against the multiheadAttention function with 'Masked'\n            % set to false.\n            yExp = test.multiheadAttention(Q,K,V,'CausalMask',false);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(yAct,yExp);\n        end\n        \n        function canDropout(test)\n            % Verify dropout can be applied to attention - the expectation\n            % is that this matches simply setting the Dropout NVP in the\n            % multiheadAttention call\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            past = [];\n            numQueries = 2;\n            % Set up the fc weights to be identity matrices and biases to\n            % be 0. \n            weights = test.prepareWeightsStructWithIdentityFC(hyperParams.NumHeads,latentDim);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            p = 0.5;\n            % Set global rng between non-deterministic calls.\n            rng(0);\n            yAct = test.attention(x,past,weights,hyperParams,'Dropout',p);\n            [Q,K,V] = iSplitQKV(x,hyperParams.NumHeads,latentDim);\n            % Verify against the multiheadAttention function with 'Dropout'\n            % set to p\n            % Set global rng between non-deterministic calls.\n            rng(0);\n            yExp = test.multiheadAttention(Q,K,V,'Dropout',p);\n            yExp = iMergeHeads(yExp);\n            test.verifyEqual(yAct,yExp);\n        end\n        \n        function defaultIsNoDropout(test)\n            % Verify the default Dropout is 0.\n            latentDim = 10;\n            hyperParams.NumHeads = 2;\n            numQueries = 2;\n            % Set up a fake past by making an initial call to attention.\n            past = [];\n            params = test.prepareWeightsStructUsingGenerators(hyperParams.NumHeads,latentDim,@randn,@randn);\n            % Call attention on an arbitrary input.\n            x = dlarray(rand(latentDim*hyperParams.NumHeads*3,numQueries));\n            yDefault = test.attention(x,past,params,hyperParams);\n            yNoDropout = test.attention(x,past,params,hyperParams,'Dropout',0);\n            test.verifyEqual(yDefault,yNoDropout);\n        end\n    end\n    \n    methods(Access=private)\n        function s = prepareWeightsStruct(~,W1,b1,W2,b2)\n            % Prepare a struct compatible with the weights input of\n            % attention. These are for the fully connected layers.\n            s = struct(...\n                'attn_c_attn_w_0',W1,...\n                'attn_c_attn_b_0',b1,...\n                'attn_c_proj_w_0',W2,...\n                'attn_c_proj_b_0',b2);\n        end\n        \n        function s = prepareWeightsStructUsingGenerators(test,nHeads,latentDim,weightGenerator,biasGenerator)\n            % Use function handles weightGenerator and biasGenerator to\n            % create weight and bias values for the fc layers in attention.\n            % The expectation is these function handles map a size vector\n            % to an array of that size.\n            \n            % The query, key and value vectors are flattened into 1\n            % dimension for the input.\n            % There are nHeads of each, and each is a latentDim\n            % dimensional vector   \n            inputDim = 3*nHeads*latentDim;\n            W1 = dlarray(weightGenerator([inputDim,inputDim]));\n            b1 = dlarray(biasGenerator([inputDim,1]));\n            \n            % For the output we don't need the 3 - the multiheadAttention\n            % has taken an expectation of the value vectors under some\n            % probability distribution.            \n            outputDim = nHeads*latentDim;\n            W2 = dlarray(weightGenerator([outputDim,outputDim]));\n            b2 = dlarray(biasGenerator([outputDim,1]));\n            s = test.prepareWeightsStruct(W1,b1,W2,b2);\n        end\n        \n        function s = prepareWeightsStructWithIdentityFC(test,nHeads,latentDim)\n            % Prepare a struct compatible with the weights input of\n            % attention such that the fc layers are identity operations.\n            s = test.prepareWeightsStructUsingGenerators(nHeads,latentDim,@eye,@zeros);\n        end\n    end\nend\n\nfunction [Q,K,V] = iSplitQKV(X, nHeads, latentDim)\n% Split an input array X into the corresponding Q, K and V arrays.\nsplitSize = latentDim*nHeads;\nQ = iSplitHeads(X(1:splitSize,:,:),splitSize,nHeads);\nK = iSplitHeads(X((splitSize+1):2*splitSize,:,:),splitSize,nHeads);\nV = iSplitHeads(X((2*splitSize+1):3*splitSize,:,:),splitSize,nHeads);\nend\n\nfunction X = iSplitHeads(X, splitSize, numHeads)\nX = reshape(X, splitSize/numHeads, numHeads, [], size(X,3));   % Split states\nX = permute(X,[1 3 2 4]);\nend\n\nfunction X = iMergeHeads(X)\nX = permute(X, [1 3 2 4]);\nX = reshape(X, size(X,1)*size(X,2), [], size(X,4));            % Merge states\nend scope_id: tattention scope_type: script",
  "name: tdropout file_path: test/transformer/layer/tdropout.m line_range: 1-14 superclass: matlab scope_id: tdropout scope_type: script",
  "name: ones file_path: test/transformer/layer/tdropout.m start_line: 17 end_line: 19 input_parameters: ['Size'] code_snippet:         function resetGlobalSeed(~)\n            rng(0);\n        end scope_id: tdropout scope_type: script",
  "name: dlarray file_path: test/transformer/layer/tdropout.m start_line: 42 end_line: 51 input_parameters: ['ones([5', 4, '3]'] code_snippet:         \n        function supportsDlarrayAndAutodiff(test)\n            x = dlarray(ones([5,4,3]));\n            p = 0.5;\n            f = @(x) transformer.layer.dropout(x,p);\n            test.verifyWarningFree(@() f(x));\n            function [val,df_val] = df(x)\n                val = f(x);\n                df_val = dlgradient(sum(val,'all'),x);\n            end scope_id: tdropout scope_type: script",
  "name: df file_path: test/transformer/layer/tdropout.m start_line: 48 end_line: 51 input_parameters: ['x'] code_snippet:             function [val,df_val] = df(x)\n                val = f(x);\n                df_val = dlgradient(sum(val,'all'),x);\n            end scope_id: tdropout scope_type: script",
  "name: tdropout file_path: test/transformer/layer/tdropout.m start_line: 1 end_line: 57 code_snippet: classdef tdropout < matlab.unittest.TestCase\n    % tdropout   Unit tests for transformer.layer.dropout\n    \n    % Copyright 2020 The MathWorks, Inc.\n    properties(TestParameter)\n        Size = struct(...\n            'Vector',[10000,1],...\n            'Matrix',[100,100],...\n            'Tensor',[5,60,70])\n        Probability = struct(...\n            'Zero',0,...\n            'Half',0.5,...\n            'OneIsh',0.95)\n    end\n    \n    methods(TestMethodSetup)\n        function resetGlobalSeed(~)\n            rng(0);\n        end\n    end\n    \n    methods(Test)\n        function doesDropout(test,Size,Probability)\n            x = ones(Size);\n            y = transformer.layer.dropout(x,Probability);\n            % Expectation is that mean(y) is about 1 since we use inverted\n            % dropout.\n            actMean = mean(y,'all');\n            expMean = 1;\n            tol = 1e-1;\n            test.verifyEqual(actMean,expMean,'AbsTol',tol);\n        end\n        \n        function isRandom(test)\n            % Verify dropout is random on repeated calls\n            x = ones([5,4,3]);\n            p = 0.5;\n            y1 = transformer.layer.dropout(x,p);\n            y2 = transformer.layer.dropout(x,p);\n            test.verifyNotEqual(y1,y2);\n        end\n        \n        function supportsDlarrayAndAutodiff(test)\n            x = dlarray(ones([5,4,3]));\n            p = 0.5;\n            f = @(x) transformer.layer.dropout(x,p);\n            test.verifyWarningFree(@() f(x));\n            function [val,df_val] = df(x)\n                val = f(x);\n                df_val = dlgradient(sum(val,'all'),x);\n            end\n            [f_val,df_val] = dlfeval(@df,x);\n            % Gradient of dropout is itself\n            test.verifyEqual(df_val,f_val);\n        end\n    end\nend scope_id: tdropout scope_type: script",
  "name: tinferTypeID file_path: test/bert/internal/tinferTypeID.m line_range: 1-4 superclass: matlab scope_id: tinferTypeID scope_type: script",
  "name: tinferTypeID file_path: test/bert/internal/tinferTypeID.m start_line: 1 end_line: 71 code_snippet: classdef tinferTypeID < matlab.unittest.TestCase\n    properties(Constant)\n        inferTypeID = @bert.internal.inferTypeID\n    end\n    \n    methods(Test)\n        function oneObsOneSeparator(test)\n            % When there is one separator and one observation the expected\n            % types are only 1.\n            sep = 1;\n            x = [2,3,sep];\n            act = test.inferTypeID(x,sep);\n            test.verifyEqual(act,ones(size(x)));\n        end\n        \n        function oneObsOneSeparatorWithPadding(test)\n            % When there is one separator followed by more data, the\n            % expectation is that this data should be padding, and gets\n            % type 1.\n            sep = 1;\n            x = [2,3,sep,4];\n            act = test.inferTypeID(x,sep);\n            test.verifyEqual(act,ones(size(x)));\n        end\n        \n        function oneObsTwoSeparators(test)\n            % When there are two separators the x should correspond to a\n            % sentence-pair task. Label everything after the first separtor\n            % as type 2.\n            sep = 1;\n            x = [2,3,sep,4,sep];\n            exp = [1,1,1,2,2];\n            act = test.inferTypeID(x,sep);\n            test.verifyEqual(act,exp);            \n        end\n        \n        function oneObsTwoSeparatorsEdgeCase(test)\n            % Verify behaviour when there is no token between the two\n            % separators\n            sep = 1;\n            x = [2,3,sep,sep];\n            exp = [1,1,1,2];\n            act = test.inferTypeID(x,sep);\n            test.verifyEqual(act,exp);\n        end\n        \n        function oneObsTwoSeparatorsPadded(test)\n            % When tokens follow the second separator, mark them as type 1.\n            % In proper workflows these should only ever be padding.\n            sep = 1;\n            x = [2,3,sep,4,sep,5,6];\n            exp = [1,1,1,2,2,1,1];\n            act = test.inferTypeID(x,sep);\n            test.verifyEqual(act,exp);\n        end\n        \n        function batchedCase(test)\n            % Verify behaviour for a batched case\n            sep = 1;\n            pad = 100;\n            x1 = [2,3,sep,pad,pad,pad,pad,pad];\n            type1 = ones(size(x1));\n            x2 = [4,5,6,sep,7,8,sep,pad];\n            type2 = [1,1,1,1,2,2,2,1];\n            x = cat(3,x1,x2);\n            exp = cat(3,type1,type2);\n            act = test.inferTypeID(x,sep);\n            test.verifyEqual(act,exp)\n        end\n    end\nend scope_id: tinferTypeID scope_type: script",
  "name: iJapaneseTokenizerConstructor file_path: test/bert/tokenizer/tBERTTokenizerForJP.m start_line: 59 end_line: 64 input_parameters: ['vocabLocation'] code_snippet: \nfunction japaneseBERTTokenizer = iJapaneseTokenizerConstructor(vocabLocation)\nbtok = bert.tokenizer.internal.TokenizedDocumentTokenizer(\"Language\",\"ja\",\"TokenizeMethod\",\"mecab\",IgnoreCase=false);\nftok = bert.tokenizer.internal.FullTokenizer(vocabLocation,BasicTokenizer=btok);\njapaneseBERTTokenizer = bert.tokenizer.BERTTokenizer(vocabLocation,FullTokenizer=ftok);\nend     scope_id: tBERTTokenizerForJP scope_type: script",
  "name: tBERTTokenizerForJP file_path: test/bert/tokenizer/tBERTTokenizerForJP.m start_line: 1 end_line: 64 code_snippet: classdef(SharedTestFixtures = {\n        DownloadJPBERTFixture}) tBERTTokenizerForJP < matlab.unittest.TestCase\n    % tBERTTokenizerForJP   Unit tests for the BERTTokenizer using Japanese\n    % BERT models.\n    \n    % Copyright 2023 The MathWorks, Inc.\n    \n    properties(TestParameter)\n        VocabFiles = iVocabFiles()\n    end\n    \n    properties(Constant)\n        Constructor = @iJapaneseTokenizerConstructor\n    end\n    \n    methods(Test)\n                \n        function hasExpectedProperties(test, VocabFiles)\n            tok = test.Constructor(VocabFiles);\n            test.verifyEqual(tok.PaddingToken, \"[PAD]\");\n            test.verifyEqual(tok.StartToken, \"[CLS]\");\n            test.verifyEqual(tok.SeparatorToken, \"[SEP]\");\n            test.verifyEqual(tok.PaddingCode, 1);\n            test.verifyEqual(tok.SeparatorCode, 4);\n            test.verifyEqual(tok.StartCode, 3);\n        end\n        \n        function matchesExpectedEncoding(test, VocabFiles)\n            tok = test.Constructor(VocabFiles);\n            text = \"月夜の想い、謎めく愛。君の謎。\";\n            expectedEncoding = [3 38 29340 6 12385 7 5939 2088 28504 768 ...\n                9 2607 6 5939 9 4];\n            y = tok.encode(text);\n            test.verifyClass(y,'cell');\n            y1 = y{1};\n            test.verifyEqual(y1(1),tok.StartCode);\n            test.verifyEqual(y1(end),tok.SeparatorCode);\n            test.verifyEqual(y1,expectedEncoding);\n        end    \n    end    \nend\n\nfunction modelNames = iModelNames\n% struct friendly model names\nmodelNames = [\"japanese_base\", \"japanese_base_wwm\"];\nend\n\nfunction vocabFiles = iVocabFiles\nmodelDir = [\"bert-base-japanese\", \"bert-base-japanese-whole-word-masking\"];\nmodelNames = iModelNames;\nvocabFiles = struct();\nfor i = 1:numel(modelNames)\n    versionName = modelDir(i);\n    vocabDir = fullfile(\"data\", \"networks\", \"ja_bert\", versionName, \"vocab.txt\");\n    model = modelNames(i);\n    vocabFiles.(replace(model, \"-\", \"_\")) = fullfile(matlab.internal.examples.utils.getSupportFileDir(),\"nnet\",vocabDir);\nend\nend\n\nfunction japaneseBERTTokenizer = iJapaneseTokenizerConstructor(vocabLocation)\nbtok = bert.tokenizer.internal.TokenizedDocumentTokenizer(\"Language\",\"ja\",\"TokenizeMethod\",\"mecab\",IgnoreCase=false);\nftok = bert.tokenizer.internal.FullTokenizer(vocabLocation,BasicTokenizer=btok);\njapaneseBERTTokenizer = bert.tokenizer.BERTTokenizer(vocabLocation,FullTokenizer=ftok);\nend     scope_id: tBERTTokenizerForJP scope_type: script",
  "name: canEncodeMultipleSentences file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 58 end_line: 74 input_parameters: ['test', 'VocabFiles'] code_snippet:         \n        function canEncodeMultipleSentences(test, VocabFiles)\n            % Test encoding multiple observations;\n            tok = test.Constructor(VocabFiles);\n            x = [\n                \"foo, bar. Baz!\"\n                \"hello world\"\n                \"Bidirectional Encoder Representations from Transformers\"\n                \"Word piece tokenization\"];\n            % give x an interesting shape\n            x = reshape(x,2,1,2);\n            act_y = tok.encode(x);\n            % Expect this to simply match encoding each observation in\n            % turn.\n            exp_y = reshape(arrayfun(@(x) tok.encode(x), x), size(x));\n            test.verifyEqual(act_y,exp_y);\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: canEncodeSentencePair file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 75 end_line: 91 input_parameters: ['test', 'VocabFiles'] code_snippet:         \n        function canEncodeSentencePair(test, VocabFiles)\n            % Test encoding sentence pairs\n            tok = test.Constructor(VocabFiles);\n            x1 = \"foo, bar. Baz!\";\n            x2 = \"hello world\";\n            act_y = tok.encode(x1,x2);\n            % Expected value is to encode x1 and x2 and join on the\n            % separator - however we encoding appends and prepends the\n            % closing separator and start token, so remember that here.\n            y1 = tok.encode(x1);\n            y1 = y1{1};\n            y2 = tok.encode(x2);\n            y2 = y2{1};\n            exp_y = {[y1,y2(2:end)]};\n            test.verifyEqual(act_y,exp_y);\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: canEncodeMultipleSentencePairs file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 92 end_line: 113 input_parameters: ['test', 'VocabFiles'] code_snippet:         \n        function canEncodeMultipleSentencePairs(test, VocabFiles)\n            % Test encoding multiple sentence pairs\n            tok = test.Constructor(VocabFiles);\n            x1 = [\n                \"foo, bar. Baz!\"\n                \"hello world\"\n                \"Bidirectional Encoder Representations from Transformers\"\n                \"Word piece tokenization\"];\n            % Give x1 an interesting shape\n            x1 = reshape(x1,2,1,2);\n            x2 = [\n                \"sentence pair\"\n                \"multiple sentence pairs\"\n                \"BERT model\"\n                \"test\"];\n            act_y = tok.encode(x1,x2);\n            % The expectation is that each pair is encoded in turn\n            exp_y = cell(4,1);\n            for i = 1:4\n                exp_y(i) = tok.encode(x1(i),x2(i));\n            end scope_id: tBERTTokenizer scope_type: script",
  "name: errorsForDifferentNumberOfSentencePairObservations file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 118 end_line: 126 input_parameters: ['test', 'VocabFiles'] code_snippet:         \n        function errorsForDifferentNumberOfSentencePairObservations(test, VocabFiles)\n            % Verify an error is thrown when different number of\n            % observations are used for sentence pairs\n            tok = test.Constructor(VocabFiles);\n            x1 = \"foo\";\n            x2 = [\"bar\",\"baz\"];\n            test.verifyError(@() tok.encode(x1,x2), 'bert:tokenizer:SentencePairNumelMismatch');\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: canDecode file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 127 end_line: 137 input_parameters: ['test'] code_snippet:         \n        function canDecode(test)\n            % Verify the decode method is \"nearly\" an inverse of encode -\n            % it leaves the special tokens in and assumes space separation.\n            x = \"foo, bar. Baz!\";\n            tok = test.Constructor();\n            y = tok.encode(x);\n            act_decode = tok.decode(y{1});\n            exp_decode = \"[CLS] foo , bar . ba ##z ! [SEP]\";\n            test.verifyEqual(act_decode,exp_decode);\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: canDecodeMultipleObservations file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 138 end_line: 149 input_parameters: ['test', 'VocabFiles'] code_snippet:         \n        function canDecodeMultipleObservations(test, VocabFiles)\n            % verify multiple observations can be decoded\n            x = [\n                \"foo, bar. Baz!\"\n                \"hello world\"];\n            tok = test.Constructor(VocabFiles);\n            y = tok.encode(x);\n            act_decode = tok.decode(y);\n            exp_decode = [tok.decode(y{1});tok.decode(y{2})];\n            test.verifyEqual(act_decode,exp_decode);\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: canDecodePaddedBatch file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 150 end_line: 158 input_parameters: ['test', 'VocabFiles'] code_snippet:         \n        function canDecodePaddedBatch(test, VocabFiles)\n            % decode also supports padded inputs in CTB format\n            x = reshape(1:6,1,3,2);\n            tok = test.Constructor(VocabFiles);\n            act_decode = tok.decode(x);\n            exp_decode = [tok.decode(x(:,:,1));tok.decode(x(:,:,2))];\n            test.verifyEqual(act_decode,exp_decode);\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: canIgnoreCase file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 159 end_line: 175 input_parameters: ['test', 'VocabFiles'] code_snippet:         \n        function canIgnoreCase(test, VocabFiles)\n            % Check that BERTTokenizer is case insensitive if IgnoreCase is\n            % TRUE.\n            \n            % Set 'IgnoreCase' equal to TRUE.\n            tok = test.Constructor(VocabFiles, 'IgnoreCase', true);\n            xLower = \"foo, bar, baz!\";\n            xUpper = upper(xLower);\n            \n            yLower = tok.encode(xLower);\n            yUpper = tok.encode(xUpper);\n            \n            % Verify that both encodings are equal for both lower and upper\n            % case inputs.\n            test.verifyThat(yLower, iIsEqualTo(yUpper));\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: checkCaseSensitivity file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 176 end_line: 193 input_parameters: ['test', 'VocabFiles'] code_snippet:                  \n        function checkCaseSensitivity(test, VocabFiles)\n            % Check that BERTTokenizer is case sensitive if IgnoreCase is\n            % FALSE.\n            \n            % Set 'IgnoreCase' equal to FALSE.\n            tok = test.Constructor(VocabFiles, 'IgnoreCase', false);\n            xLower = \"foo, bar, baz!\";\n            xUpper = upper(xLower);\n            \n            yLower = tok.encode(xLower);\n            yUpper = tok.encode(xUpper);\n            \n            % Verify that encodings are not equal for lower and upper\n            % case inputs.\n            isNotEqualTo_yUpper = ~iIsEqualTo(yUpper);\n            test.verifyThat(yLower, isNotEqualTo_yUpper);\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: reshape file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 150 end_line: 158 input_parameters: ['1:6', 1, 3, 2] code_snippet:         \n        function canDecodePaddedBatch(test, VocabFiles)\n            % decode also supports padded inputs in CTB format\n            x = reshape(1:6,1,3,2);\n            tok = test.Constructor(VocabFiles);\n            act_decode = tok.decode(x);\n            exp_decode = [tok.decode(x(:,:,1));tok.decode(x(:,:,2))];\n            test.verifyEqual(act_decode,exp_decode);\n        end scope_id: tBERTTokenizer scope_type: script",
  "name: iIsEqualTo file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 196 end_line: 199 input_parameters: ['varargin'] code_snippet: \nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend scope_id: tBERTTokenizer scope_type: script",
  "name: iVocabFiles file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 200 end_line: 206 code_snippet: \nfunction vocabFiles = iVocabFiles()\nversions = [\"base\",\"tiny\",\"mini\",\"small\",\"medium\",\"multilingual-cased\"];\nvocabFiles = cell(size(versions));\nfor i = 1:numel(versions)\n    vocabFiles{i} = bert.internal.getSupportFilePath(versions(i),\"vocab.txt\");\nend scope_id: tBERTTokenizer scope_type: script",
  "name: tBERTTokenizer file_path: test/bert/tokenizer/tBERTTokenizer.m start_line: 1 end_line: 207 code_snippet: classdef(SharedTestFixtures = {\n        DownloadBERTFixture}) tBERTTokenizer < matlab.unittest.TestCase\n    % tBERTTokenizer   Unit tests for the BERTTokenizer.\n    \n    % Copyright 2020-2021 The MathWorks, Inc.\n    \n    properties(TestParameter)\n        VocabFiles = iVocabFiles()\n    end\n    \n    properties(Constant)\n        Constructor = @bert.tokenizer.BERTTokenizer\n    end\n    \n    methods(Test)\n        function canConstruct(test)\n            % Note - the tokenizer requires a downloaded model to be\n            % constructed. In particular it needs the vocab.txt\n            test.verifyWarningFree(@()test.Constructor());\n        end\n        \n        function canConstructWithNonDefaultModel(test)\n            % Verify the optional model argument\n            tok = test.Constructor();\n            tok2 = test.Constructor(bert.internal.getSupportFilePath(\"base\",\"vocab.txt\"));\n            % The argument is only used for locating the vocab.txt, and\n            % these are the same for every model so far, so the tokenizers\n            % are equal.\n            test.verifyEqual(tok,tok2);\n        end\n        \n        function hasExpectedProperties(test, VocabFiles)\n            % Verify the public gettable properties have the expected\n            % values\n            tok = test.Constructor(VocabFiles);\n            test.verifyEqual(tok.PaddingToken, \"[PAD]\");\n            test.verifyEqual(tok.StartToken, \"[CLS]\");\n            test.verifyEqual(tok.SeparatorToken, \"[SEP]\");\n            % Technically the following can be derived from FullTokenizer -\n            % but that would be duplicating what the source code does.\n            test.verifyEqual(tok.PaddingCode, 1);\n            test.verifyEqual(tok.SeparatorCode, 103);\n            test.verifyEqual(tok.StartCode, 102);\n        end\n        \n        function canEncodeOneSentence(test)\n            % Test encoding of a single observation\n            tok = test.Constructor();\n            x = \"foo, bar. Baz!\";\n            y = tok.encode(x);\n            test.verifyClass(y,'cell');\n            y1 = y{1};\n            test.verifyEqual(y1(1),tok.StartCode);\n            test.verifyEqual(y1(end),tok.SeparatorCode);\n            % Regression test against hard-coded values\n            test.verifyEqual(y1(2:end-1),[29380, 1011, 3348, 1013, 8671, 2481, 1000]);\n        end\n        \n        function canEncodeMultipleSentences(test, VocabFiles)\n            % Test encoding multiple observations;\n            tok = test.Constructor(VocabFiles);\n            x = [\n                \"foo, bar. Baz!\"\n                \"hello world\"\n                \"Bidirectional Encoder Representations from Transformers\"\n                \"Word piece tokenization\"];\n            % give x an interesting shape\n            x = reshape(x,2,1,2);\n            act_y = tok.encode(x);\n            % Expect this to simply match encoding each observation in\n            % turn.\n            exp_y = reshape(arrayfun(@(x) tok.encode(x), x), size(x));\n            test.verifyEqual(act_y,exp_y);\n        end\n        \n        function canEncodeSentencePair(test, VocabFiles)\n            % Test encoding sentence pairs\n            tok = test.Constructor(VocabFiles);\n            x1 = \"foo, bar. Baz!\";\n            x2 = \"hello world\";\n            act_y = tok.encode(x1,x2);\n            % Expected value is to encode x1 and x2 and join on the\n            % separator - however we encoding appends and prepends the\n            % closing separator and start token, so remember that here.\n            y1 = tok.encode(x1);\n            y1 = y1{1};\n            y2 = tok.encode(x2);\n            y2 = y2{1};\n            exp_y = {[y1,y2(2:end)]};\n            test.verifyEqual(act_y,exp_y);\n        end\n        \n        function canEncodeMultipleSentencePairs(test, VocabFiles)\n            % Test encoding multiple sentence pairs\n            tok = test.Constructor(VocabFiles);\n            x1 = [\n                \"foo, bar. Baz!\"\n                \"hello world\"\n                \"Bidirectional Encoder Representations from Transformers\"\n                \"Word piece tokenization\"];\n            % Give x1 an interesting shape\n            x1 = reshape(x1,2,1,2);\n            x2 = [\n                \"sentence pair\"\n                \"multiple sentence pairs\"\n                \"BERT model\"\n                \"test\"];\n            act_y = tok.encode(x1,x2);\n            % The expectation is that each pair is encoded in turn\n            exp_y = cell(4,1);\n            for i = 1:4\n                exp_y(i) = tok.encode(x1(i),x2(i));\n            end\n            % However the shape of the output matches the first input\n            exp_y = reshape(exp_y,size(x1));\n            test.verifyEqual(act_y,exp_y);\n        end\n        \n        function errorsForDifferentNumberOfSentencePairObservations(test, VocabFiles)\n            % Verify an error is thrown when different number of\n            % observations are used for sentence pairs\n            tok = test.Constructor(VocabFiles);\n            x1 = \"foo\";\n            x2 = [\"bar\",\"baz\"];\n            test.verifyError(@() tok.encode(x1,x2), 'bert:tokenizer:SentencePairNumelMismatch');\n        end\n        \n        function canDecode(test)\n            % Verify the decode method is \"nearly\" an inverse of encode -\n            % it leaves the special tokens in and assumes space separation.\n            x = \"foo, bar. Baz!\";\n            tok = test.Constructor();\n            y = tok.encode(x);\n            act_decode = tok.decode(y{1});\n            exp_decode = \"[CLS] foo , bar . ba ##z ! [SEP]\";\n            test.verifyEqual(act_decode,exp_decode);\n        end\n        \n        function canDecodeMultipleObservations(test, VocabFiles)\n            % verify multiple observations can be decoded\n            x = [\n                \"foo, bar. Baz!\"\n                \"hello world\"];\n            tok = test.Constructor(VocabFiles);\n            y = tok.encode(x);\n            act_decode = tok.decode(y);\n            exp_decode = [tok.decode(y{1});tok.decode(y{2})];\n            test.verifyEqual(act_decode,exp_decode);\n        end\n        \n        function canDecodePaddedBatch(test, VocabFiles)\n            % decode also supports padded inputs in CTB format\n            x = reshape(1:6,1,3,2);\n            tok = test.Constructor(VocabFiles);\n            act_decode = tok.decode(x);\n            exp_decode = [tok.decode(x(:,:,1));tok.decode(x(:,:,2))];\n            test.verifyEqual(act_decode,exp_decode);\n        end\n        \n        function canIgnoreCase(test, VocabFiles)\n            % Check that BERTTokenizer is case insensitive if IgnoreCase is\n            % TRUE.\n            \n            % Set 'IgnoreCase' equal to TRUE.\n            tok = test.Constructor(VocabFiles, 'IgnoreCase', true);\n            xLower = \"foo, bar, baz!\";\n            xUpper = upper(xLower);\n            \n            yLower = tok.encode(xLower);\n            yUpper = tok.encode(xUpper);\n            \n            % Verify that both encodings are equal for both lower and upper\n            % case inputs.\n            test.verifyThat(yLower, iIsEqualTo(yUpper));\n        end\n                 \n        function checkCaseSensitivity(test, VocabFiles)\n            % Check that BERTTokenizer is case sensitive if IgnoreCase is\n            % FALSE.\n            \n            % Set 'IgnoreCase' equal to FALSE.\n            tok = test.Constructor(VocabFiles, 'IgnoreCase', false);\n            xLower = \"foo, bar, baz!\";\n            xUpper = upper(xLower);\n            \n            yLower = tok.encode(xLower);\n            yUpper = tok.encode(xUpper);\n            \n            % Verify that encodings are not equal for lower and upper\n            % case inputs.\n            isNotEqualTo_yUpper = ~iIsEqualTo(yUpper);\n            test.verifyThat(yLower, isNotEqualTo_yUpper);\n        end\n    end    \nend\n\nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend\n\nfunction vocabFiles = iVocabFiles()\nversions = [\"base\",\"tiny\",\"mini\",\"small\",\"medium\",\"multilingual-cased\"];\nvocabFiles = cell(size(versions));\nfor i = 1:numel(versions)\n    vocabFiles{i} = bert.internal.getSupportFilePath(versions(i),\"vocab.txt\");\nend\nend     scope_id: tBERTTokenizer scope_type: script",
  "name: tWhitespaceTokenizer file_path: test/bert/tokenizer/internal/tWhitespaceTokenizer.m line_range: 1-13 superclass: matlab scope_id: tWhitespaceTokenizer scope_type: script",
  "name: canTokenize file_path: test/bert/tokenizer/internal/tWhitespaceTokenizer.m line_range: 7-16 parameters: test scope_id: tWhitespaceTokenizer scope_type: class",
  "name: tWhitespaceTokenizer file_path: test/bert/tokenizer/internal/tWhitespaceTokenizer.m start_line: 1 end_line: 16 code_snippet: classdef tWhitespaceTokenizer < matlab.unittest.TestCase\n    % tWhitespaceTokenizer   Unit tests for WhitespaceTokenizer.\n    \n    % Copyright 2021 The MathWorks, Inc.\n    \n    methods(Test)\n        function canTokenize(test)\n            tok = bert.tokenizer.internal.WhitespaceTokenizer();\n            str = \"foo bar baz \";\n            exp_out = [\"foo\",\"bar\",\"baz\"];\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);            \n        end       \n    end\nend\n scope_id: tWhitespaceTokenizer scope_type: script",
  "name: canTokenize file_path: test/bert/tokenizer/internal/tWordPieceTokenizer.m start_line: 59 end_line: 69 input_parameters: ['test'] code_snippet:         \n        function canTokenize(test)\n            enc = wordEncoding([\"foo\",\"bar\",\"##foo\"]);\n            tok = bert.tokenizer.internal.WordPieceTokenizer(enc);\n            str = \"foo bar foobar barba bafoobar barfoo\";\n            wsTok = bert.tokenizer.internal.WhitespaceTokenizer;\n            ustr = textanalytics.unicode.UTF32(wsTok.tokenize(str));\n            act_out = tok.tokenize(ustr);\n            exp_out = [\"foo\",\"bar\",tok.Unk,tok.Unk,tok.Unk,\"bar\",\"##foo\"];\n            test.verifyEqual(act_out,exp_out);\n        end scope_id: tWordPieceTokenizer scope_type: script",
  "name: hasExpectedVocabSize file_path: test/bert/tokenizer/internal/tWordPieceTokenizer.m start_line: 70 end_line: 75 input_parameters: ['test', 'ModelAndExpectedVocabSize'] code_snippet:         \n        function hasExpectedVocabSize(test,ModelAndExpectedVocabSize)\n            vocab = bert.internal.getSupportFilePath(ModelAndExpectedVocabSize.Model,\"vocab.txt\");\n            tok = bert.tokenizer.internal.WordPieceTokenizer(vocab);\n            test.verifyEqual(tok.Vocab.NumWords,ModelAndExpectedVocabSize.VocabSize);\n        end scope_id: tWordPieceTokenizer scope_type: script",
  "name: wordEncoding file_path: test/bert/tokenizer/internal/tWordPieceTokenizer.m start_line: 14 end_line: 19 input_parameters: ['\"foo\"'] code_snippet:         function canConstruct(test)\n            enc = wordEncoding(\"foo\");\n            tok = bert.tokenizer.internal.WordPieceTokenizer(enc);\n            test.verifyClass(tok,'bert.tokenizer.internal.WordPieceTokenizer');\n            test.verifyInstanceOf(tok,'bert.tokenizer.internal.Tokenizer');\n        end scope_id: tWordPieceTokenizer scope_type: script",
  "name: tWordPieceTokenizer file_path: test/bert/tokenizer/internal/tWordPieceTokenizer.m start_line: 1 end_line: 77 code_snippet: classdef(SharedTestFixtures = {\n        DownloadBERTFixture}) tWordPieceTokenizer < matlab.unittest.TestCase\n    % tWordPieceTokenizer   Unit tests for WordPieceTokenizer\n    \n    % Copyright 2021 The MathWorks, Inc.\n    \n    properties(TestParameter)\n        ModelAndExpectedVocabSize = struct(...\n            'uncased',struct('Model','base','VocabSize',30522),...\n            'multilingualCased',struct('Model','multilingual-cased','VocabSize',119547))\n    end\n    \n    methods(Test)\n        function canConstruct(test)\n            enc = wordEncoding(\"foo\");\n            tok = bert.tokenizer.internal.WordPieceTokenizer(enc);\n            test.verifyClass(tok,'bert.tokenizer.internal.WordPieceTokenizer');\n            test.verifyInstanceOf(tok,'bert.tokenizer.internal.Tokenizer');\n        end\n        \n        function canConstructWithFile(test)\n            words = [\"foo\",\"bar\"];\n            enc = wordEncoding(words);\n            fixture = matlab.unittest.fixtures.TemporaryFolderFixture();\n            test.applyFixture(fixture);\n            folder = fixture.Folder;\n            testVocab = fullfile(folder,\"testvocab.txt\");\n            fid = fopen(testVocab,'w','n','utf-8');\n            fprintf(fid,join(words,newline));\n            fclose(fid);\n            tokFromEncoding = bert.tokenizer.internal.WordPieceTokenizer(enc);\n            tokFromFile = bert.tokenizer.internal.WordPieceTokenizer(testVocab);\n            test.verifyEqual(tokFromEncoding.Vocab,tokFromFile.Vocab);\n        end\n        \n        function canSetUnknownToken(test)\n            enc = wordEncoding(\"foo\");\n            unk = \"bar\";\n            tok = bert.tokenizer.internal.WordPieceTokenizer(enc,'UnknownToken',unk);\n            test.verifyEqual(tok.Unk,unk)\n            str = \"blah\";\n            ustr = textanalytics.unicode.UTF32(str);\n            act_out = tok.tokenize(ustr);\n            exp_out = unk;\n            test.verifyEqual(act_out,exp_out);\n        end\n        \n        function canSetMaxTokenLength(test)\n            enc = wordEncoding(\"foo\");\n            maxLen = 2;\n            tok = bert.tokenizer.internal.WordPieceTokenizer(enc,'MaxTokenLength',maxLen);\n            test.verifyEqual(tok.MaxChar,maxLen);\n            str = \"foo\";\n            ustr = textanalytics.unicode.UTF32(str);\n            act_out = tok.tokenize(ustr);\n            exp_out = tok.Unk;\n            test.verifyEqual(act_out,exp_out);\n        end\n        \n        function canTokenize(test)\n            enc = wordEncoding([\"foo\",\"bar\",\"##foo\"]);\n            tok = bert.tokenizer.internal.WordPieceTokenizer(enc);\n            str = \"foo bar foobar barba bafoobar barfoo\";\n            wsTok = bert.tokenizer.internal.WhitespaceTokenizer;\n            ustr = textanalytics.unicode.UTF32(wsTok.tokenize(str));\n            act_out = tok.tokenize(ustr);\n            exp_out = [\"foo\",\"bar\",tok.Unk,tok.Unk,tok.Unk,\"bar\",\"##foo\"];\n            test.verifyEqual(act_out,exp_out);\n        end\n        \n        function hasExpectedVocabSize(test,ModelAndExpectedVocabSize)\n            vocab = bert.internal.getSupportFilePath(ModelAndExpectedVocabSize.Model,\"vocab.txt\");\n            tok = bert.tokenizer.internal.WordPieceTokenizer(vocab);\n            test.verifyEqual(tok.Vocab.NumWords,ModelAndExpectedVocabSize.VocabSize);\n        end\n    end\nend scope_id: tWordPieceTokenizer scope_type: script",
  "name: tFullTokenizer file_path: test/bert/tokenizer/internal/tFullTokenizer.m start_line: 1 end_line: 35 code_snippet: classdef(SharedTestFixtures = {\n        DownloadBERTFixture}) tFullTokenizer < matlab.mock.TestCase\n    % tFullTokenizer   Unit tests for the FullTokenizer.\n    \n    % Copyright 2021-2023 The MathWorks, Inc.\n    \n    methods(Test)\n        function matchesExpectedTokenization(test)\n            % Test the tokenizer\n            vocabFile = bert.internal.getSupportFilePath(\"base\",\"vocab.txt\");\n            tok = bert.tokenizer.internal.FullTokenizer(vocabFile);\n            \n            % Create a string to tokenize.\n            str = \"UNwant\"+compose(\"\\x00E9\")+\"d,running.\";\n            exp_toks = {[\"unwanted\",\",\",\"running\",\".\"]};\n            act_toks = tok.tokenize(str);\n            test.verifyEqual(act_toks,exp_toks);\n        end\n\n        function errorsIfBasicTokenizerIsNotTokenizer(test)\n            vocabFile = bert.internal.getSupportFilePath(\"base\",\"vocab.txt\");\n            makeTok = @() bert.tokenizer.internal.FullTokenizer(vocabFile,Basic=vocabFile);\n            test.verifyError(makeTok,\"MATLAB:validators:mustBeA\");\n        end\n\n        function canSetBasicTokenizer(test)\n            [mock,behaviour] = test.createMock(?bert.tokenizer.internal.Tokenizer);\n            test.assignOutputsWhen(withAnyInputs(behaviour.tokenize),\"hello\");\n            vocabFile = bert.internal.getSupportFilePath(\"base\",\"vocab.txt\");\n            tok = bert.tokenizer.internal.FullTokenizer(vocabFile,BasicTokenizer=mock);\n            toks = tok.tokenize(\"anything\");\n            test.verifyEqual(toks,{\"hello\"}); %#ok<STRSCALR> \n        end\n    end\nend scope_id: tFullTokenizer scope_type: script",
  "name: tTokenizedDocumentTokenizer file_path: test/bert/tokenizer/internal/tTokenizedDocumentTokenizer.m line_range: 1-16 superclass: matlab scope_id: tTokenizedDocumentTokenizer scope_type: script",
  "name: tokenizationMatchesTokenizedDocument file_path: test/bert/tokenizer/internal/tTokenizedDocumentTokenizer.m line_range: 7-22 parameters: test scope_id: tTokenizedDocumentTokenizer scope_type: class",
  "name: tTokenizedDocumentTokenizer file_path: test/bert/tokenizer/internal/tTokenizedDocumentTokenizer.m start_line: 1 end_line: 30 code_snippet: classdef tTokenizedDocumentTokenizer < matlab.unittest.TestCase\n    % tTokenizedDocumentTokenizer   Unit tests for TokenizedDocumentTokenizer.\n    \n    % Copyright 2023 The MathWorks, Inc.\n    \n    methods(Test)\n        function tokenizationMatchesTokenizedDocument(test)\n            % TokenizedDocumentTokenizer does what it says on the tin -\n            % uses tokenizedDocument.\n            tok = bert.tokenizer.internal.TokenizedDocumentTokenizer;\n            str = \"a random string. doesn't matter.\";\n            toks = tok.tokenize(str);\n            doc = tokenizedDocument(str);\n            toksExp = {string(doc)};\n            test.verifyEqual(toks,toksExp);\n        end\n\n        function canSetOptions(test)\n            % We can pass in tokenization options matching\n            % tokenizedDocument's NVPs.\n            customToken = \"foo bar\";\n            tok = bert.tokenizer.internal.TokenizedDocumentTokenizer(CustomTokens=customToken);\n            str = \"in this case \"+customToken+\" is one token.\";\n            toks = tok.tokenize(str);\n            import matlab.unittest.constraints.AnyElementOf\n            import matlab.unittest.constraints.IsEqualTo\n            test.verifyThat(AnyElementOf(toks{1}),IsEqualTo(customToken));\n        end\n    end\nend scope_id: tTokenizedDocumentTokenizer scope_type: script",
  "name: tBasicTokenizer file_path: test/bert/tokenizer/internal/tBasicTokenizer.m line_range: 1-11 superclass: matlab scope_id: tBasicTokenizer scope_type: script",
  "name: canConstruct file_path: test/bert/tokenizer/internal/tBasicTokenizer.m line_range: 7-17 parameters: test scope_id: tBasicTokenizer scope_type: class",
  "name: tokenizesCJK file_path: test/bert/tokenizer/internal/tBasicTokenizer.m start_line: 51 end_line: 61 input_parameters: ['test'] code_snippet:         \n        function tokenizesCJK(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();            \n            str = strcat(...\n                compose(\"Arbitrary \\x4E01\\x4E02 CJK chars \\xD869\\xDF00\\xD86D\\xDE3E\"),...\n                \"more\");\n            exp_out = {[\"arbitrary\",compose(\"\\x4E01\"),compose(\"\\x4E02\"),\"cjk\",\"chars\",...\n                compose(\"\\xD869\\xDF00\"),compose(\"\\xD86D\\xDE3E\"),\"more\"]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end scope_id: tBasicTokenizer scope_type: script",
  "name: splitsOnPunctuation file_path: test/bert/tokenizer/internal/tBasicTokenizer.m start_line: 62 end_line: 69 input_parameters: ['test'] code_snippet:         \n        function splitsOnPunctuation(test)\n            tok = bert.tokenizer.internal.BasicTokenizer(); \n            str = \"hello. hello, world? hello world! hello\";\n            exp_out = {[\"hello\",\".\",\"hello\",\",\",\"world\",\"?\",\"hello\",\"world\",\"!\",\"hello\"]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end scope_id: tBasicTokenizer scope_type: script",
  "name: stripsAccents file_path: test/bert/tokenizer/internal/tBasicTokenizer.m start_line: 70 end_line: 77 input_parameters: ['test'] code_snippet:         \n        function stripsAccents(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();\n            str = compose(\"h\\x00E9llo\");\n            exp_out = {\"hello\"};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end scope_id: tBasicTokenizer scope_type: script",
  "name: canBeCaseSensitive file_path: test/bert/tokenizer/internal/tBasicTokenizer.m start_line: 78 end_line: 85 input_parameters: ['test'] code_snippet:         \n        function canBeCaseSensitive(test)\n            tok = bert.tokenizer.internal.BasicTokenizer('IgnoreCase',false);\n            str = \"FOO bAr baz\";\n            exp_out = {[\"FOO\",\"bAr\",\"baz\"]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end scope_id: tBasicTokenizer scope_type: script",
  "name: tBasicTokenizer file_path: test/bert/tokenizer/internal/tBasicTokenizer.m start_line: 1 end_line: 88 code_snippet: classdef tBasicTokenizer < matlab.unittest.TestCase\n    % tBasicTokenizer   Unit tests for the BasicTokenizer\n    \n    % Copyright 2021-2023 The MathWorks, Inc.    \n    \n    methods(Test)\n        function canConstruct(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();\n            test.verifyClass(tok,'bert.tokenizer.internal.BasicTokenizer');\n            test.verifyInstanceOf(tok,'bert.tokenizer.internal.Tokenizer');\n        end\n        \n        function canTokenize(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();\n            str = \"foo bar baz\";\n            exp_out = {[\"foo\",\"bar\",\"baz\"]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);           \n        end\n\n        function canTokenizeBatch(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();\n            manyStrs = repmat(\"foo bar baz\",1,20);\n            act_out = tokenize(tok, manyStrs);\n            exp_out = arrayfun(@(str) tokenize(tok,str),manyStrs,UniformOutput=false);\n            exp_out = [exp_out{:}];\n            test.verifyEqual(act_out,exp_out);           \n        end\n        \n        function removesControlCharactersAndWhitespace(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();\n            aControlChar = compose('\\x000A');\n            aFormatChar = compose('\\xDB40\\xDC7E');\n            aSpaceChar = compose('\\x00A0');\n            words = [\"Testing\",\"a\",\"blah\"];\n            str = strcat(words(1),\" \",aFormatChar,\" \",...\n                words(2),aFormatChar,\" \",aControlChar,\" \",words(3),aSpaceChar);\n            exp_out = {[lower(words(1)),words(2),words(3)]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end\n        \n        function splitsOnNewlines(test)\n            % Regression test for a bug\n            tok = bert.tokenizer.internal.BasicTokenizer();\n            str = \"hello\"+newline+\"world\";\n            act_toks = tok.tokenize(str);\n            exp_toks = {[\"hello\",\"world\"]};\n            test.verifyEqual(act_toks,exp_toks);\n        end\n        \n        function tokenizesCJK(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();            \n            str = strcat(...\n                compose(\"Arbitrary \\x4E01\\x4E02 CJK chars \\xD869\\xDF00\\xD86D\\xDE3E\"),...\n                \"more\");\n            exp_out = {[\"arbitrary\",compose(\"\\x4E01\"),compose(\"\\x4E02\"),\"cjk\",\"chars\",...\n                compose(\"\\xD869\\xDF00\"),compose(\"\\xD86D\\xDE3E\"),\"more\"]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end\n        \n        function splitsOnPunctuation(test)\n            tok = bert.tokenizer.internal.BasicTokenizer(); \n            str = \"hello. hello, world? hello world! hello\";\n            exp_out = {[\"hello\",\".\",\"hello\",\",\",\"world\",\"?\",\"hello\",\"world\",\"!\",\"hello\"]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end\n        \n        function stripsAccents(test)\n            tok = bert.tokenizer.internal.BasicTokenizer();\n            str = compose(\"h\\x00E9llo\");\n            exp_out = {\"hello\"};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end\n        \n        function canBeCaseSensitive(test)\n            tok = bert.tokenizer.internal.BasicTokenizer('IgnoreCase',false);\n            str = \"FOO bAr baz\";\n            exp_out = {[\"FOO\",\"bAr\",\"baz\"]};\n            act_out = tok.tokenize(str);\n            test.verifyEqual(act_out,exp_out);\n        end\n    end\nend\n scope_id: tBasicTokenizer scope_type: script",
  "name: tblock file_path: test/gpt2/layer/tblock.m line_range: 1-8 superclass: matlab scope_id: tblock scope_type: script",
  "name: dlarray file_path: test/gpt2/layer/tblock.m start_line: 18 end_line: 28 input_parameters: ['Input'] code_snippet:         function outputHasInputSize(test,Input)\n            % The block is simply a composition of other layers. Simply\n            % verify the output of a block is the same size as the input,\n            % and as such the blocks can be stacked.\n            x = dlarray(Input);\n            C = size(Input,1);\n            weights = test.randomWeights(C);\n            hyperParameters.NumHeads = 1;\n            y = test.block(x,[],weights,hyperParameters);\n            test.verifySize(y,size(x));\n        end scope_id: tblock scope_type: script",
  "name: randomWeights file_path: test/gpt2/layer/tblock.m start_line: 52 end_line: 67 input_parameters: ['test', 'C'] code_snippet:         function weights = randomWeights(test,C)\n            % C is num features, or latent dimension of the block\n            g1 = dlarray(rand(C,1));\n            b1 = dlarray(rand(C,1));\n            g2 = dlarray(rand(C,1));\n            b2 = dlarray(rand(C,1));\n            W_A1 = dlarray(rand(3*C,C));\n            W_A2 = dlarray(rand(C));\n            b_A1 = dlarray(rand(3*C,1));\n            b_A2 = dlarray(rand(C,1));\n            W_P1 = dlarray(rand(C));\n            b_P1 = dlarray(rand(C,1));\n            W_P2 = dlarray(rand(C));\n            b_P2 = dlarray(rand(C,1));\n            weights = test.prepareBlockWeightsStruct(g1,b1,W_A1,b_A1,W_A2,b_A2,g2,b2,W_P1,b_P1,W_P2,b_P2);\n        end scope_id: tblock scope_type: script",
  "name: prepareBlockWeightsStruct file_path: test/gpt2/layer/tblock.m start_line: 68 end_line: 83 input_parameters: ['test', 'g1', 'b1', 'W_A1', 'b_A1', 'W_A2', 'b_A2', 'g2', 'b2', 'W_P1', 'b_P1', 'W_P2', 'b_P2'] code_snippet:         \n        function s = prepareBlockWeightsStruct(test,g1,b1,W_A1,b_A1,W_A2,b_A2,g2,b2,W_P1,b_P1,W_P2,b_P2)\n            % Merge various structs that have the appropriate weight naming\n            % syntax.\n            s_ln = test.prepareLayerNormWeightsStruct(g1,b1,g2,b2);\n            s_attn = test.prepareAttentionWeightsStruct(W_A1,b_A1,W_A2,b_A2);\n            s_mlp = test.prepareMLPWeightsStruct(W_P1,b_P1,W_P2,b_P2);\n            c = {s_ln,s_attn,s_mlp};\n            fn = cellfun(@fieldnames,c,'UniformOutput',false);\n            fn = cat(1,fn{:});\n            fv = cellfun(@struct2cell,c,'UniformOutput',false);\n            fv = cat(1,fv{:});\n            s = struct();\n            for i = 1:numel(fn)\n                s.(fn{i}) = fv{i};\n            end scope_id: tblock scope_type: script",
  "name: prepareAttentionWeightsStruct file_path: test/gpt2/layer/tblock.m start_line: 85 end_line: 94 input_parameters: ['~', 'W1', 'b1', 'W2', 'b2'] code_snippet:         \n        function s = prepareAttentionWeightsStruct(~,W1,b1,W2,b2)\n            % Prepare a struct compatible with the weights input of\n            % attention. These are for the fully connected layers.\n            s = struct(...\n                'attn_c_attn_w_0',W1,...\n                'attn_c_attn_b_0',b1,...\n                'attn_c_proj_w_0',W2,...\n                'attn_c_proj_b_0',b2);\n        end scope_id: tblock scope_type: script",
  "name: prepareLayerNormWeightsStruct file_path: test/gpt2/layer/tblock.m start_line: 95 end_line: 104 input_parameters: ['~', 'g1', 'b1', 'g2', 'b2'] code_snippet:         \n        function s = prepareLayerNormWeightsStruct(~,g1,b1,g2,b2)\n            % Prepare a struct of weights compatible with the two layer\n            % norm calls in block\n            s = struct(...\n                'ln_1_g_0',g1,...\n                'ln_1_b_0',b1,...\n                'ln_2_g_0',g2,...\n                'ln_2_b_0',b2);\n        end scope_id: tblock scope_type: script",
  "name: prepareMLPWeightsStruct file_path: test/gpt2/layer/tblock.m start_line: 105 end_line: 114 input_parameters: ['~', 'W1', 'b1', 'W2', 'b2'] code_snippet:         \n        function s = prepareMLPWeightsStruct(~,W1,b1,W2,b2)\n            % Create a struct of weights to be consumed by\n            % transformer.layer.multiLayerPerceptron\n            s = struct(...\n                'mlp_c_fc_w_0',W1,...\n                'mlp_c_fc_b_0',b1,...\n                'mlp_c_proj_w_0',W2,...\n                'mlp_c_proj_b_0',b2);\n        end scope_id: tblock scope_type: script",
  "name: tblock file_path: test/gpt2/layer/tblock.m start_line: 1 end_line: 116 code_snippet: classdef tblock < matlab.unittest.TestCase\n    % tblock   Unit tests for transformer.layer.block\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    properties(Constant, Access=private)\n        block = @gpt2.layer.block\n    end\n    \n    properties(TestParameter)\n        Input = struct(...\n            'Scalar', 1,...\n            'Vector', 1:5,...\n            'Matrix', reshape(1:6,[3,2]))\n    end\n    \n    methods(Test)\n        function outputHasInputSize(test,Input)\n            % The block is simply a composition of other layers. Simply\n            % verify the output of a block is the same size as the input,\n            % and as such the blocks can be stacked.\n            x = dlarray(Input);\n            C = size(Input,1);\n            weights = test.randomWeights(C);\n            hyperParameters.NumHeads = 1;\n            y = test.block(x,[],weights,hyperParameters);\n            test.verifySize(y,size(x));\n        end\n        \n        function outputHasInputSizeWithPasts(test,Input)\n            % As above but using \"pasts\" - a concatenation of key and value\n            % matrices.\n            x = dlarray(Input);\n            C = size(Input,1);\n            weights = test.randomWeights(C);\n            hyperParameters.NumHeads = 1;\n            % Provide a fake past of sequence length 1\n            K_fake = dlarray(rand(C,1));\n            V_fake = dlarray(rand(C,1));\n            past = cat(5,K_fake,V_fake);\n            [y,present] = test.block(x,past,weights,hyperParameters);\n            test.verifySize(y,size(x));\n            % The size of presents is the size of past except the sequence\n            % dimension gets extended by the sequence length of y\n            exp_present_size = size(past);\n            exp_present_size(2) = exp_present_size(2)+size(y,2);\n            test.verifySize(present,exp_present_size);\n        end\n    end\n    \n    methods(Access=private)\n        function weights = randomWeights(test,C)\n            % C is num features, or latent dimension of the block\n            g1 = dlarray(rand(C,1));\n            b1 = dlarray(rand(C,1));\n            g2 = dlarray(rand(C,1));\n            b2 = dlarray(rand(C,1));\n            W_A1 = dlarray(rand(3*C,C));\n            W_A2 = dlarray(rand(C));\n            b_A1 = dlarray(rand(3*C,1));\n            b_A2 = dlarray(rand(C,1));\n            W_P1 = dlarray(rand(C));\n            b_P1 = dlarray(rand(C,1));\n            W_P2 = dlarray(rand(C));\n            b_P2 = dlarray(rand(C,1));\n            weights = test.prepareBlockWeightsStruct(g1,b1,W_A1,b_A1,W_A2,b_A2,g2,b2,W_P1,b_P1,W_P2,b_P2);\n        end\n        \n        function s = prepareBlockWeightsStruct(test,g1,b1,W_A1,b_A1,W_A2,b_A2,g2,b2,W_P1,b_P1,W_P2,b_P2)\n            % Merge various structs that have the appropriate weight naming\n            % syntax.\n            s_ln = test.prepareLayerNormWeightsStruct(g1,b1,g2,b2);\n            s_attn = test.prepareAttentionWeightsStruct(W_A1,b_A1,W_A2,b_A2);\n            s_mlp = test.prepareMLPWeightsStruct(W_P1,b_P1,W_P2,b_P2);\n            c = {s_ln,s_attn,s_mlp};\n            fn = cellfun(@fieldnames,c,'UniformOutput',false);\n            fn = cat(1,fn{:});\n            fv = cellfun(@struct2cell,c,'UniformOutput',false);\n            fv = cat(1,fv{:});\n            s = struct();\n            for i = 1:numel(fn)\n                s.(fn{i}) = fv{i};\n            end\n        end\n        \n        function s = prepareAttentionWeightsStruct(~,W1,b1,W2,b2)\n            % Prepare a struct compatible with the weights input of\n            % attention. These are for the fully connected layers.\n            s = struct(...\n                'attn_c_attn_w_0',W1,...\n                'attn_c_attn_b_0',b1,...\n                'attn_c_proj_w_0',W2,...\n                'attn_c_proj_b_0',b2);\n        end\n        \n        function s = prepareLayerNormWeightsStruct(~,g1,b1,g2,b2)\n            % Prepare a struct of weights compatible with the two layer\n            % norm calls in block\n            s = struct(...\n                'ln_1_g_0',g1,...\n                'ln_1_b_0',b1,...\n                'ln_2_g_0',g2,...\n                'ln_2_b_0',b2);\n        end\n        \n        function s = prepareMLPWeightsStruct(~,W1,b1,W2,b2)\n            % Create a struct of weights to be consumed by\n            % transformer.layer.multiLayerPerceptron\n            s = struct(...\n                'mlp_c_fc_w_0',W1,...\n                'mlp_c_fc_b_0',b1,...\n                'mlp_c_proj_w_0',W2,...\n                'mlp_c_proj_b_0',b2);\n        end\n    end\nend scope_id: tblock scope_type: script",
  "name: testTokenization file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 60 end_line: 66 input_parameters: ['test', 'StringToTokenize'] code_snippet:         \n        function testTokenization(test, StringToTokenize)\n            % Test the regex tokenization used by the Encoder.\n            enc = test.Encoder;\n            toks = iTokenize(StringToTokenize.string,enc);\n            test.verifyEqual(toks,StringToTokenize.exp);\n        end         scope_id: tGPT2Tokenizer scope_type: script",
  "name: byteEncoderSize file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 67 end_line: 72 input_parameters: ['test'] code_snippet:         \n        function byteEncoderSize(test)\n            % Verify the expected size of the byte encoder.\n            enc = test.Encoder;\n            test.verifyEqual(numel(unique(enc.ByteEncoder)),256);\n        end scope_id: tGPT2Tokenizer scope_type: script",
  "name: independenceOfInputClass file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 73 end_line: 80 input_parameters: ['test', 'StringToEncode'] code_snippet:         \n        function independenceOfInputClass(test, StringToEncode)\n            % Verify that it doesn't matter if input text is string or char\n            enc = test.Encoder;\n            str = convertCharsToStrings(StringToEncode);\n            ch = convertStringsToChars(StringToEncode);\n            test.verifyEqual(enc.encode(str),enc.encode(ch));\n        end scope_id: tGPT2Tokenizer scope_type: script",
  "name: commentNotInBPE file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 81 end_line: 101 input_parameters: ['test'] code_snippet:         \n        function commentNotInBPE(test)\n            % The vocab.bpe includes a comment that we must manually strip\n            % out. Ensure we do that.\n            vocabFile = gpt2.internal.getSupportFilePath(\"gpt2_vocab.bpe\");\n            fid = fopen(vocabFile,'r','n','utf-8');\n            s = textscan(fid,'%s', 'Delimiter', '\\n');\n            fclose(fid);\n            s = s{1};\n            % first line of vocab.bpe is the comment\n            firstLine = s(1);\n            % split as in Encoder and check we don't accidentally add these\n            % values to BPERanks.\n            splitfirstLine = split(convertCharsToStrings(firstLine));\n            enc = test.Encoder;\n            actBPE = enc.BPERanks;\n            % splitfirstLine is a string, ensure actBPE is\n            test.assertClass(actBPE,'string');\n            actBPEMatchesComment = any(actBPE==splitfirstLine(1),'all') || any(actBPE==splitfirstLine(2),'all');\n            test.verifyFalse(actBPEMatchesComment, \"A match was found between the encoder's BPERanks and the comment in vocab.bpe\");\n        end scope_id: tGPT2Tokenizer scope_type: script",
  "name: decodeInvertsEncode file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 102 end_line: 109 input_parameters: ['test', 'StringToEncode'] code_snippet:         \n        function decodeInvertsEncode(test,StringToEncode)\n            % Ensure decoding inverts encoding\n            enc = test.Encoder;\n            encoded = enc.encode(StringToEncode);\n            decoded = enc.decode(encoded);\n            test.verifyMatches(decoded,StringToEncode);\n        end scope_id: tGPT2Tokenizer scope_type: script",
  "name: setupEncoder file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 113 end_line: 117 input_parameters: ['test'] code_snippet:         function setupEncoder(test)\n            % Setup an encoder to be used by multiple tests.\n            % Note this is a handle.\n            test.Encoder = gpt2.tokenizer.GPT2Tokenizer(iModelName(),iModelPath());\n        end scope_id: tGPT2Tokenizer scope_type: script",
  "name: iModelName file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 120 end_line: 123 code_snippet: \nfunction modelName = iModelName()\nmodelName = 'gpt2-355M';\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iModelPath file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 124 end_line: 127 code_snippet: \nfunction modelPath = iModelPath()\nmodelPath = fullfile(getRepoRoot());\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iIsEqualTo file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 128 end_line: 131 input_parameters: ['varargin'] code_snippet: \nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iMatches file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 132 end_line: 135 input_parameters: ['varargin'] code_snippet: \nfunction constraint = iMatches(varargin)\nconstraint = matlab.unittest.constraints.Matches(varargin{:});\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iGetUnicodeTextAndEncodedText file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 136 end_line: 159 code_snippet: \nfunction parameter = iGetUnicodeTextAndEncodedText()\n\nparameter = struct;\n\nparameter.NormalSentenceFragment1 = struct( ...\n    'UnicodeText', 'In this tutorial we will see', ...\n    'EncodedText', [818 428 11808 356 481 766] + 1 );\n\nparameter.NonsenseFragment1 = struct( ...\n    'UnicodeText', 'esgarghr', ...\n    'EncodedText', [274 4563 456 81] + 1 );\n\n% Weird edge case I found when I wasn't doing the Unicode decoding\n% correctly. If you set \"k=1\" and input the sentence \"In this tutorial \",\n% into GPT-2 the first output is a special Unicode character known as a\n% \"Non-breaking space\". After thinking about this, this weird behaviour is\n% likely due to the fact that the tokenizer for GPT-2 tokenizes words so\n% that they are preceded by spaces.\nparameter.NonBreakingSpace = struct( ...\n    'UnicodeText', native2unicode([194 160], 'UTF-8'), ...\n    'EncodedText', 1849 + 1 );\n\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iStringToEncode file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 160 end_line: 171 code_snippet: \nfunction c = iStringToEncode()\n% A test parameter of strings to be passed to the encoder.\nc = {\n    'In this tutorial we will see'\n    'esgarghr'\n    'foo bar baz'\n    'A word and numbers 123 or 456word and decimal 7.89'\n    'particles like don''t do that'\n    'multiple white    space characters'\n    };\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iStringToTokenize file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 172 end_line: 188 code_snippet: \nfunction s = iStringToTokenize()\n% A test parameter of strings to be tokenized using the regular expression\n% attached to tokenizer.GPT2Tokenizer, and the expected output tokens.\ns = struct( ...\n    'oneWord', iStringToTokenizeStruct(\"foo\",\"foo\"), ...\n    'twoWords', iStringToTokenizeStruct(\"foo bar\",[\"foo\",\" bar\"]), ...\n    'possessive', iStringToTokenizeStruct(\"foo's\",[\"foo\",\"'s\"]), ...\n    'not', iStringToTokenizeStruct(\"foo't\", [\"foo\",\"'t\"]), ...\n    'are', iStringToTokenizeStruct(\"foo're\", [\"foo\",\"'re\"]), ...\n    'have', iStringToTokenizeStruct(\"foo've\", [\"foo\",\"'ve\"]), ...\n    'm', iStringToTokenizeStruct(\"foo'm\",[\"foo\",\"'m\"]), ...\n    'will', iStringToTokenizeStruct(\"foo'll\",[\"foo\",\"'ll\"]), ...\n    'would', iStringToTokenizeStruct(\"foo'd\",[\"foo\",\"'d\"]), ...\n    'numbers', iStringToTokenizeStruct(\"foo 123bar\",[\"foo\",\" 123\",\"bar\"]), ...\n    'multiWS', iStringToTokenizeStruct(\"foo   bar\",[\"foo\", \"  \", \" bar\"]));\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iStringToTokenizeStruct file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 189 end_line: 193 input_parameters: ['str', 'expTokens'] code_snippet: \nfunction s = iStringToTokenizeStruct(str,expTokens)\n% A struct for a case of the StringToTokenize test parameter.\ns = struct('string',str,'exp',expTokens);\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: iTokenize file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 194 end_line: 198 input_parameters: ['str', 'enc'] code_snippet: \nfunction toks = iTokenize(str,enc)\n% Tokenize equivalently to tokenizer.GPT2Tokenizer\n[toks,~] = regexp(str,enc.TokenizationExpression,'match','split');\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: tGPT2Tokenizer file_path: test/gpt2/tokenizer/tGPT2Tokenizer.m start_line: 1 end_line: 198 code_snippet: classdef(SharedTestFixtures = {DownloadGPT2Fixture}) tGPT2Tokenizer < matlab.unittest.TestCase\n    % tGPT2Tokenizer   Tests for the text encoder class\n    \n    % Copyright 2020 The MathWorks, Inc.\n    \n    properties\n        % Setup a default encoder for testing\n        Encoder\n    end\n    \n    properties(TestParameter)\n        % UnicodeTextAndEncodedText\n        UnicodeTextAndEncodedText = iGetUnicodeTextAndEncodedText()\n        \n        % Strings to be tokenized by the regex and expected results\n        StringToTokenize = iStringToTokenize()\n        \n        % Test strings for encoding\n        StringToEncode = iStringToEncode()\n    end\n    \n    methods(TestClassSetup)\n        function setup(test)\n            test.setupEncoder();\n        end\n    end\n    \n    methods(Test)\n        function encodeGivesCorrectResults(test, UnicodeTextAndEncodedText)\n            % Unpack the test parameter\n            inputText = UnicodeTextAndEncodedText.UnicodeText;\n            expectedOutputText = UnicodeTextAndEncodedText.EncodedText;\n            \n            % Create the encoder\n            enc = gpt2.tokenizer.GPT2Tokenizer(iModelName(), iModelPath());\n            \n            % Get the output text\n            actualOutputText = enc.encode(inputText);\n            \n            % Verify the output is correct\n            test.verifyThat( ...\n                actualOutputText, iIsEqualTo(expectedOutputText) );\n        end\n        \n        function decodeGivesCorrectResults(test, UnicodeTextAndEncodedText)\n            % Unpack the test parameter\n            inputText = UnicodeTextAndEncodedText.EncodedText;\n            expectedOutputText = UnicodeTextAndEncodedText.UnicodeText;\n            \n            % Create the encoder\n            enc = gpt2.tokenizer.GPT2Tokenizer(iModelName(), iModelPath());\n            \n            % Get the output text\n            actualOutputText = enc.decode(inputText);\n            \n            % Verify the output is correct\n            test.verifyThat( ...\n                actualOutputText, iMatches(expectedOutputText) );\n        end        \n        \n        function testTokenization(test, StringToTokenize)\n            % Test the regex tokenization used by the Encoder.\n            enc = test.Encoder;\n            toks = iTokenize(StringToTokenize.string,enc);\n            test.verifyEqual(toks,StringToTokenize.exp);\n        end        \n        \n        function byteEncoderSize(test)\n            % Verify the expected size of the byte encoder.\n            enc = test.Encoder;\n            test.verifyEqual(numel(unique(enc.ByteEncoder)),256);\n        end\n        \n        function independenceOfInputClass(test, StringToEncode)\n            % Verify that it doesn't matter if input text is string or char\n            enc = test.Encoder;\n            str = convertCharsToStrings(StringToEncode);\n            ch = convertStringsToChars(StringToEncode);\n            test.verifyEqual(enc.encode(str),enc.encode(ch));\n        end\n        \n        function commentNotInBPE(test)\n            % The vocab.bpe includes a comment that we must manually strip\n            % out. Ensure we do that.\n            vocabFile = gpt2.internal.getSupportFilePath(\"gpt2_vocab.bpe\");\n            fid = fopen(vocabFile,'r','n','utf-8');\n            s = textscan(fid,'%s', 'Delimiter', '\\n');\n            fclose(fid);\n            s = s{1};\n            % first line of vocab.bpe is the comment\n            firstLine = s(1);\n            % split as in Encoder and check we don't accidentally add these\n            % values to BPERanks.\n            splitfirstLine = split(convertCharsToStrings(firstLine));\n            enc = test.Encoder;\n            actBPE = enc.BPERanks;\n            % splitfirstLine is a string, ensure actBPE is\n            test.assertClass(actBPE,'string');\n            actBPEMatchesComment = any(actBPE==splitfirstLine(1),'all') || any(actBPE==splitfirstLine(2),'all');\n            test.verifyFalse(actBPEMatchesComment, \"A match was found between the encoder's BPERanks and the comment in vocab.bpe\");\n        end\n        \n        function decodeInvertsEncode(test,StringToEncode)\n            % Ensure decoding inverts encoding\n            enc = test.Encoder;\n            encoded = enc.encode(StringToEncode);\n            decoded = enc.decode(encoded);\n            test.verifyMatches(decoded,StringToEncode);\n        end\n    end\n    \n    methods(Access=private)\n        function setupEncoder(test)\n            % Setup an encoder to be used by multiple tests.\n            % Note this is a handle.\n            test.Encoder = gpt2.tokenizer.GPT2Tokenizer(iModelName(),iModelPath());\n        end\n    end\nend\n\nfunction modelName = iModelName()\nmodelName = 'gpt2-355M';\nend\n\nfunction modelPath = iModelPath()\nmodelPath = fullfile(getRepoRoot());\nend\n\nfunction constraint = iIsEqualTo(varargin)\nconstraint = matlab.unittest.constraints.IsEqualTo(varargin{:});\nend\n\nfunction constraint = iMatches(varargin)\nconstraint = matlab.unittest.constraints.Matches(varargin{:});\nend\n\nfunction parameter = iGetUnicodeTextAndEncodedText()\n\nparameter = struct;\n\nparameter.NormalSentenceFragment1 = struct( ...\n    'UnicodeText', 'In this tutorial we will see', ...\n    'EncodedText', [818 428 11808 356 481 766] + 1 );\n\nparameter.NonsenseFragment1 = struct( ...\n    'UnicodeText', 'esgarghr', ...\n    'EncodedText', [274 4563 456 81] + 1 );\n\n% Weird edge case I found when I wasn't doing the Unicode decoding\n% correctly. If you set \"k=1\" and input the sentence \"In this tutorial \",\n% into GPT-2 the first output is a special Unicode character known as a\n% \"Non-breaking space\". After thinking about this, this weird behaviour is\n% likely due to the fact that the tokenizer for GPT-2 tokenizes words so\n% that they are preceded by spaces.\nparameter.NonBreakingSpace = struct( ...\n    'UnicodeText', native2unicode([194 160], 'UTF-8'), ...\n    'EncodedText', 1849 + 1 );\n\nend\n\nfunction c = iStringToEncode()\n% A test parameter of strings to be passed to the encoder.\nc = {\n    'In this tutorial we will see'\n    'esgarghr'\n    'foo bar baz'\n    'A word and numbers 123 or 456word and decimal 7.89'\n    'particles like don''t do that'\n    'multiple white    space characters'\n    };\nend\n\nfunction s = iStringToTokenize()\n% A test parameter of strings to be tokenized using the regular expression\n% attached to tokenizer.GPT2Tokenizer, and the expected output tokens.\ns = struct( ...\n    'oneWord', iStringToTokenizeStruct(\"foo\",\"foo\"), ...\n    'twoWords', iStringToTokenizeStruct(\"foo bar\",[\"foo\",\" bar\"]), ...\n    'possessive', iStringToTokenizeStruct(\"foo's\",[\"foo\",\"'s\"]), ...\n    'not', iStringToTokenizeStruct(\"foo't\", [\"foo\",\"'t\"]), ...\n    'are', iStringToTokenizeStruct(\"foo're\", [\"foo\",\"'re\"]), ...\n    'have', iStringToTokenizeStruct(\"foo've\", [\"foo\",\"'ve\"]), ...\n    'm', iStringToTokenizeStruct(\"foo'm\",[\"foo\",\"'m\"]), ...\n    'will', iStringToTokenizeStruct(\"foo'll\",[\"foo\",\"'ll\"]), ...\n    'would', iStringToTokenizeStruct(\"foo'd\",[\"foo\",\"'d\"]), ...\n    'numbers', iStringToTokenizeStruct(\"foo 123bar\",[\"foo\",\" 123\",\"bar\"]), ...\n    'multiWS', iStringToTokenizeStruct(\"foo   bar\",[\"foo\", \"  \", \" bar\"]));\nend\n\nfunction s = iStringToTokenizeStruct(str,expTokens)\n% A struct for a case of the StringToTokenize test parameter.\ns = struct('string',str,'exp',expTokens);\nend\n\nfunction toks = iTokenize(str,enc)\n% Tokenize equivalently to tokenizer.GPT2Tokenizer\n[toks,~] = regexp(str,enc.TokenizationExpression,'match','split');\nend scope_id: tGPT2Tokenizer scope_type: script",
  "name: gpt2.layer.block file_path: +gpt2/+layer/block.m start_line: 1 end_line: 82 input_parameters: ['X', 'past', 'weights', 'hyperParameters'] code_snippet: function [X, present] = block(X, past, weights, hyperParameters)\n% block   Transformer block for GPT-2\n%\n%   [X, present] = block(X, past, weights, hyperParameters) computes a\n%   GPT-2 style transformer block on the input X as described in [1] (see\n%   Section 2.3). One difference between this style of transformer block\n%   and others is that this block uses layer normalization at the\n%   beginning.\n%\n%   Inputs:\n%       X               - A (numFeatures*numHeads)-by-numInputSubwords\n%                         input array.\n%       past            - A numFeatures-by-numPastSubwords-by-numHeads-by-2\n%                         array. This contains the 'keys' and 'values' for\n%                         past subwords. These are needed to predict future\n%                         outputs in an autoregressive manner. 'keys' are\n%                         stored in past(:,:,:,1) and 'values' are stored\n%                         in past(:,:,:,2).\n%       weights         - The weights for the transformer block stored in a\n%                         struct. In this block we have:\n%                           - ln_1_g_0: Weight vector for the first layer\n%                             normalization.\n%                           - ln_1_b_0: Bias vector for the first layer\n%                             normalization.\n%                           - ln_2_g_0: Weight vector for the second layer\n%                             normalization.\n%                           - ln_2_b_0: Bias vector for the second layer\n%                             normalization.\n%                         In the attention sub-block:\n%                           - attn_c_attn_w_0: A weight matrix for the\n%                             first fully connected layer.\n%                           - attn_c_attn_b_0: A bias vector for the first\n%                             fully connected layer.\n%                           - attn_c_proj_w_0: A weight matrix for the\n%                             final fully connected layer.\n%                           - attn_c_proj_b_0: A bias vector for the final\n%                             fully connected layer.\n%                         In the multi-layer perceptron block:\n%                           - mlp_c_fc_w_0: A weight matrix for the first\n%                             fully connected layer.\n%                           - mlp_c_fc_b_0: A bias vector for the first\n%                             fully connected layer.\n%                           - mlp_c_proj_w_0: A weight matrix for the\n%                             second fully connected layer.\n%                           - mlp_c_proj_b_0: A bias vector for the second\n%                             fully connected layer.\n%       numHeads        - The number of attention heads. This is a\n%                         hyper-parameter.\n%\n%   Outputs:\n%       Z               - A (numFeatures*numHeads)-by-numInputSubwords\n%                         output array.\n%       present         - A numFeatures-by-numAllSubwords-by-numHeads-by-2\n%                         array. This contains the 'keys' and 'values' that\n%                         are created from inputs. These need to passed\n%                         back in as the 'past' input if we want to predict\n%                         future outputs in an autoregressive manner. 'keys'\n%                         are stored in present(:,:,:,1) and 'values' are\n%                         stored in present(:,:,:,2).\n%\n%   References:\n%\n%   [1] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,\n%       Ilya Sutskever, \"Language Models are Unsupervised Multitask\n%       Learners\",\n%       https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n\nXNorm1 = transformer.layer.normalization(X, ...\n    weights.ln_1_g_0, weights.ln_1_b_0);\n\n[A, present] = transformer.layer.attention(XNorm1, past, weights, hyperParameters);\n\nX = X + A;\n \nXNorm2 = transformer.layer.normalization(X, ...\n    weights.ln_2_g_0, weights.ln_2_b_0);\n\nM = transformer.layer.multiLayerPerceptron(XNorm2, weights);\n\nX = X + M;\n\nend scope_id: block scope_type: script",
  "name: block file_path: +gpt2/+layer/block.m start_line: 1 end_line: 82 code_snippet: function [X, present] = block(X, past, weights, hyperParameters)\n% block   Transformer block for GPT-2\n%\n%   [X, present] = block(X, past, weights, hyperParameters) computes a\n%   GPT-2 style transformer block on the input X as described in [1] (see\n%   Section 2.3). One difference between this style of transformer block\n%   and others is that this block uses layer normalization at the\n%   beginning.\n%\n%   Inputs:\n%       X               - A (numFeatures*numHeads)-by-numInputSubwords\n%                         input array.\n%       past            - A numFeatures-by-numPastSubwords-by-numHeads-by-2\n%                         array. This contains the 'keys' and 'values' for\n%                         past subwords. These are needed to predict future\n%                         outputs in an autoregressive manner. 'keys' are\n%                         stored in past(:,:,:,1) and 'values' are stored\n%                         in past(:,:,:,2).\n%       weights         - The weights for the transformer block stored in a\n%                         struct. In this block we have:\n%                           - ln_1_g_0: Weight vector for the first layer\n%                             normalization.\n%                           - ln_1_b_0: Bias vector for the first layer\n%                             normalization.\n%                           - ln_2_g_0: Weight vector for the second layer\n%                             normalization.\n%                           - ln_2_b_0: Bias vector for the second layer\n%                             normalization.\n%                         In the attention sub-block:\n%                           - attn_c_attn_w_0: A weight matrix for the\n%                             first fully connected layer.\n%                           - attn_c_attn_b_0: A bias vector for the first\n%                             fully connected layer.\n%                           - attn_c_proj_w_0: A weight matrix for the\n%                             final fully connected layer.\n%                           - attn_c_proj_b_0: A bias vector for the final\n%                             fully connected layer.\n%                         In the multi-layer perceptron block:\n%                           - mlp_c_fc_w_0: A weight matrix for the first\n%                             fully connected layer.\n%                           - mlp_c_fc_b_0: A bias vector for the first\n%                             fully connected layer.\n%                           - mlp_c_proj_w_0: A weight matrix for the\n%                             second fully connected layer.\n%                           - mlp_c_proj_b_0: A bias vector for the second\n%                             fully connected layer.\n%       numHeads        - The number of attention heads. This is a\n%                         hyper-parameter.\n%\n%   Outputs:\n%       Z               - A (numFeatures*numHeads)-by-numInputSubwords\n%                         output array.\n%       present         - A numFeatures-by-numAllSubwords-by-numHeads-by-2\n%                         array. This contains the 'keys' and 'values' that\n%                         are created from inputs. These need to passed\n%                         back in as the 'past' input if we want to predict\n%                         future outputs in an autoregressive manner. 'keys'\n%                         are stored in present(:,:,:,1) and 'values' are\n%                         stored in present(:,:,:,2).\n%\n%   References:\n%\n%   [1] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,\n%       Ilya Sutskever, \"Language Models are Unsupervised Multitask\n%       Learners\",\n%       https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n\nXNorm1 = transformer.layer.normalization(X, ...\n    weights.ln_1_g_0, weights.ln_1_b_0);\n\n[A, present] = transformer.layer.attention(XNorm1, past, weights, hyperParameters);\n\nX = X + A;\n \nXNorm2 = transformer.layer.normalization(X, ...\n    weights.ln_2_g_0, weights.ln_2_b_0);\n\nM = transformer.layer.multiLayerPerceptron(XNorm2, weights);\n\nX = X + M;\n\nend scope_id: block scope_type: script",
  "name: gpt2.internal.getSupportFilePath file_path: +gpt2/+internal/getSupportFilePath.m start_line: 1 end_line: 9 input_parameters: ['fileName'] code_snippet: function filePath = getSupportFilePath(fileName)\n% getSupportFilePath   This function is for converting any differences\n% between the model names presented to the user and the support files\n% URLs.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    fileName (1,1) string\nend scope_id: getSupportFilePath scope_type: script",
  "name: gpt2.internal.legacySupportFilePath file_path: +gpt2/+internal/getSupportFilePath.m start_line: 23 end_line: 37 input_parameters: ['fileName'] code_snippet: \nfunction filePath = legacySupportFilePath(fileName)\n% For releases before matlab.internal.examples.downloadSupportFile,\n% use manual download code. We save to the repo's root directory instead of\n% the userpath.\n% Create directories for the model.\nmodelType = 'gpt2-355M';\nmodelDirectory = fullfile(fileparts(mfilename('fullpath')),'..','..',modelType);\nfilePath = fullfile(modelDirectory,fileName);\niCreateDirectoryIfItDoesNotExist(modelDirectory);\n\niDownloadFileIfItDoesNotExist( ...\n    filePath, ...\n    \"https://ssd.mathworks.com/supportfiles/nnet/data/networks/\"+fileName );\nend scope_id: getSupportFilePath scope_type: script",
  "name: getSupportFilePath file_path: +gpt2/+internal/getSupportFilePath.m start_line: 1 end_line: 55 code_snippet: function filePath = getSupportFilePath(fileName)\n% getSupportFilePath   This function is for converting any differences\n% between the model names presented to the user and the support files\n% URLs.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    fileName (1,1) string\nend\nif ismember(version('-release'),[\"2020a\",\"2020b\"])\n    filePath = legacySupportFilePath(fileName);\n    return\nend\nsd = matlab.internal.examples.utils.getSupportFileDir();\nlocalFileDir = {\"data\",\"networks\"};%#ok\nlocalFile = fullfile(sd,\"nnet\",localFileDir{:},fileName);\nif exist(localFile,'file')~=2\n    disp(\"Downloading \"+fileName+\" to: \"+localFile);\nend\nfileURL = strjoin([localFileDir,fileName],\"/\");\nfilePath = matlab.internal.examples.downloadSupportFile(\"nnet\",fileURL);\nend\n\nfunction filePath = legacySupportFilePath(fileName)\n% For releases before matlab.internal.examples.downloadSupportFile,\n% use manual download code. We save to the repo's root directory instead of\n% the userpath.\n% Create directories for the model.\nmodelType = 'gpt2-355M';\nmodelDirectory = fullfile(fileparts(mfilename('fullpath')),'..','..',modelType);\nfilePath = fullfile(modelDirectory,fileName);\niCreateDirectoryIfItDoesNotExist(modelDirectory);\n\niDownloadFileIfItDoesNotExist( ...\n    filePath, ...\n    \"https://ssd.mathworks.com/supportfiles/nnet/data/networks/\"+fileName );\nend\n\nfunction iCreateDirectoryIfItDoesNotExist(directory)\nif ~exist(directory, 'dir')\n    fprintf('Creating directory ''%s''...\\n', directory);\n    mkdir(directory);\nelse\n    fprintf('Skipped creating directory ''%s'' as it already exists\\n', directory);\nend\nend\n\nfunction iDownloadFileIfItDoesNotExist(destination, source)\nif ~exist(destination, 'file')\n    fprintf('Downloading file ''%s'' ...\\n', destination);\n    websave(destination, source);\nelse\n    fprintf('Skipped downloading file ''%s'' as it already exists\\n', destination);\nend\nend scope_id: getSupportFilePath scope_type: script",
  "name: GPT2Tokenizer file_path: +gpt2/+tokenizer/GPT2Tokenizer.m line_range: 1-13 superclass: handle scope_id: GPT2Tokenizer scope_type: script",
  "name: gpt2.tokenizer.GPT2Tokenizer file_path: +gpt2/+tokenizer/GPT2Tokenizer.m start_line: 69 end_line: 94 input_parameters: ['~', '~'] code_snippet:         function this = GPT2Tokenizer(~, ~)\n            % Read in the vocabulary. The UTF-8 part is really important to\n            % make this work on Windows.\n            vocabFile = gpt2.internal.getSupportFilePath(\"gpt2_vocab.bpe\");\n            fid = fopen(vocabFile, 'r', 'n', 'UTF-8');\n            bpeData = textscan(fid,'%s', 'Delimiter', '\\n');\n            fclose(fid);\n            \n            bpeData = bpeData{1};   % textscan always reads everything in a cell\n            bpeData(1) = [];        % Delete the first line we read in (it's a comment)\n            \n            % Split the bpe data into two columns.\n            this.BPERanks = split(string(bpeData));\n            \n            % Read in the encoding data. The UTF-8 part is really important\n            % to make this work on Windows.\n            encoderFile = gpt2.internal.getSupportFilePath(\"gpt2_encoder.txt\");\n            fid = fopen(encoderFile, 'r', 'n', 'UTF-8');\n            encoderData = textscan(fid,'%s', 'Delimiter', '\\n');\n            fclose(fid);\n            \n            encoderData = encoderData{1};\n            \n            % Set the encoding\n            this.Encoding = string(encoderData);\n        end scope_id: GPT2Tokenizer scope_type: script",
  "name: gpt2.tokenizer.encode file_path: +gpt2/+tokenizer/GPT2Tokenizer.m start_line: 95 end_line: 136 input_parameters: ['this', 'text'] code_snippet:         \n        function numericTokens = encode(this, text)\n            \n            % Note that this function returns tokens with indices that\n            % begin at 1. The Python implementation indexes from 0.\n            \n            % Step 1: Apply regular expression to split text into words.\n            % See the comment for 'TokenizationExpression' for more detail\n            % on what is going on here.\n            [inputTokens, ~] = regexp( ...\n                text, ...\n                this.TokenizationExpression, ...\n                'match', 'split');\n            \n            % Step 2: The incoming text is Unicode. Unicode has a huge set\n            % of characters. We do not want our BPE algorithm to deal with\n            % a huge set of characters, because that will inflate the BPE\n            % vocabulary. So we need to reduce the set of characters. We do\n            % this by converting the Unicode text to the UTF-8 encoding,\n            % and then we replace each UTF-8 byte with another Unicode\n            % character, out of a set of 256 Unicode characters. This will\n            % mean that our original Unicode string which could have\n            % contained any Unicode character will now contain only one of\n            % 256 characters.\n            encodedTokens = cellfun( @(x)unicode2native(x, 'UTF-8'), ...\n                inputTokens, 'UniformOutput', false );\n            encodedTokens = cellfun( @(x)this.ByteEncoder(x+1), ...\n                encodedTokens, 'UniformOutput', false );\n            \n            % Step 3: Do the BPE encoding on a per word basis. Words are\n            % either left as they are, or for rare words we split them into\n            % word fragments.\n            bpeTokens = cellfun(@(x)this.bpe(x), encodedTokens, 'UniformOutput', false);\n            \n            % Step 4: Look up each word or word fragment and replace it\n            % with a number.\n            numericTokens = [];\n            for i = 1:numel(bpeTokens)\n                bpeTokensSplit = split(bpeTokens{i});\n                for j = 1:numel(bpeTokensSplit)\n                    numericTokens = [numericTokens find(this.Encoding == bpeTokensSplit(j))]; %#ok<AGROW>\n                end scope_id: GPT2Tokenizer scope_type: script",
  "name: gpt2.tokenizer.decode file_path: +gpt2/+tokenizer/GPT2Tokenizer.m start_line: 139 end_line: 153 input_parameters: ['this', 'numericTokens'] code_snippet:         \n        function text = decode(this, numericTokens)\n            \n            % Note that this function expects tokens that begin at 1!\n            \n            % Step 1: Turn tokens into text\n            text = join(this.Encoding(numericTokens),'');\n            \n            % Step 2: Replace characters with byte values\n            [~,text] = max( char(text) == this.ByteEncoder' );\n            text = text -1;\n            \n            % Step 3: Decode byte values as UTF-8\n            text = native2unicode(text, 'UTF-8');\n        end scope_id: GPT2Tokenizer scope_type: script",
  "name: gpt2.tokenizer.bpe file_path: +gpt2/+tokenizer/GPT2Tokenizer.m start_line: 157 end_line: 171 input_parameters: ['this', 'token'] code_snippet:         function word = bpe(this, token)\n            if this.Cache.isKey(token)\n                word = this.Cache(token);\n            elseif isempty(token)\n                word = token;\n            else\n                wordFragments = string(num2cell(token));\n                pairs = iGetPairs(wordFragments);\n                \n                while true\n                    matches = [];\n                    for i = 1:numel(pairs)\n                        match = find(sum(pairs{i} == this.BPERanks, 2) == 2);\n                        matches = [matches match]; %#ok<AGROW>\n                    end scope_id: GPT2Tokenizer scope_type: script",
  "name: gpt2.tokenizer.iBytesToUnicode file_path: +gpt2/+tokenizer/GPT2Tokenizer.m start_line: 224 end_line: 237 code_snippet: \nfunction cs = iBytesToUnicode()\n% Note that the third character here is not the letter i! It is the\n% extended Unicode character corresponding to the number 161.\n%cs = ['!':'~' '¡':'¬' '®':'ÿ'];\ncs = char([33:126 161:172 174:255]);\nbs = double(cs);\nn = 0;\nfor b = 0:255\n    if ~any(b == bs)\n        bs = [bs b]; %#ok<AGROW>\n        cs = [cs 256+n]; %#ok<AGROW>\n        n = n + 1;\n    end scope_id: GPT2Tokenizer scope_type: script",
  "name: gpt2.tokenizer.iGetPairs file_path: +gpt2/+tokenizer/GPT2Tokenizer.m start_line: 242 end_line: 251 input_parameters: ['wordFragments'] code_snippet: \nfunction pairs = iGetPairs(wordFragments)\nnumLetters = length(wordFragments);\npairIndices = [1:(numLetters-1); 2:numLetters]';\npairIndices = mat2cell(pairIndices, ones(numLetters-1,1), 2);\npairs = cellfun(@(x)wordFragments(x), pairIndices, ...\n    'UniformOutput', false);\npairs = cellfun(@(x)[string(x(1)) string(x(2))], pairs, ...\n    'UniformOutput', false);\nend scope_id: GPT2Tokenizer scope_type: script",
  "name: GPT2Tokenizer file_path: +gpt2/+tokenizer/GPT2Tokenizer.m start_line: 1 end_line: 251 code_snippet: classdef GPT2Tokenizer < handle\n    % GPT2Tokenizer   Object for encoding text so it can be fed to GPT2\n    \n    properties(SetAccess = private)\n        % Encoding\n        Encoding\n        \n        % BPERanks\n        BPERanks\n        \n        % Cache\n        Cache = containers.Map()\n    end\n    \n    properties(Constant)\n        % TokenizationExpression   Regular expression used for tokenization\n        %\n        %   This is the regular expression used for the first stage of\n        %   tokenization. It was hard-coded by the creators of GPT-2. It\n        %   appears to apply a tokenization rule that can be summarised as\n        %   follows:\n        %\n        %   A token is one of the following things:\n        %\n        %   - An exact string match for 's, 't, 're, 've, 'm, 'll, or 'd.\n        %     This means common contractions in words like don't and you'll\n        %     will get split into their own tokens.\n        %   - Zero or one spaces followed by one or more Unicode letters.\n        %   - Zero or one spaces followed by one or more Unicode numbers.\n        %   - Zero or one spaces followed by one or more things that are\n        %     not whitespace, a Unicode letter or a Unicode number.\n        %   - One or more whitespace characters not followed by a\n        %     non-whitepace character. This is tricky to understand, but\n        %     basically it means that a string with a word preceeded by\n        %     several spaces like '   Hello' will get split into '  ' and\n        %     ' Hello'.\n        %   - One or more whitespace characters.\n        %\n        %   Note that we have had to modify the original expression, which\n        %   is shown below:\n        %\n        %       '''s|''t|''re|''ve|''m|''ll|''d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+'\n        %\n        %   MATLAB's regexp function does not support the \\p flag, so we\n        %   have replaced it with something with equivalent functionality.\n        TokenizationExpression = '''s|''t|''re|''ve|''m|''ll|''d| ?((?![\\d_])\\w)+| ?\\d+| ?(_|[^\\s\\w])+|\\s+(?!\\S)|\\s+';\n        \n        % ByteEncoder   Encodes bytes into a set of 256 Unicode characters\n        %\n        %   The size of the output vocabulary from this encoder determines\n        %   the size of the embedding needed by the GPT-2 transformer\n        %   model. The creators of GPT-2 wanted to keep this at around\n        %   50,000. However, they wanted to be able to encode any Unicode\n        %   string. Unicode has potentially hundreds of thousands of\n        %   characters. So to keep the overall vocabulary low, we go\n        %   through an extra encoding stage:\n        %   \n        %   - The raw Unicode string (which can contain any Unicode\n        %     character) is converted into UTF-8 bytes. Note that UTF-8 is\n        %     a variable length encoding scheme, so each character can get\n        %     mapped to between 1 to 4 bytes.\n        %   - These individual bytes are then mapped to a restricted\n        %     vocabulary of 256 Unicode characters. ByteEncoder defines\n        %     this mapping.\n        ByteEncoder = iBytesToUnicode()\n    end\n    \n    methods\n        function this = GPT2Tokenizer(~, ~)\n            % Read in the vocabulary. The UTF-8 part is really important to\n            % make this work on Windows.\n            vocabFile = gpt2.internal.getSupportFilePath(\"gpt2_vocab.bpe\");\n            fid = fopen(vocabFile, 'r', 'n', 'UTF-8');\n            bpeData = textscan(fid,'%s', 'Delimiter', '\\n');\n            fclose(fid);\n            \n            bpeData = bpeData{1};   % textscan always reads everything in a cell\n            bpeData(1) = [];        % Delete the first line we read in (it's a comment)\n            \n            % Split the bpe data into two columns.\n            this.BPERanks = split(string(bpeData));\n            \n            % Read in the encoding data. The UTF-8 part is really important\n            % to make this work on Windows.\n            encoderFile = gpt2.internal.getSupportFilePath(\"gpt2_encoder.txt\");\n            fid = fopen(encoderFile, 'r', 'n', 'UTF-8');\n            encoderData = textscan(fid,'%s', 'Delimiter', '\\n');\n            fclose(fid);\n            \n            encoderData = encoderData{1};\n            \n            % Set the encoding\n            this.Encoding = string(encoderData);\n        end\n        \n        function numericTokens = encode(this, text)\n            \n            % Note that this function returns tokens with indices that\n            % begin at 1. The Python implementation indexes from 0.\n            \n            % Step 1: Apply regular expression to split text into words.\n            % See the comment for 'TokenizationExpression' for more detail\n            % on what is going on here.\n            [inputTokens, ~] = regexp( ...\n                text, ...\n                this.TokenizationExpression, ...\n                'match', 'split');\n            \n            % Step 2: The incoming text is Unicode. Unicode has a huge set\n            % of characters. We do not want our BPE algorithm to deal with\n            % a huge set of characters, because that will inflate the BPE\n            % vocabulary. So we need to reduce the set of characters. We do\n            % this by converting the Unicode text to the UTF-8 encoding,\n            % and then we replace each UTF-8 byte with another Unicode\n            % character, out of a set of 256 Unicode characters. This will\n            % mean that our original Unicode string which could have\n            % contained any Unicode character will now contain only one of\n            % 256 characters.\n            encodedTokens = cellfun( @(x)unicode2native(x, 'UTF-8'), ...\n                inputTokens, 'UniformOutput', false );\n            encodedTokens = cellfun( @(x)this.ByteEncoder(x+1), ...\n                encodedTokens, 'UniformOutput', false );\n            \n            % Step 3: Do the BPE encoding on a per word basis. Words are\n            % either left as they are, or for rare words we split them into\n            % word fragments.\n            bpeTokens = cellfun(@(x)this.bpe(x), encodedTokens, 'UniformOutput', false);\n            \n            % Step 4: Look up each word or word fragment and replace it\n            % with a number.\n            numericTokens = [];\n            for i = 1:numel(bpeTokens)\n                bpeTokensSplit = split(bpeTokens{i});\n                for j = 1:numel(bpeTokensSplit)\n                    numericTokens = [numericTokens find(this.Encoding == bpeTokensSplit(j))]; %#ok<AGROW>\n                end\n            end\n        end\n        \n        function text = decode(this, numericTokens)\n            \n            % Note that this function expects tokens that begin at 1!\n            \n            % Step 1: Turn tokens into text\n            text = join(this.Encoding(numericTokens),'');\n            \n            % Step 2: Replace characters with byte values\n            [~,text] = max( char(text) == this.ByteEncoder' );\n            text = text -1;\n            \n            % Step 3: Decode byte values as UTF-8\n            text = native2unicode(text, 'UTF-8');\n        end\n    end\n    \n    methods(Access = private)\n        function word = bpe(this, token)\n            if this.Cache.isKey(token)\n                word = this.Cache(token);\n            elseif isempty(token)\n                word = token;\n            else\n                wordFragments = string(num2cell(token));\n                pairs = iGetPairs(wordFragments);\n                \n                while true\n                    matches = [];\n                    for i = 1:numel(pairs)\n                        match = find(sum(pairs{i} == this.BPERanks, 2) == 2);\n                        matches = [matches match]; %#ok<AGROW>\n                    end\n                    minIndex = min(matches);\n                    if isempty(minIndex)\n                        break;\n                    end\n                    bigram = this.BPERanks(minIndex,:);\n                    \n                    first = bigram(1);\n                    second = bigram(2);\n                    newWordFragments = [];\n                    i = 1;\n                    while i < length(wordFragments)+1\n                        j = find( ...\n                            wordFragments == first & ...\n                            [zeros(1,(i-1)) ones(1,length(wordFragments)-i+1)]);\n                        if isempty(j)\n                            newWordFragments = [newWordFragments wordFragments(i:end)]; %#ok<AGROW>\n                            break\n                        else\n                            newWordFragments = [newWordFragments wordFragments(i:(j(1)-1))]; %#ok<AGROW>\n                            i = j(1);\n                        end\n                        \n                        if wordFragments(i) == first && ...\n                                i < length(wordFragments) && ...\n                                wordFragments(i+1) == second\n                            newWordFragments = [newWordFragments first+second]; %#ok<AGROW>\n                            i = i + 2;\n                        else\n                            newWordFragments = [newWordFragments wordFragments(i)]; %#ok<AGROW>\n                            i = i + 1;\n                        end\n                    end\n                    \n                    % We have a new word because we have merged some of the\n                    % word fragments. If there is only one element in\n                    % 'wordFragments', we have merges all of the fragments,\n                    % and can stop now, so we break. Otherwise, we generate\n                    % pairs again, and start the process again.\n                    wordFragments = newWordFragments;\n                    if numel(wordFragments) == 1\n                        break;\n                    else\n                        pairs = iGetPairs(wordFragments);\n                    end\n                end\n                \n                word = join(wordFragments, ' ');\n                this.Cache(token) = word;\n            end\n        end\n    end\nend\n\nfunction cs = iBytesToUnicode()\n% Note that the third character here is not the letter i! It is the\n% extended Unicode character corresponding to the number 161.\n%cs = ['!':'~' '¡':'¬' '®':'ÿ'];\ncs = char([33:126 161:172 174:255]);\nbs = double(cs);\nn = 0;\nfor b = 0:255\n    if ~any(b == bs)\n        bs = [bs b]; %#ok<AGROW>\n        cs = [cs 256+n]; %#ok<AGROW>\n        n = n + 1;\n    end\nend\n[~,sortedIndices] = sort(bs);\ncs = cs(sortedIndices);\nend\n\nfunction pairs = iGetPairs(wordFragments)\nnumLetters = length(wordFragments);\npairIndices = [1:(numLetters-1); 2:numLetters]';\npairIndices = mat2cell(pairIndices, ones(numLetters-1,1), 2);\npairs = cellfun(@(x)wordFragments(x), pairIndices, ...\n    'UniformOutput', false);\npairs = cellfun(@(x)[string(x(1)) string(x(2))], pairs, ...\n    'UniformOutput', false);\nend scope_id: GPT2Tokenizer scope_type: script",
  "name: transformer.layer.gelu file_path: +transformer/+layer/gelu.m start_line: 1 end_line: 14 input_parameters: ['X'] code_snippet: function Z = gelu(X)\n% gelu   GELU activation function\n%\n%   Z = gelu(X) computes the GELU activation function on X. The GELU\n%   activation function is described in [1]. It is an element-wise\n%   activation function, so the input and output are the same size.\n%\n%   References:\n%\n%   [1] Dan Hendrycks, Kevin Gimpel, \"Gaussian Error Linear Units (GELUs)\",\n%       https://arxiv.org/abs/1606.08415\n\nZ = 0.5*X.*( 1 + tanh( sqrt(2/pi)*(X+0.044715*(X.^3)) ) );\nend scope_id: gelu scope_type: script",
  "name: gelu file_path: +transformer/+layer/gelu.m start_line: 1 end_line: 14 code_snippet: function Z = gelu(X)\n% gelu   GELU activation function\n%\n%   Z = gelu(X) computes the GELU activation function on X. The GELU\n%   activation function is described in [1]. It is an element-wise\n%   activation function, so the input and output are the same size.\n%\n%   References:\n%\n%   [1] Dan Hendrycks, Kevin Gimpel, \"Gaussian Error Linear Units (GELUs)\",\n%       https://arxiv.org/abs/1606.08415\n\nZ = 0.5*X.*( 1 + tanh( sqrt(2/pi)*(X+0.044715*(X.^3)) ) );\nend scope_id: gelu scope_type: script",
  "name: transformer.layer.attention file_path: +transformer/+layer/attention.m start_line: 1 end_line: 69 input_parameters: ['X', 'past', 'weights', 'hyperParameters', 'nvp'] code_snippet: function [A, present] = attention(X, past, weights, hyperParameters, nvp)\n% attention   Full Multi-head Attention\n%\n%   [A, present] = attention(X, past, weights, hyperParameters) computes a\n%   multi-head attention block on X as outlined in Section 3.2.2 and Figure\n%   2 in [1]. See below for details of inputs and outputs.\n%\n%   Inputs:\n%       X               - A (numFeatures*numHeads)-by-numInputSubwords-by-numObs\n%                         input array.\n%       past            - A numFeatures-by-numPastSubwords-by-numHeads-by-numObs-by-2\n%                         array. This contains the 'keys' and 'values' for\n%                         past subwords. These are needed to predict future\n%                         outputs in an autoregressive manner. 'keys' are\n%                         stored in past(:,:,:,:,1) and 'values' are stored\n%                         in past(:,:,:,:,2).\n%       weights         - The weights for the full multi-head attention\n%                         block stored in a struct. This includes:\n%                           - attn_c_attn_w_0: A weight matrix for the\n%                             first fully connected layer.\n%                           - attn_c_attn_b_0: A bias vector for the first\n%                             fully connected layer.\n%                           - attn_c_proj_w_0: A weight matrix for the\n%                             final fully connected layer.\n%                           - attn_c_proj_b_0: A bias vector for the final\n%                             fully connected layer.\n%       numHeads        - The number of attention heads. This is a\n%                         hyper-parameter.\n%\n%   Outputs:\n%       A               - A (numFeatures*numHeads)-by-numInputSubwords-by-numObs\n%                         output array.\n%       present         - A numFeatures-by-numAllSubwords-by-numHeads-by-numObs-by-2\n%                         array. This contains the 'keys' and 'values' that\n%                         are created from inputs. These need to passed\n%                         back in as the 'past' input if we want to predict\n%                         future outputs in an autoregressive manner. 'keys'\n%                         are stored in present(:,:,:,:,1) and 'values' are\n%                         stored in present(:,:,:,:,2).\n%\n%   [A, present] = attention(X, past, weights, hyperParameters, 'PARAM1',\n%   VAL1, 'PARAM2', VAL2, ...) specifies the optional parameter name/value\n%   pairs:\n%\n%     'CausalMask'  - A scalar logical to turn causal masking on or off. Causal\n%                     masking prevents tokens at time T attending to tokens\n%                     at time S<T. The default is true.\n%\n%     'Dropout'     - The dropout probability for the attention\n%                     probabilities. The default is 0.\n%\n%     'InputMask'   - A logical mask to mask attending to particular\n%                     tokens, for example padding tokens. The default is\n%                     [], interpreted as not applying any masking.\n%\n%   References:\n%\n%   [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n%       Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, \"Attention\n%       Is All You Need\", https://arxiv.org/abs/1706.03762\narguments\n    X\n    past\n    weights\n    hyperParameters\n    nvp.CausalMask (1,1) logical = true\n    nvp.Dropout (1,1) double {mustBeNonnegative,mustBeLessThanOrEqual(nvp.Dropout,1)} = 0\n    nvp.InputMask = []\nend scope_id: attention scope_type: script",
  "name: transformer.layer.iSplitHeads file_path: +transformer/+layer/attention.m start_line: 109 end_line: 119 input_parameters: ['X', 'splitSize', 'numHeads'] code_snippet: \nfunction Z = iSplitHeads(X, splitSize, numHeads)\n% We permute the data to put the dimension for the heads last, so that we\n% can use batched matrix multiplication to compute attention for all of the\n% heads at once.\n%\n% X     - A (numFeatures*numHeads)-by-numSubwords-by-numObs array.\n% Z     - A numFeatures-by-numSubwords-by-numHeads-by-numObs array.\nX = reshape(X, splitSize/numHeads, numHeads, [], size(X,3));\nZ = permute(X,[1 3 2 4]);\nend scope_id: attention scope_type: script",
  "name: transformer.layer.iMergeHeads file_path: +transformer/+layer/attention.m start_line: 120 end_line: 126 input_parameters: ['X'] code_snippet: \nfunction Z = iMergeHeads(X)\n% X     - A numFeatures-by-numSubwords-by-numHeads-by-numObs array.\n% Z     - A (numFeatures*numHeads)-by-numSubwords-by-numObs array.\nX = permute(X, [1 3 2 4]);\nZ = reshape(X, size(X,1)*size(X,2), [], size(X,4));\nend scope_id: attention scope_type: script",
  "name: attention file_path: +transformer/+layer/attention.m start_line: 1 end_line: 126 code_snippet: function [A, present] = attention(X, past, weights, hyperParameters, nvp)\n% attention   Full Multi-head Attention\n%\n%   [A, present] = attention(X, past, weights, hyperParameters) computes a\n%   multi-head attention block on X as outlined in Section 3.2.2 and Figure\n%   2 in [1]. See below for details of inputs and outputs.\n%\n%   Inputs:\n%       X               - A (numFeatures*numHeads)-by-numInputSubwords-by-numObs\n%                         input array.\n%       past            - A numFeatures-by-numPastSubwords-by-numHeads-by-numObs-by-2\n%                         array. This contains the 'keys' and 'values' for\n%                         past subwords. These are needed to predict future\n%                         outputs in an autoregressive manner. 'keys' are\n%                         stored in past(:,:,:,:,1) and 'values' are stored\n%                         in past(:,:,:,:,2).\n%       weights         - The weights for the full multi-head attention\n%                         block stored in a struct. This includes:\n%                           - attn_c_attn_w_0: A weight matrix for the\n%                             first fully connected layer.\n%                           - attn_c_attn_b_0: A bias vector for the first\n%                             fully connected layer.\n%                           - attn_c_proj_w_0: A weight matrix for the\n%                             final fully connected layer.\n%                           - attn_c_proj_b_0: A bias vector for the final\n%                             fully connected layer.\n%       numHeads        - The number of attention heads. This is a\n%                         hyper-parameter.\n%\n%   Outputs:\n%       A               - A (numFeatures*numHeads)-by-numInputSubwords-by-numObs\n%                         output array.\n%       present         - A numFeatures-by-numAllSubwords-by-numHeads-by-numObs-by-2\n%                         array. This contains the 'keys' and 'values' that\n%                         are created from inputs. These need to passed\n%                         back in as the 'past' input if we want to predict\n%                         future outputs in an autoregressive manner. 'keys'\n%                         are stored in present(:,:,:,:,1) and 'values' are\n%                         stored in present(:,:,:,:,2).\n%\n%   [A, present] = attention(X, past, weights, hyperParameters, 'PARAM1',\n%   VAL1, 'PARAM2', VAL2, ...) specifies the optional parameter name/value\n%   pairs:\n%\n%     'CausalMask'  - A scalar logical to turn causal masking on or off. Causal\n%                     masking prevents tokens at time T attending to tokens\n%                     at time S<T. The default is true.\n%\n%     'Dropout'     - The dropout probability for the attention\n%                     probabilities. The default is 0.\n%\n%     'InputMask'   - A logical mask to mask attending to particular\n%                     tokens, for example padding tokens. The default is\n%                     [], interpreted as not applying any masking.\n%\n%   References:\n%\n%   [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n%       Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, \"Attention\n%       Is All You Need\", https://arxiv.org/abs/1706.03762\narguments\n    X\n    past\n    weights\n    hyperParameters\n    nvp.CausalMask (1,1) logical = true\n    nvp.Dropout (1,1) double {mustBeNonnegative,mustBeLessThanOrEqual(nvp.Dropout,1)} = 0\n    nvp.InputMask = []\nend\n\n% Use a fully connected layer to generate queries, keys and values from the\n% input.\nC = transformer.layer.convolution1d( X, ...\n    weights.attn_c_attn_w_0, ...\n    weights.attn_c_attn_b_0 );\n\n% Split the results into Q (Query), K (Keys) and V (Values).\nsplitSize = size(C,1)/3;\nQ = C(1:splitSize,:,:);\nK = C((splitSize+1):(2*splitSize),:,:);\nV = C((2*splitSize+1):(3*splitSize),:,:);\n\n% Split heads\nQ = iSplitHeads(Q, splitSize, hyperParameters.NumHeads);\nK = iSplitHeads(K, splitSize, hyperParameters.NumHeads);\nV = iSplitHeads(V, splitSize, hyperParameters.NumHeads);\n\n% Use the past\nif ~isempty(past)\n    PK = past(:,:,:,:,1);\n    PV = past(:,:,:,:,2);\n    K = cat(2,PK,K);\n    V = cat(2,PV,V);\nend\n\n% Set present. Note that this is done differently from the original\n% implementation which sets the value of present before the previous if\n% statement\npresent = cat(5,K,V);\n\nA = transformer.layer.multiheadAttention(Q,K,V,'CausalMask',nvp.CausalMask,'Dropout',nvp.Dropout,'InputMask',nvp.InputMask);\n\nA = iMergeHeads(A);\n\nA = transformer.layer.convolution1d( A, ...\n    weights.attn_c_proj_w_0, ...\n    weights.attn_c_proj_b_0 );\nend\n\nfunction Z = iSplitHeads(X, splitSize, numHeads)\n% We permute the data to put the dimension for the heads last, so that we\n% can use batched matrix multiplication to compute attention for all of the\n% heads at once.\n%\n% X     - A (numFeatures*numHeads)-by-numSubwords-by-numObs array.\n% Z     - A numFeatures-by-numSubwords-by-numHeads-by-numObs array.\nX = reshape(X, splitSize/numHeads, numHeads, [], size(X,3));\nZ = permute(X,[1 3 2 4]);\nend\n\nfunction Z = iMergeHeads(X)\n% X     - A numFeatures-by-numSubwords-by-numHeads-by-numObs array.\n% Z     - A (numFeatures*numHeads)-by-numSubwords-by-numObs array.\nX = permute(X, [1 3 2 4]);\nZ = reshape(X, size(X,1)*size(X,2), [], size(X,4));\nend scope_id: attention scope_type: script",
  "name: transformer.layer.multiheadAttention file_path: +transformer/+layer/multiheadAttention.m start_line: 1 end_line: 46 input_parameters: ['Q', 'K', 'V', 'nvp'] code_snippet: function A = multiheadAttention(Q, K, V,nvp)\n% multiheadAttention   Multi-head Attention\n%\n%   A = multiheadAttention(Q, K, V) computes scaled dot product attention\n%   for multiple attention heads as outlined in [1] (see Section 3.2.1 and\n%   Figure 2). Note that this function computes the attention for multiple\n%   attention heads at once for efficiency. Q is a collection of query\n%   matrices, K is a collection of key matrices and V is a collection of\n%   value matrices. The output A is a collection of attention matrices. See\n%   below for details.\n%\n%   Inputs:\n%       Q   - numFeatures-by-numInputSubWords-by-numHeads-by-numObs array of queries.\n%       K   - numFeatures-by-numAllSubWords-by-numHeads-by-numObs array of keys.\n%       V   - numFeatures-by-numAllSubWords-by-numHeads-by-numObs array of values.\n%\n%   Outputs:\n%       A   - numFeatures-by-numInputSubWords-by-numHeads-by-numObs array of attention matrices.\n%\n%   A = multiheadAttention(Q, K, V, 'PARAM1', VAL1, 'PARAM2', VAL2, ...)\n%   specifies the optional parameter name/value pairs:\n%\n%     'CausalMask'  - A scalar logical to turn causal masking on or off. Causal\n%                     masking prevents tokens at time T attending to tokens\n%                     at time S<T. The default is true.\n%\n%     'Dropout'     - The dropout probability for the attention\n%                     probabilities. The default is 0.\n%\n%     'InputMask'   - A logical mask to mask attending to particular\n%                     tokens, for example padding tokens. The default is\n%                     [], interpreted as not applying any masking.\n%\n%   References:\n%\n%   [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n%       Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, \"Attention\n%       Is All You Need\", https://arxiv.org/abs/1706.03762\narguments\n    Q\n    K\n    V\n    nvp.CausalMask (1,1) logical = true\n    nvp.Dropout (1,1) double {mustBeNonnegative,mustBeLessThanOrEqual(nvp.Dropout,1)} = 0\n    nvp.InputMask = []\nend scope_id: multiheadAttention scope_type: script",
  "name: multiheadAttention file_path: +transformer/+layer/multiheadAttention.m start_line: 1 end_line: 71 code_snippet: function A = multiheadAttention(Q, K, V,nvp)\n% multiheadAttention   Multi-head Attention\n%\n%   A = multiheadAttention(Q, K, V) computes scaled dot product attention\n%   for multiple attention heads as outlined in [1] (see Section 3.2.1 and\n%   Figure 2). Note that this function computes the attention for multiple\n%   attention heads at once for efficiency. Q is a collection of query\n%   matrices, K is a collection of key matrices and V is a collection of\n%   value matrices. The output A is a collection of attention matrices. See\n%   below for details.\n%\n%   Inputs:\n%       Q   - numFeatures-by-numInputSubWords-by-numHeads-by-numObs array of queries.\n%       K   - numFeatures-by-numAllSubWords-by-numHeads-by-numObs array of keys.\n%       V   - numFeatures-by-numAllSubWords-by-numHeads-by-numObs array of values.\n%\n%   Outputs:\n%       A   - numFeatures-by-numInputSubWords-by-numHeads-by-numObs array of attention matrices.\n%\n%   A = multiheadAttention(Q, K, V, 'PARAM1', VAL1, 'PARAM2', VAL2, ...)\n%   specifies the optional parameter name/value pairs:\n%\n%     'CausalMask'  - A scalar logical to turn causal masking on or off. Causal\n%                     masking prevents tokens at time T attending to tokens\n%                     at time S<T. The default is true.\n%\n%     'Dropout'     - The dropout probability for the attention\n%                     probabilities. The default is 0.\n%\n%     'InputMask'   - A logical mask to mask attending to particular\n%                     tokens, for example padding tokens. The default is\n%                     [], interpreted as not applying any masking.\n%\n%   References:\n%\n%   [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n%       Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, \"Attention\n%       Is All You Need\", https://arxiv.org/abs/1706.03762\narguments\n    Q\n    K\n    V\n    nvp.CausalMask (1,1) logical = true\n    nvp.Dropout (1,1) double {mustBeNonnegative,mustBeLessThanOrEqual(nvp.Dropout,1)} = 0\n    nvp.InputMask = []\nend\n\n% We compute attention weights by taking the product between Q and K\n% matrices. W is numAllSubWords-by-numInputSubWords-by-numHeads. Each\n% element of W is the dot product of a query vector from Q and a key vector\n% from K.\nW = dlmtimes(permute(K, [2 1 3 4]), Q);\n\n% Divide by square root of d\nW = W./sqrt(size(Q,1));\n\n% Apply masking\nW = transformer.layer.maskAttentionWeights(W,'CausalMask',nvp.CausalMask,'InputMask',nvp.InputMask);\n\n% Apply softmax\nW = softmax(W, 'DataFormat', 'CTUB');\n\n% Apply dropout\nW = transformer.layer.dropout(W,nvp.Dropout);\n\n% We compute the attention by taking products between the attention weights\n% W and V. A is numFeatures-by-numInputSubWords-by-numHeads. One\n% interpretation of A is that it is the expected value of V according to\n% the probability distribution given by W.\nA = dlmtimes(V, W);\nend scope_id: multiheadAttention scope_type: script",
  "name: transformer.layer.normalization file_path: +transformer/+layer/normalization.m start_line: 1 end_line: 29 input_parameters: ['X', 'g', 'b'] code_snippet: function Z = normalization(X, g, b)\n% normalization   Layer Normalization\n%\n%   Z = normalization(X, g, b) applies layer normalization to the input X.\n%   Layer normzalization is described in [1].\n%\n%   Inputs:\n%       X - A numFeatures-by-numInputSubwords-by-numObs input array.\n%       g - A numFeatures-by-1 weight vector.\n%       b - A numFeatures-by-1 bias vector.\n%\n%   Outputs:\n%       Z - A numFeatures-by-numInputSubwords-by-numObs output array.\n%\n%   References:\n%\n%   [1] Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, \"Layer\n%       Normalization\", https://arxiv.org/abs/1607.06450\n\nnormalizationDimension = 1;\n\nepsilon = single(1e-5);\n\nU = mean(X, normalizationDimension);\nS = mean((X-U).^2, normalizationDimension);\nX = (X-U) ./ sqrt(S + epsilon);\nZ = g.*X + b;\n\nend scope_id: normalization scope_type: script",
  "name: normalization file_path: +transformer/+layer/normalization.m start_line: 1 end_line: 29 code_snippet: function Z = normalization(X, g, b)\n% normalization   Layer Normalization\n%\n%   Z = normalization(X, g, b) applies layer normalization to the input X.\n%   Layer normzalization is described in [1].\n%\n%   Inputs:\n%       X - A numFeatures-by-numInputSubwords-by-numObs input array.\n%       g - A numFeatures-by-1 weight vector.\n%       b - A numFeatures-by-1 bias vector.\n%\n%   Outputs:\n%       Z - A numFeatures-by-numInputSubwords-by-numObs output array.\n%\n%   References:\n%\n%   [1] Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton, \"Layer\n%       Normalization\", https://arxiv.org/abs/1607.06450\n\nnormalizationDimension = 1;\n\nepsilon = single(1e-5);\n\nU = mean(X, normalizationDimension);\nS = mean((X-U).^2, normalizationDimension);\nX = (X-U) ./ sqrt(S + epsilon);\nZ = g.*X + b;\n\nend scope_id: normalization scope_type: script",
  "name: transformer.layer.dropout file_path: +transformer/+layer/dropout.m start_line: 1 end_line: 6 input_parameters: ['z', 'p'] code_snippet: function z = dropout(z,p)\n% z = dropout(x,p)  Applies inverted dropout with probability p to input x.\narguments\n    z\n    p (1,1) double {mustBeNonnegative, mustBeLessThan(p,1)}\nend scope_id: dropout scope_type: script",
  "name: dropout file_path: +transformer/+layer/dropout.m start_line: 1 end_line: 13 code_snippet: function z = dropout(z,p)\n% z = dropout(x,p)  Applies inverted dropout with probability p to input x.\narguments\n    z\n    p (1,1) double {mustBeNonnegative, mustBeLessThan(p,1)}\nend\n\n% Copyright 2020 The MathWorks, Inc.\n\nps = rand(size(z),'like',z);\ndropoutMask = (1-(ps<p)) ./ (1-p);\nz = z.*dropoutMask;\nend scope_id: dropout scope_type: script",
  "name: transformer.layer.maskAttentionWeights file_path: +transformer/+layer/maskAttentionWeights.m start_line: 1 end_line: 32 input_parameters: ['W', 'nvp'] code_snippet: function W = maskAttentionWeights(W,nvp)\n% maskAttentionWeights   Function for masking attention weights\n%\n%   W = maskAttentionWeights(W) applies masking to a set of attention\n%   matrices W (prior to the application of the softmax function). This\n%   prevents subwords from attending to other subwords that come AFTER\n%   them, which is necessary when we are using scaled dot product attention\n%   for a 'decoder' transformer that predicts future outputs.\n%\n%   Inputs:\n%       W   - A numAllSubwords-by-numInputSubwords-by-numHeads array.\n%\n%   Outputs:\n%       W   - A numAllSubwords-by-numInputSubwords-by-numHeads array.\n%\n%   W = maskAttentionWeights(W, 'PARAM1', VAL1, 'PARAM2', VAL2, ...)\n%   specifies the optional parameter name/value pairs:\n%\n%      'CausalMask' - A scalar logical to turn causal masking on or off. \n%                     Causal masking prevents tokens at time T attending to\n%                     tokens at time S<T. The default is true.\n% \n%      'InputMask'  - A logical mask to mask attending to particular\n%                     tokens, for example padding tokens. The default is\n%                     [], interpreted as not applying any masking.\n\n% Copyright 2020-2021 The MathWorks, Inc.\narguments\n    W\n    nvp.CausalMask (1,1) logical = true\n    nvp.InputMask = []\nend scope_id: maskAttentionWeights scope_type: script",
  "name: maskAttentionWeights file_path: +transformer/+layer/maskAttentionWeights.m start_line: 1 end_line: 61 code_snippet: function W = maskAttentionWeights(W,nvp)\n% maskAttentionWeights   Function for masking attention weights\n%\n%   W = maskAttentionWeights(W) applies masking to a set of attention\n%   matrices W (prior to the application of the softmax function). This\n%   prevents subwords from attending to other subwords that come AFTER\n%   them, which is necessary when we are using scaled dot product attention\n%   for a 'decoder' transformer that predicts future outputs.\n%\n%   Inputs:\n%       W   - A numAllSubwords-by-numInputSubwords-by-numHeads array.\n%\n%   Outputs:\n%       W   - A numAllSubwords-by-numInputSubwords-by-numHeads array.\n%\n%   W = maskAttentionWeights(W, 'PARAM1', VAL1, 'PARAM2', VAL2, ...)\n%   specifies the optional parameter name/value pairs:\n%\n%      'CausalMask' - A scalar logical to turn causal masking on or off. \n%                     Causal masking prevents tokens at time T attending to\n%                     tokens at time S<T. The default is true.\n% \n%      'InputMask'  - A logical mask to mask attending to particular\n%                     tokens, for example padding tokens. The default is\n%                     [], interpreted as not applying any masking.\n\n% Copyright 2020-2021 The MathWorks, Inc.\narguments\n    W\n    nvp.CausalMask (1,1) logical = true\n    nvp.InputMask = []\nend\n\nif nvp.CausalMask\n    numAllSubwords = size(W,1);\n    numInputSubwords = size(W,2);\n    numPreviousSubwords = numInputSubwords-numAllSubwords;\n\n    % The mask is numAllSubwords-by-numInputSubwords. Input subwords should not\n    % attend to other input subwords that come after them. The matrix will have\n    % zeroes in positions we want to mask out, and ones in positions we want to\n    % keep.\n    mask = triu( ...\n        ones([numAllSubwords numInputSubwords],'like', extractdata(W)), ...\n        numPreviousSubwords );\n\n    % We want to make sure that when softmax is applied to the matrix W, the\n    % probability for the things we want to mask out is zero. We do this by\n    % setting those entries to a large negative value.\n    W = W.*mask - (1e10)*(1-mask);\nend\n\nif ~isempty(nvp.InputMask)\n    % mask is in CTB format, permute to attention shape - TT(head)B\n    mask = permute(nvp.InputMask,[2,1,4,3]);\n    % expand mask to W size - T and B dimensions should be good, just\n    % repeat over extra T and heads dimension.\n    mask = repmat(mask,[1,size(W,2),size(W,3),1]);\n    W = W-1e4.*(~mask);\nend\nend scope_id: maskAttentionWeights scope_type: script",
  "name: transformer.layer.convolution1d file_path: +transformer/+layer/convolution1d.m start_line: 1 end_line: 18 input_parameters: ['X', 'W', 'b'] code_snippet: function Z = convolution1d(X, W, b)\n% convolution1d   A fully connected layer\n%\n%   Z = convolution1d(X, W, b) applies a fully connected layer. We call it\n%   1-D convolution because this is what it's called in the original GPT-2\n%   repo.\n%\n%   Inputs:\n%       X   - A numInputFeatures-by-numInputSubwords array.\n%       W   - A numOutputFeatures-by-numInputFeatures weight matrix.\n%       b   - A numOutputFeatures-by-1 bias vector.\n%\n%   Output:\n%       Z   - A numOutputFeatures-by-numInputSubwords array.\n\nZ = dlmtimes(W,X) + b;\n\nend scope_id: convolution1d scope_type: script",
  "name: convolution1d file_path: +transformer/+layer/convolution1d.m start_line: 1 end_line: 18 code_snippet: function Z = convolution1d(X, W, b)\n% convolution1d   A fully connected layer\n%\n%   Z = convolution1d(X, W, b) applies a fully connected layer. We call it\n%   1-D convolution because this is what it's called in the original GPT-2\n%   repo.\n%\n%   Inputs:\n%       X   - A numInputFeatures-by-numInputSubwords array.\n%       W   - A numOutputFeatures-by-numInputFeatures weight matrix.\n%       b   - A numOutputFeatures-by-1 bias vector.\n%\n%   Output:\n%       Z   - A numOutputFeatures-by-numInputSubwords array.\n\nZ = dlmtimes(W,X) + b;\n\nend scope_id: convolution1d scope_type: script",
  "name: transformer.layer.multiLayerPerceptron file_path: +transformer/+layer/multiLayerPerceptron.m start_line: 1 end_line: 31 input_parameters: ['X', 'weights'] code_snippet: function Z = multiLayerPerceptron(X, weights)\n% multiLayerPerceptron   A multi-layer perceptron\n%\n%   Z = multiLayerPerceptron(X, weights) applies a multi-layer perceptron\n%   to the input array X.\n%\n%   Inputs:\n%       X           - A numFeatures-by-numInputSubwords input array.\n%       weights     - The weights for the multi-layer perceptron stored in\n%                     a struct. This includes:\n%                       - mlp_c_fc_w_0: A weight matrix for the first fully\n%                         connected layer.\n%                       - mlp_c_fc_b_0: A bias vector for the first fully\n%                         connected layer.\n%                       - mlp_c_proj_w_0: A weight matrix for the second\n%                         fully connected layer.\n%                       - mlp_c_proj_b_0: A bias vector for the second\n%                         fully connected layer.\n%\n%   Outputs:\n%       Z           - A numFeatures-by-numInputSubwords output array.\n\nZ = transformer.layer.convolution1d( X, ...\n    weights.mlp_c_fc_w_0, ...\n    weights.mlp_c_fc_b_0 );\nZ = transformer.layer.gelu(Z);\nZ = transformer.layer.convolution1d( Z, ...\n    weights.mlp_c_proj_w_0, ...\n    weights.mlp_c_proj_b_0 );\n\nend scope_id: multiLayerPerceptron scope_type: script",
  "name: multiLayerPerceptron file_path: +transformer/+layer/multiLayerPerceptron.m start_line: 1 end_line: 31 code_snippet: function Z = multiLayerPerceptron(X, weights)\n% multiLayerPerceptron   A multi-layer perceptron\n%\n%   Z = multiLayerPerceptron(X, weights) applies a multi-layer perceptron\n%   to the input array X.\n%\n%   Inputs:\n%       X           - A numFeatures-by-numInputSubwords input array.\n%       weights     - The weights for the multi-layer perceptron stored in\n%                     a struct. This includes:\n%                       - mlp_c_fc_w_0: A weight matrix for the first fully\n%                         connected layer.\n%                       - mlp_c_fc_b_0: A bias vector for the first fully\n%                         connected layer.\n%                       - mlp_c_proj_w_0: A weight matrix for the second\n%                         fully connected layer.\n%                       - mlp_c_proj_b_0: A bias vector for the second\n%                         fully connected layer.\n%\n%   Outputs:\n%       Z           - A numFeatures-by-numInputSubwords output array.\n\nZ = transformer.layer.convolution1d( X, ...\n    weights.mlp_c_fc_w_0, ...\n    weights.mlp_c_fc_b_0 );\nZ = transformer.layer.gelu(Z);\nZ = transformer.layer.convolution1d( Z, ...\n    weights.mlp_c_proj_w_0, ...\n    weights.mlp_c_proj_b_0 );\n\nend scope_id: multiLayerPerceptron scope_type: script",
  "name: finbert.internal.getSupportFilePath file_path: +finbert/+internal/getSupportFilePath.m start_line: 1 end_line: 10 input_parameters: ['modelName', 'fileName'] code_snippet: function filePath = getSupportFilePath(modelName,fileName)\n% getSupportFilePath   This function is for converting any differences\n% between the model names presented to the user and the support files\n% URLs.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string\n    fileName (1,1) string\nend scope_id: getSupportFilePath scope_type: script",
  "name: getSupportFilePath file_path: +finbert/+internal/getSupportFilePath.m start_line: 1 end_line: 19 code_snippet: function filePath = getSupportFilePath(modelName,fileName)\n% getSupportFilePath   This function is for converting any differences\n% between the model names presented to the user and the support files\n% URLs.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    modelName (1,1) string\n    fileName (1,1) string\nend\ndirectory = finbert.internal.convertModelNameToDirectories(modelName);\nsd = matlab.internal.examples.utils.getSupportFileDir();\nlocalFile = fullfile(sd,\"nnet\",directory{:},fileName);\nif exist(localFile,'file')~=2\n    disp(\"Downloading \"+fileName+\" to: \"+localFile);\nend\nfileURL = strjoin([directory,fileName],\"/\");\nfilePath = matlab.internal.examples.downloadSupportFile(\"nnet\",fileURL);\nend scope_id: getSupportFilePath scope_type: script",
  "name: finbert.internal.convertModelNameToDirectories file_path: +finbert/+internal/convertModelNameToDirectories.m start_line: 1 end_line: 8 input_parameters: ['name'] code_snippet: function path = convertModelNameToDirectories(name)\n% convertModelNameToDirectories   Converts the user facing model name to\n% the directory name used by support files.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    name (1,1) string\nend scope_id: convertModelNameToDirectories scope_type: script",
  "name: convertModelNameToDirectories file_path: +finbert/+internal/convertModelNameToDirectories.m start_line: 1 end_line: 10 code_snippet: function path = convertModelNameToDirectories(name)\n% convertModelNameToDirectories   Converts the user facing model name to\n% the directory name used by support files.\n\n% Copyright 2021 The MathWorks, Inc.\narguments\n    name (1,1) string\nend\npath = {\"data\",\"networks\",\"finbert\",name};\nend scope_id: convertModelNameToDirectories scope_type: script"
]