import asyncio
import numpy as np
from neo4j_graphrag.embeddings.ollama import OllamaEmbeddings
from neo4j_graphrag.llm.ollama_llm import OllamaLLM

def cosine_similarity(vec1, vec2):
    """Calculates the cosine similarity between two vectors."""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
        
    return dot_product / (norm_vec1 * norm_vec2)

async def main():
    """
    A simple, standalone script to test the Ollama embedding and LLM models.
    """
    print("--- Testing Ollama Models ---")

    # 1. Test Embedding Model with Similarity Calculation
    try:
        print("\n[1] Initializing embedding model: dengcao/Qwen3-Embedding-8B:Q5_K_M...")
        embedder = OllamaEmbeddings(model="dengcao/Qwen3-Embedding-8B:Q5_K_M")
        
        # Load documents from the file generated by the KG builder
        import json
        print("    Loading documents from texts_for_embedding.json...")
        try:
            with open('texts_for_embedding.json', 'r', encoding='utf-8') as f:
                documents = json.load(f)
            print(f"    Loaded {len(documents)} documents.")
        except FileNotFoundError:
            print("\n[ERROR] 'texts_for_embedding.json' not found.")
            print("Please run the 'kg_builder_from_code.py' script first to generate it.")
            return
        
        query = "这个代码可能来自什么文件：function resetGlobalSeed(~)\n            rng(0);\n        end scope_id: tdropout scope_type: script"
        
        print("    Embedding documents...")
        doc_embeddings = [embedder.embed_query(doc) for doc in documents]
        
        print("    Embedding query...")
        query_embedding = embedder.embed_query(query)
        
        print("\n    Calculating similarities...")
        similarities = [cosine_similarity(query_embedding, doc_emb) for doc_emb in doc_embeddings]
        
        print(f'    Query: "{query}"')
        print("    --- Results ---")
        for i, doc in enumerate(documents):
            print(f'    Similarity: {similarities[i]:.4f} | Document: "{doc}"')
            
        most_similar_index = np.argmax(similarities)
        print("\n    Most similar document:")
        print(f'    -> "{documents[most_similar_index]}" (Score: {similarities[most_similar_index]:.4f})')
        print("\n    Success! Embedding similarity test complete.")

    except Exception as e:
        print(f"    [ERROR] Failed to test embedding model: {e}")

    # 2. Augment with LLM for a final answer
    try:
        print("\n[2] Using LLM to answer the query based on retrieved context...")
        llm = OllamaLLM(model_name="deepseek-r1:14b", model_params={"temperature": 0})
        
        context_document = documents[most_similar_index]
        
        prompt = f"""
        Based on the following code context, please answer the user's question.

        Context:
        ---
        {context_document}
        ---

        Question: {query}

        Answer:
        """
        
        print(f'    Asking LLM with the retrieved context...')
        
        response = await llm.ainvoke(prompt)
        
        print(f"    Success! LLM responded.")
        print(f"    --- Final Answer ---")
        print(response.content)

    except Exception as e:
        print(f"    [ERROR] Failed to generate answer with LLM: {e}")

    print("\n--- Test Complete ---")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except RuntimeError as e:
        if "cannot run loop while another loop is running" in str(e):
            loop = asyncio.get_event_loop()
            loop.create_task(main())
        else:
            raise
